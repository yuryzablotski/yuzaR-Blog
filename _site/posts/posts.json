[
  {
    "path": "posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress/",
    "title": "Exploratory Data Analysis (EDA) and BEYOND in R (in progress)",
    "description": "Exploratory Data Analysis (EDA) is an important first step on the long way to the final result, be it statistical inference in a published scientific paper or a machine learning algorithm in production. This way between EDA and final result is often rocky, bumpy, annoying and highly iterative. However, it's the most important part of data science, which includes data cleaning, data transformation, generating hypothesis, testing assumptions etc. Unfortunatly this part often remains a black box, simply due to the individual differences in knowledge and preferences of data scientists. So that we often don't know how and, most importantly WHY, the data was cleaned or transformed on the way to the final result. This article tries to narrow this gap by adding one further step to the EDA, which helps to generate hypotheses and identify next steps.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-09",
    "categories": [
      "EDA",
      "videos"
    ],
    "contents": "\n\nContents\nCreating visualised reports of the whole dataset with only one function!DataExplorer\nSmartEDA\n\nBig Picture of your dataDataExplorer\nskimr\n\nView missing value distribution for airquality dataDataExplorer\nvisdata\n\nVisualise distribution of categorical (discrete) variablesDataExplorer\nggstatsplot\n\nVisualise distribution of numeric variablesDataExplorer\nggpubr\n\nVisualize correlationDataExplorer\nPerformanceAnalytics\n\nVisualize numerical and categorical at the same timeDataExplorer\nggstatsplot\n\nVisualize outlier\nExploratory modelling\nFurther readings and references\n\n\n\n# install.packages(\"tidyverse\")\n# install.packages(\"DataExplorer\")\n\nlibrary(tidyverse)        # for data wrangling and visualization\nlibrary(datasets)         # for getting the data \nlibrary(DataExplorer)     # for exploratory data analysis\nlibrary(skimr)\nlibrary(PerformanceAnalytics)\nlibrary(ggstatsplot)\nlibrary(performance)\n\n\n\nI love R, but I sometimes find it very time consuming to look for the best functions for a particular task, there are so many great packages! Thus, if you know better functions or packages, please let me know in the comments below and let us together create a one-stop solution for EDA in R.\nCreating visualised reports of the whole dataset with only one function!\nDataExplorer\n\n\nlibrary(DataExplorer)     # for exploratory data analysis\n\nairquality <- airquality %>% \n  mutate(Month = factor(Month))\n\ncreate_report(airquality)\n\n# report with a response variable\ncreate_report(diamonds, y = \"price\")\n\n\n\nSmartEDA\n\n\n# Bonus 4:\nSmartEDA::ExpReport(airquality, op_dir  = 'other_docs/', op_file = 'smarteda.html')\n\n\n\nBig Picture of your data\nInstead of running create_report(), you may also run each function individually for your analysis, e.g.Â view basic description for airquality data.\nDataExplorer\n\n\nintroduce(airquality)\n\n\n  rows columns discrete_columns continuous_columns\n1  153       6                0                  6\n  all_missing_columns total_missing_values complete_rows\n1                   0                   44           111\n  total_observations memory_usage\n1                918         6376\n\nplot_intro(airquality)\n\n\n\n\nskimr\n\n\nskimr::skim(airquality)\n\n\nTable 1: Data summary\nName\nairquality\nNumber of rows\n153\nNumber of columns\n6\n_______________________\n\nColumn type frequency:\n\nnumeric\n6\n________________________\n\nGroup variables\nNone\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nOzone\n37\n0.76\n42.13\n32.99\n1.0\n18.00\n31.5\n63.25\n168.0\nâ–‡â–ƒâ–‚â–â–\nSolar.R\n7\n0.95\n185.93\n90.06\n7.0\n115.75\n205.0\n258.75\n334.0\nâ–…â–ƒâ–…â–‡â–…\nWind\n0\n1.00\n9.96\n3.52\n1.7\n7.40\n9.7\n11.50\n20.7\nâ–‚â–‡â–‡â–ƒâ–\nTemp\n0\n1.00\n77.88\n9.47\n56.0\n72.00\n79.0\n85.00\n97.0\nâ–‚â–ƒâ–‡â–‡â–ƒ\nMonth\n0\n1.00\n6.99\n1.42\n5.0\n6.00\n7.0\n8.00\n9.0\nâ–‡â–‡â–‡â–‡â–‡\nDay\n0\n1.00\n15.80\n8.86\n1.0\n8.00\n16.0\n23.00\n31.0\nâ–‡â–‡â–‡â–‡â–†\n\nView missing value distribution for airquality data\nDataExplorer\n\n\nplot_missing(airquality)\n\n\n\n\nvisdata\n\n\nvisdat::vis_dat(airquality)\n\n\n\n\nVisualise distribution of categorical (discrete) variables\nDataExplorer\n\n\nplot_bar(diamonds)\n\n\n\nplot_bar(diamonds, with = \"price\")\n\n\n\n## View frequency distribution by a discrete variable\nplot_bar(diamonds, by = \"cut\")\n\n\n\n\nggstatsplot\n\n\nggbarstats(data = diamonds, x = cut, y = clarity)\n\n\n\n\nVisualise distribution of numeric variables\nDataExplorer\n\n\n## View histogram of all continuous variables\nplot_histogram(airquality)\n\n\n\n## View estimated density distribution of all continuous variables\nplot_density(airquality)\n\n\n\n## View quantile-quantile plot of all continuous variables\nplot_qq(airquality)\n\n\n\n## View quantile-quantile plot of all continuous variables by feature `cut`\nplot_qq(airquality, by = \"Month\")\n\n\n\n\nggpubr\n\n\nlibrary(ggpubr)\nggqqplot(airquality$Temp)\n\n\n\nggqqplot(airquality$Temp[airquality$Month == 5])\n\n\n\n\nVisualize correlation\nDataExplorer\n\n\n## View overall correlation heatmap\nplot_correlation(na.omit(airquality), type = \"c\")\n\n\n\n\nPerformanceAnalytics\nCorrelation coeffitients and correlation color are amazing, they give us an idea about where there is a potentias for further exploration. However, they are quite limited though. Firstly, we donâ€™t really know whether these correlations are significant, so there are no tests behind it. Secondly, we donâ€™t even know what correlation method produced these coeffitients in the first place, was it the Pearson correaltion, or Spearman. Finally we donâ€™t see how data are scattered, so that some relationships might be very non-linear and correlation analysis would be not appropriate at all. The solution for all three problems is provided by the PerformanceAnalytics package, which offers a chart.Correlation() function. It produces histograms for every particular numeric variable and the scatterplots for every combination of numeric variables. I found the significance stars particularly helpful. Besides, we can easily choose the method we measure the correlation by, for instance, in the example below we use a robust non-parametric kendall correlation which is more appropriate for non-normally and non-too-linearly distributed values with some outliers. If we do not specify the methos, weâ€™ll produce a parametric (usual) Pearson correlation which is only appropriate for perfect data, which â€¦ rarely happens.\n\n\nPerformanceAnalytics::chart.Correlation(airquality %>% select(-Month)) \n\n\n\n# method = \"kendall\" or \"spearman\"\nPerformanceAnalytics::chart.Correlation(airquality %>% select(-Month), method = \"kendall\") \n\n\n\n\nVisualize numerical and categorical at the same time\nDataExplorer\n\n\n## View bivariate continuous distribution based on `cut`\nplot_boxplot(airquality, by = \"Month\")\n\n\n\n\nggstatsplot\n\n\nggstatsplot::ggbetweenstats(data = airquality, x = Month, y = Ozone, type = \"np\")\n\n\n\n## I did 3 videos about these fancy box plots with statistics\n\n\n\nVisualize outlier\nThe performance package provide an easy way to visualize outliers.\n\n\ncheck_outliers(airquality$Ozone, method = \"iqr\")\n\n\nWarning: 2 outliers detected (cases 62, 117).\n\nplot(check_outliers(airquality$Ozone, method = \"iqr\"))\n\n\n\n\nExploratory modelling\n\n\nggplot(airquality, aes(Solar.R, Temp))+\n  geom_point()+\n  geom_smooth()+\n  facet_wrap(~Month)\n\n\n\nggplot(diamonds, aes(price, carat))+\n  geom_point()+\n  geom_smooth()+\n  facet_grid(cut ~ clarity)\n\n\n\n\nIf you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.\nThank you for learning!\nFurther readings and references\n\n\n\n",
    "preview": "posts/2021-01-09-exploratory-data-analysis-and-beyond-in-r-in-progress/1.png",
    "last_modified": "2021-01-21T22:58:37+01:00",
    "input_file": "exploratory-data-analysis-and-beyond-in-r-in-progress.utf8.md",
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data/",
    "title": "R package reviews {DataExplorer} explore your data!",
    "description": "What is the best way to explore the data quick? I think it's visualization. And what it the best way to visualize the data quick? I think it's - {DataExplorer} package, because it can visualize all your data in seconds using only one function! Check this out...",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-06",
    "categories": [
      "R package reviews",
      "EDA",
      "videos"
    ],
    "contents": "\n\nContents\nR demo (ca. 9 min) about DataExplorer package\nAutomated EDA: visualise everything with only one function!\nTable of contents\nBig Picture of your data\nView missing value distribution\nVisualise distribution of categorical (discrete) variables\nVisualize correlation\nVisualise distribution of numeric variables\nVisualize boxplot\nVisualize scatterplots\nVisualize principal component analysis\nData transformation or Feature EngineeringSum up small categories\nDummification\nSet all missing values to a particular value.\nTransform or drop columns\n\nConclusion\nWhatâ€™s next?\nFurther readings and references\n\n\n\n# install.packages(\"tidyverse\")\n# install.packages(\"DataExplorer\")\n# install.packages(\"datasets\")\n\nlibrary(tidyverse)        # for data wrangling and visualization\nlibrary(DataExplorer)     # for exploratory data analysis\nlibrary(datasets)         # for getting the data \n\n\n\nR demo (ca. 9 min) about DataExplorer package\n\n\n\n\n\n\n\nAutomated EDA: visualise everything with only one function!\ncreate_report() function will produce, save and open an HTML document in your web-browser with visualization of all your data in seconds! Moreover, we can determine a single most important variable in our dataset and let the report display the data in the light of this variable. Just run create_report() function with two different datasets, as shown below and see the difference for yourself.\n\n\nairquality <- datasets::airquality %>% \n  mutate(Month = factor(Month))\n\n\n\n\n\ncreate_report(airquality)\n\ncreate_report(diamonds, y = \"price\")\n\n\n\nEvery report consists of several parts and always starts with â€œTable of contentsâ€.\nTable of contents\n\nTable of contents shows what exactly the exploratory analysis visualized. Namely, it first generally describes the dataset with raw numbers and percentages, then visualizes missing values and every single (univariate) numeric or categorical variable, while checking the distribution of numeric variables with histograms and Quantile-Quantile plots. The report then explores multiple variables simultaneously by displaying correlation between then and conducting a principal component analyses. It finishes with box and scatter plots, which I find especially informative.\nNot bad for a single function, if you ask me ;)\nBut, should we then always produced the whole report? Of coarse not. Conveniently, every part of the report can be produced separately, which not only allows you to explore every aspect of our data even deeper than in the big report, but also gives you the opportunity to adjust the appearance of plots as you wish. Letâ€™s have a look at this!\nBig Picture of your data\nBelow we can see the basic overview of diamonds data in absolute numbers. First, we see the number of rows and columns (variables). Then we see how many numeric and categorical variables do we have in our dataset and whether there are missing values or even missing columns.\n Such quick introduction into our dataset can be reached by the function introduce() for absolute numbers (â€œtâ€ simply transposes the result) or by the plot_intro() for percentages.\n\n\nintroduce(diamonds) %>% t()\n\n\n                        [,1]\nrows                   53940\ncolumns                   10\ndiscrete_columns           3\ncontinuous_columns         7\nall_missing_columns        0\ntotal_missing_values       0\ncomplete_rows          53940\ntotal_observations    539400\nmemory_usage         3457760\n\nplot_intro(airquality)\n\n\n\n\nView missing value distribution\nOne of the most useful functions from DataExplorer package, which I use almost daily is plot_missing(). It not only shows - how many percent of data is missing in every columns, but also explains what the amount of missing values means for us, namely that missing around 25% of data is still kind of OK and we can continue working. This legend helps me to better communicate with my collaborators, which often want me to continue analysis, even when I tell them there is not enough data in some columns. Yes, :) some people prefer to believe statistical software then a human being. Besides, it helps to quickly identify variables with missing values, if some models refuse to run because missing values are present.\nMoreover, all plots in DataExplorer can be pimped with the syntax from ggplot2 package:\n\n\n# pimp your plot with ggplot2\nplot_missing(airquality, ggtheme = theme_minimal())\n\n\n\n\nVisualise distribution of categorical (discrete) variables\nThe best way to visualize categorical variables are bar-plots. And a simple plot_bar() function visualizes all categorical variables from our dataset at once. It even sorts the categories for us putting the most frequent categories on the top. Itâ€™s very useful to see how many data-points every category has and usually, in my job, I see way too many small categories which are then supposed to be thrown together. By the way DataExplorer package can do this too, and easily, but more about this later. Until then, we can go one step further as compared to the create_report() function and display the frequency distribution of our categorical variable by any other variable, be it numeric (e.g.Â price) or discrete (e.g.Â cut). So, the long time of using table() function to count categorical variables is over for me :)\n\n\nplot_bar(diamonds)\n\n\n\n## View frequency distribution by a numeric variable\nplot_bar(diamonds, with = \"price\", ggtheme = theme_classic())\n\n\n\n## View frequency distribution by a discrete variable\nplot_bar(diamonds, by = \"cut\", ggtheme = theme_bw())\n\n\n\n\nVisualize correlation\nOne of the most powerful tools to summarize a large dataset and to identify patterns and connections between variables is a correlation matrix. A correlation matrix is simply a cross table with correlation coefficients between all combinations of variables pairwisely. This table is at the same time a heatmap, which colorcoded correlation from the negative (in blue) to the positive (in red). In practice, high correlation can be desirable, for example where it may allow to predict one variable by the other. However, it also may be unwanted, for example two highly correlated variables in a statistical model would screw up the results by providing the same information, like weight and BMI. Thus, knowing correlation is always useful for developing better hypotheses.\n\n\n## View overall correlation heatmap\nplot_correlation(na.omit(airquality), type = \"continuous\")\n\n\n\n\nVisualise distribution of numeric variables\nThe distribution of numeric variables is usually explored by the means of histograms, density plots, Quantile-Quantile plots and scatterplots. Well, DataExplorer package provides easy functions for each of them. Moreover, it allows to produce the Quantile-Quantile plots for every category of a discrete variable, which I personally find very useful, since usual groups comparison (be it a t-test or ANOVA) requires checking the normality of every group, and not of the whole numeric variable.\nAs you probably remember, the big-picture from the create_report() function does not provide density and categorical Quantile-Quantile plots, showing that itâ€™s worth to go beyond the general report. For instance, you immediately see that Wind and Temperature can be analyzed using parametric methods (e.g.Â ANOVA), while Ozone would either need a non-parametric approach (e.g.Â Kruskal-Wallis) or would need to be transformed before modeling.\n\n\n## View histogram of all continuous variables\nplot_histogram(airquality)\n\n\n\n## View estimated density distribution of all continuous variables\nplot_density(airquality, ncol = 3)\n\n\n\n## View quantile-quantile plots of all continuous variables\nplot_qq(airquality, ncol = 3, ggtheme = theme_linedraw())\n\n\n\n## View quantile-quantile plots of all continuous variables of all categories \nplot_qq(iris, by = \"Species\", \n        ncol = 2,\n        ggtheme = theme_bw(), \n        theme_config = list(legend.position = c(\"top\")))\n\n\n\n\nVisualize boxplot\nBoxplots beautifully show where the most of the data is, namely inside of the box, and whether variables have many outliers. Boxplots are also useful for a quick comparison of distributions of several groups, samples or categories, because they visualize center, spread and range of every group. Thus, similarly to correlation analysis above, boxplots also help to develop hypotheses. I love boxplots and have already three videos on them on my YouTube channel.\n\n\n## View bivariate continuous distribution based on `Month`\nplot_boxplot(iris, by = \"Species\", \n             ncol = 2, \n             ggtheme = theme_bw())\n\n\n\n\nVisualize scatterplots\nScatterplots are kind of self-explanatory.\n\n\nplot_scatterplot(airquality, by = \"Temp\", ncol = 2, ggtheme = theme_classic())\n\n\n\n\nVisualize principal component analysis\nThe goal of PCA is to reduce the number of variables in a dataset, while preserving as much information from the original dataset as possible. PCA transforms larger datasets with a lot of variables, some of which might be highly correlated, into a smaller dataset with a few new variables - (which are) the principal components and are uncorrelated with each other.\n\n\nplot_prcomp(diamonds, maxcat = 5L)\n\n\n\n\nData transformation or Feature Engineering\nSum up small categories\nTransforming and cleaning your data is the next step after EDA. Being able to make it quick and simple frees your time and brain capacity for the important creative work instead of solving problems. The best package for data manipulation is definitely - {dplyr}! However, some other worth to know packages also provide very simple and useful ways, but fade in the bright light of dplyr.\nDataExplorer is a good example. For instance, if your have a lot of small categories in a variable, you can aggregate them into a single category - â€œOTHERâ€, you just have to call the variable you are interested in and set the threshold for how many percent of your data suppose to go into this â€œOTHERâ€ category. For instance, the variable clarity has 4 categories, all of which have less then 10% of observations. Cumulatively (together) they add up to 20%. Thus, we can use 20% as a threshold and end up with only 5 big categories:\n\n\nplot_bar(diamonds)\n\n\n\njanitor::tabyl(diamonds$clarity) %>% \n  mutate(percent = round(percent*100)) %>% \n  arrange(-percent)\n\n\n diamonds$clarity     n percent\n              SI1 13065      24\n              VS2 12258      23\n              SI2  9194      17\n              VS1  8171      15\n             VVS2  5066       9\n             VVS1  3655       7\n               IF  1790       3\n               I1   741       1\n\n## Group bottom 20% `clarity` by frequency\nbla <- group_category(\n  diamonds, \n  feature = \"clarity\", \n  threshold = 0.2, \n  update = T)\n\njanitor::tabyl(bla$clarity) %>% \n  mutate(percent = round(percent*100)) %>% \n  arrange(-percent)\n\n\n bla$clarity     n percent\n         SI1 13065      24\n         VS2 12258      23\n       OTHER 11252      21\n         SI2  9194      17\n         VS1  8171      15\n\nplot_bar(bla)\n\n\n\n\nDummification\nSometimes, especially in the machine learning field, it is useful to produce several dummy variables (0s & 1s) from one categorical variable. We can easily do this by calling a dummify() function and specify the variable which needs to be dummified. Interestingly, if we donâ€™t specify any, all categorical variables will be transferred.\n\n\n## Dummify diamonds dataset\nglimpse(dummify(diamonds, select = \"cut\"))\n\n\nRows: 53,940\nColumns: 14\n$ carat         <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.2â€¦\n$ depth         <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.â€¦\n$ table         <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 5â€¦\n$ price         <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, â€¦\n$ x             <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.0â€¦\n$ y             <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.1â€¦\n$ z             <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.5â€¦\n$ color         <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, â€¦\n$ clarity       <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2â€¦\n$ cut_Fair      <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, â€¦\n$ cut_Good      <int> 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, â€¦\n$ cut_Ideal     <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, â€¦\n$ cut_Premium   <int> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, â€¦\n$ cut_Very.Good <int> 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, â€¦\n\nglimpse(dummify(diamonds))\n\n\nRows: 53,940\nColumns: 27\n$ carat         <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.2â€¦\n$ depth         <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.â€¦\n$ table         <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 5â€¦\n$ price         <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, â€¦\n$ x             <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.0â€¦\n$ y             <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.1â€¦\n$ z             <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.5â€¦\n$ cut_Fair      <int> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, â€¦\n$ cut_Good      <int> 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, â€¦\n$ cut_Ideal     <int> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, â€¦\n$ cut_Premium   <int> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, â€¦\n$ cut_Very.Good <int> 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, â€¦\n$ color_D       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ color_E       <int> 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, â€¦\n$ color_F       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, â€¦\n$ color_G       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ color_H       <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, â€¦\n$ color_I       <int> 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ color_J       <int> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, â€¦\n$ clarity_I1    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ clarity_IF    <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ clarity_SI1   <int> 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, â€¦\n$ clarity_SI2   <int> 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, â€¦\n$ clarity_VS1   <int> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, â€¦\n$ clarity_VS2   <int> 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, â€¦\n$ clarity_VVS1  <int> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n$ clarity_VVS2  <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, â€¦\n\nSet all missing values to a particular value.\n\n\ndf <- airquality %>% \n  slice(1:10)\n\ndf\n\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5     NA      NA 14.3   56     5   5\n6     28      NA 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10    NA     194  8.6   69     5  10\n\nset_missing(df, list(0L, \"unknown\"))\n\n\n   Ozone Solar.R Wind Temp Month Day\n1     41     190  7.4   67     5   1\n2     36     118  8.0   72     5   2\n3     12     149 12.6   74     5   3\n4     18     313 11.5   62     5   4\n5      0       0 14.3   56     5   5\n6     28       0 14.9   66     5   6\n7     23     299  8.6   65     5   7\n8     19      99 13.8   59     5   8\n9      8      19 20.1   61     5   9\n10     0     194  8.6   69     5  10\n\nTransform or drop columns\nAgain, dplyr is much better for data manipulation. But if you still donâ€™t know dplyr, first, your should, and secondly, you can use functions below to quickly update or drop columns.\n\n\n## Update columns\nupdate_columns(airquality, c(\"Month\", \"Temp\"), as.factor) %>% head()\n\n\n  Ozone Solar.R Wind Temp Month Day\n1    41     190  7.4   67     5   1\n2    36     118  8.0   72     5   2\n3    12     149 12.6   74     5   3\n4    18     313 11.5   62     5   4\n5    NA      NA 14.3   56     5   5\n6    28      NA 14.9   66     5   6\n\nupdate_columns(airquality, c(\"Temp\", \"Wind\"), function(x) x^2) %>% head()\n\n\n  Ozone Solar.R   Wind Temp Month Day\n1    41     190  54.76 4489     5   1\n2    36     118  64.00 5184     5   2\n3    12     149 158.76 5476     5   3\n4    18     313 132.25 3844     5   4\n5    NA      NA 204.49 3136     5   5\n6    28      NA 222.01 4356     5   6\n\n## Drop columns\nmtcars %>% drop_columns(2:10) %>% head()\n\n\n   mpg carb\n1 21.0    4\n2 21.0    4\n3 22.8    1\n4 21.4    1\n5 18.7    2\n6 18.1    1\n\ndiamonds %>% drop_columns(c(\"x\", \"y\", \"z\")) %>% head() \n\n\n# A tibble: 6 x 7\n  carat cut       color clarity depth table price\n  <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int>\n1 0.23  Ideal     E     SI2      61.5    55   326\n2 0.21  Premium   E     SI1      59.8    61   326\n3 0.23  Good      E     VS1      56.9    65   327\n4 0.290 Premium   I     VS2      62.4    58   334\n5 0.31  Good      J     SI2      63.3    58   335\n6 0.24  Very Good J     VVS2     62.8    57   336\n\nConclusion\nDataExplorer allows us not only to quickly explore our data, but also helps to get initial insights and to generate new hypotheses. Moreover, it greatly simplifies the workflow, which we need to do anyway, saves time, reduces the probability of mistakes by coding everything manually and accelerates delivering final results. So, DataExplorer alone would be enough for a reasonably good exploration of data. But, there are of coarse many other useful packages for EDA. Thus, in the next post Iâ€™ll present the best functions from different packages which will take your EDA to the next level.\nWhatâ€™s next?\nThe next steps in your journey to the final results are cleaning and further pre-processing your data.\nIf you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.\nThank you for learning!\nFurther readings and references\nhttp://boxuancui.github.io/DataExplorer/\n\n\n\n",
    "preview": "posts/2021-01-06-r-package-reviews-dataexplorer-explore-your-data/2.png",
    "last_modified": "2021-01-17T11:50:57+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-06-survival-analysis-2-parametric-survival-models/",
    "title": "Survival analysis 2: parametric survival models",
    "description": "The non-parametric Kaplan-Meier method (KM) can not describe survival probability by a smooth function, which means it can not predict anything. The parametric models (e.g. Exponential, Weibull etc.) can! Besides, in case where parametric models are appropriate, they are more exact, more effective and more informative than KM or Cox. However, unfortunately, this step is often left out due to the rear use of parametric models. In this post weâ€™ll try to close this gap.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-06",
    "categories": [
      "survival analysis",
      "videos"
    ],
    "contents": "\n\nContents\nPrevious topics\nWhy do we need parametric survival modelsIs Time a Variable or a Constant?\nSteady change in hazard and survival\nPositive exponential change in hazard and survival\nNegative exponential change in hazard and survival\nB(u)ilding Exponential model â€¦ finally ðŸ¥³\n\nHow to compute parametric models\nFinal thoughts\nFurther readings and references\n\n\n\nlibrary(tidyverse)  # data wrangling and visualization\nlibrary(sjPlot)     # for plotting results of log.regr.\nlibrary(effects)    # for probability output and plots\nlibrary(survival)   # for core survival analysis routines\nlibrary(survminer)  # for drawing survival curves\nlibrary(flexsurv)   # for Parametric Survival Modelling\nlibrary(knitr)      # for a wonderful-looking tables \n\n\n\nPrevious topics\nA good understanding of Kaplan-Meier method (KM) is a prerequisite for this post, but since you are here, I suppose you are already familiar with it ðŸ˜‰\nIf you are more of a visual person, you can watch the video below first, but for more details and R-code have a look at the article.\n\n\n\n\n\n\n\nWhy do we need parametric survival models\n\n\n\nThe main disadvantage of the non-parametric Kaplan-Meier method (KM) shown in the picture above is that it can not be described survival probability by a smooth function, which means it can not predict anything. The parametric models (e.g.Â Exponential, Weibull etc.) can! Moreover, parametric models are the logical step on the way from the KM to the semi-parametric Cox models, because they beautifully connect the dots between KM and Cox models and thus greatly improve understanding of survival analysis. Besides, in case where parametric models are appropriate, they are more exact, more effective and more informative than KM or Cox. However, unfortunately, this step is often left out due to the rear use of parametric models. In this post weâ€™ll try to close this gap.\nIs Time a Variable or a Constant?\nSo, how can we describe the survival with a smooth function? To answer this question, letâ€™s first describe a NOT-survival function, which happens to be a Hazard to die.\n\nImage by Leonardo Yip on unsplash.\nIâ€™d describe death with two things: the event of death itself and a particular time point at which death happens. These two things always describe a single event, because one only dies ðŸ’€ once. However, if several people die ðŸ’€ðŸ’€ðŸ’€, they will not die in the exact same moment, right? Nop. Thus, the time of death would vary and the number of death would grow over time, which would make the time itself a variable. Thatâ€™s how several events of death at different time points allow us to express death in two ways:\nvia a different number of events per fixed unit of time, which is often called a Risk (Hazard) to die, or simply Hazard (\\(\\lambda\\) - lambda). This makes the number of events a variable, and Hazard a rate of death, e.g, per day. Or,\nvia different stretches of passing time per fixed number of events. The time interval is usually measured until the next event occurs. This makes the time a variable.\nBut whatever changes, the number of events per unite of time, or time per unit of events, the change itself is a key here, and there are different kinds of changes.\nSteady change in hazard and survival\nImagine that every day exactly 3 out of 10 people die in the cold ocean water after Titanic accident. Itâ€™s a pretty stable rate of death, or Hazard, of 30%. If the hazard to die would steadily grow with the same rate, then the probability of survival would steadily decrease at the same rate. Thus, Hazard and Survival can be expressed in terms of each other. Particularly, the Hazard of dying over time can be seen as a Failed survival (\\(F(t)\\) in the left formula below). Or, the Survival over time (\\(S(t)\\) the right formula below) can be seen as the Hazard of NOT-dying, or simply a negative Hazard, which mathematically can be expressed as a â€œ-â€ minus sign, or â€œ1 -â€, in front of the Hazard. Both functions result in straight lines, where Hazard steadily increases and survival steadily decreases (plots below):\n\n\n\\[ F(t) = Hazard * t \\]\n\n\n\\[ S(t) = 1 - Hazard * t \\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuch steady increase or decrease in hazard or survival are rather not natural, not realistic. Itâ€™s kind of hard to plan and monitor death ðŸ˜‰, except you are a serial killer. The hazard usually either exponentially increases, e.g.Â in the case of hunger, or exponentially declines, e.g.Â in the case of a pandemic after vaccine was found. Thus, letâ€™s have a look at both exponential changes.\nPositive exponential change in hazard and survival\nThink about hunger for a moment. It accumulates, right? And the longer we stay hungry, the higher is the probability (risk) that we die. The left plot below shows such a development, where a Hazard of dying \\(F(t)\\) expressed in probabilities is small in the beginning (few deaths), but grows exponentially with time (more and more deaths). I like to call such a trend a positive, or accelerating, exponential change. The â€œexpâ€ in the left formula below is all we need to add in order to plot such a trend.\nAgain, since survival can be seen as a negative hazard, we can express the survival \\(S(t)\\) by simply using a minus sign (or â€œ1-â€) in front of the Hazard. The plot on the right displays the results of such survival function. Itâ€™s kind of obvious, that if the hazard of dying is low in the beginning of â€œtimeâ€, then the probability of survival is high. At the end of â€œtimeâ€ Survival exponentially drops due to a exponential increase of the Hazard.\n\n\n\\[ F(t)= exp^{Hazard * t} \\]\n\n\n\\[ S(t) = 1 - exp^{Hazard * t} \\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegative exponential change in hazard and survival\nHowever, in some cases, more people die at the beginning of â€œtimeâ€, where after rate of death declines over time. Think about a pandemic, or a titanic crash. The change is still exponential, and goes in the same direction: up, in the case of the Hazard, and down, in the case of survival. However, the exponential change is kind of turned (bend) inside out. I like to call it a negative, or decelerating, exponential change.\nSuch â€œinside out bendingâ€ can be achieved by a â€œ-â€ minus sing in front of both a Hazard and the exponential function itself (left equation and picture below). This minus does not describe the negative hazard, as in the example above. It only changes the curve from accelerating to decelerating. In order to get survival for this function, we also, as in the example above, have to use a â€œ-â€ (or â€œ1-â€) in front of the whole exponential hazard expression. Interestingly, two minus signs in front of the â€œexpâ€ in the Survival formula neutralize each other and become a plus, which leaves us with the \\(exp^{-Hazard * t}\\) (right equation and picture below).\n\n\n\\[ F(t)= - exp^{-Hazard * t} \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ \\]\n\n\n\n\n\n\\[ S(t) = 1 - ( - exp^{-Hazard * t}) = exp^{-Hazard * t} \\]\n\n\n\n\n\nB(u)ilding Exponential model â€¦ finally ðŸ¥³\n\nImage by MihÃ¥ly KÃ¶les on unsplash.\nNow we finally have our smooth survival function we wanted to describe in the beginning. Letâ€™s look at it one more time and plot it on top of the not-smooth Kaplan-Meier step function.\nSince survival function \\(S(t)\\) shows the probability of survival passed the certain time point,\n\\[ S(t) = P(T > t) \\]\nand the exponential function \\(F(t)\\) shows the probability of Failed-survival passed the certain time point,\n\\[ F(t) = P(T \\leq t) = 1 - exp^{-Hazard * t} \\]\na survival function can then be expressed in terms of not-survival (exponential) function:\n\\[ S(t) = P(T > t) = 1 - P(T \\leq t) = 1 - F(t) = 1 - [1 - exp^{-Hazard * t}] = exp^{-Hazard * t} \\]\nSo that the survival function, which shows a rate of decrease, is a flipped hazard function, which showed a rate of increase. Thus, we can rewrite the \\(S(t)\\) as:\n\\[S(t) = exp^{- Hazard * t} = exp^{- \\lambda* t} \\]\nwhere -Hazard and time are two parameters which describe our exponential change in survival probability, which is why such models do have their name - parametric exponential models. And since the Hazard is negative, and exponential function is not-linear (aka. curvy) - out model produces a negative exponential curve (see below). Such smooth rate of decrease describes survival probability much better then a Kaplan-Meier method, which abruptly (step-wisely) drops probability only after an event, while keeping the probability constant between the events.\nHow to compute parametric models\n\n\nlibrary(flexsurv)    # for Parametric Survival Modelling\n\nex <- flexsurvreg(Surv(time, status) ~ 1, data = d2, dist=\"exponential\")\nwe <- flexsurvreg(Surv(time, status) ~ 1, data = d2, dist=\"weibull\")\n\nggsurvplot(\n  ex, \n  conf.int = FALSE, \n  surv.median.line = \"hv\",\n  xlab = \"Days\", \n  ylab = \"Survival probability\", \n  break.time.by = 1,\n  risk.table = F\n)\n\n\n\n\nThe parameters of the curve (\\(-\\lambda * t\\)) allow us to model and predict survival and hazard over time, which is the main advantage of exponential models over the Kaplan-Meier method, which is not-parametric and therefore not â€œmodellableâ€. However, the non-linearity is often troublesome, and weâ€™d rather use the linear regression concept, which summarizes (regresses) a lot of numbers into a few numbers, like the intercept (\\(\\beta_0\\)) and the slope (\\(\\beta_1\\)). Fortunately, a non-linear curve can be easily â€œlinearisedâ€ via a natural logarithm. For this, we donâ€™t even have to understand how logarithm or the exponential function work, we only need to know that they neutralize each other. Moreover, using â€œlogâ€ (logarithmazing both sides of the equation below) produces three positive side effects:\nfirst, on the right side of the equation, this would transfer our curve (\\(-\\lambda * t\\)) into a line (\\(b_0 + b_1x_1 + ... + b_kx_k\\)), where we will be able to have an intercept and \\(\\beta\\) coefficients as in a usual linear logistic regression \\[Hazard = exp^{b_0 + b_1x_1 + ... + b_kx_k}\\]\nor\n\\[ log(Hazard) = b_0 + b_1x_1 + ... + b_kx_k \\]\nSuch survival model is in fact the Poisson model. Thus, it might help if you already know Poisson distribution. If not, thatâ€™s OK, you donâ€™t have to understand Poisson before (citation from Prof.Â Marinâ€™s video, see references).\nsecondly, it will help us to connect to further models, like Weibull and Cox models, because the difference between them lie mainly in the intercept â€œ\\(b_0\\)â€;\nand finally, it will greatly increase the interpretability, because the Hazard-Ratios (HRs) (thatâ€™s what exponential model delivers) can be interpreted exactly like the Odds-Ratios (ORs) from the logistic regression (as described in my posts on logistic regression). Similarly to the Odds in the logistic regression, the Hazard itself, which it the probability of dying NOW, is less useful then the Hazard-Ratios. A Hazard-Ratio is the ratio of hazard of somebody who is exposed (sick) to somebody who is not exposed (healthy). For instance if HR = 2, the risk of dying of somebody who is exposed is double compared to somebody who is not exposed.\nFinal thoughts\nAre parametric models useful? Of coarse!\n3 curves above which described different kind of changes in survival and hazard over time meant to say that distribution can be very different. If a suitable distribution can be found, parametric model is more informative than the KM or Cox model\nthey can predict survival probabilities at any given point in time, event hazard, mean and median survival times are readily available\nthey are also slightly more efficient and yield more precise estimates due a better fit (see the picture above)\nAre parametric models perfect? Of coarse not!\nparametric models need to specify the distribution, which may be difficult to identify\nthey are also mathematically more complex then, e.g.Â KM, and are therefore rarer, which greatly decrease comparability of results among studies. Due to their lower popularity I will not go deeper into any particular parametric model: Weibull, Gompertz, Accelerated Failure Time models etc.\nThus, despite the fact that parametric models are a good alternative to the KM and Coxâ€™s regression model (which do not need to specify any distribution), KM and Cox remain the most popular methods for analyzing survival data. And that is why the next logical step in your statistical journey would be learning about Cox Proportional Hazard Models (in progress).\nIf you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.\nThank you for learning!\nFurther readings and references\nM J Bradburn, T G Clark, S B Love, & D G Altman. (2003). Survival Analysis Part II: Multivariate data analysis â€“ an introduction to concepts and methods. British Journal of Cancer, 89(3), 431-436.\nExamples of focused model comparison: parametric survival models: https://cran.r-project.org/web/packages/fic/vignettes/survival.pdf\nsurvminer cheat sheet: https://rpkgs.datanovia.com/survminer/survminer_cheatsheet.pdf\nthe whole playlist of videos from MarinStatsLectures! They are amazing! https://www.youtube.com/watch?v=vX3l36ptrTU&list=PLqzoL9-eJTNDdnKvep_YHIwk2AMqHhuJ0 especially the second video of marinstatslecture: https://www.youtube.com/watch?v=MdmWdIV5k-I&list=PLqzoL9-eJTNDdnKvep_YHIwk2AMqHhuJ0&index=2\n\n\n\n",
    "preview": "posts/2021-01-06-survival-analysis-2-parametric-survival-models/thumbnail_survival_2.png",
    "last_modified": "2021-01-06T18:00:27+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/",
    "title": "R package reviews {performance} check how good your model is! ",
    "description": "There are several indicators of model quality, e.g. $R^2$ or AIC, and several assumption for every model which supposed to be checked, e.g. normality of residuals, multicollinearity etc.. R provides solutions for every indicator or assumption you can imagine. However, they are usually spread around different packages and functions. {performance} package brings all of quality indicators and all of the assumption under one roof. Thus, for me it became the one-stop solution for modelling.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-03",
    "categories": [
      "R package reviews",
      "videos",
      "visualization"
    ],
    "contents": "\n\nContents\nR demo for how check model performance and model assumptions\nCheck model performance, or model qualityUsual linear models\nComplex mixed-effects models\nFancy Bayesian mixed-effects models\nCompare models\nUse individual quality indicators\n\nCheck assumptions: or modern (Jan 2020) model diagnosticsAll assumptions at once!\nIndividual assumptions with reports of statistical tests!\n\nUseful references\n\n\n\nlibrary(tidyverse)          # data manipulation\nlibrary(performance)        # model performance\n\n\n\nR demo for how check model performance and model assumptions\nA short (ca. 15 min) video below shows how performance package works and the code youâ€™ll see in the video is provided below.\n\n\n\n\n\n\n\nCheck model performance, or model quality\nUsual linear models\nIn order to see how model performs, use the intuitive function - model_performance. It provide several quality indicators, which differ depending on the model. For instance the first model below provides the most â€œclassicalâ€ quality indicators:\nAIC - Akaikeâ€™s Information Criterion, the lower the better. AIC is an estimator of out-of-sample prediction error and thereby relative quality of statistical models for a given set of data.\nBIC - Bayesian Information Criterion, the lower the better\n\\(R^2\\) - the proportion of the variance explained, the higher the better. It is sometimes referred to as a goodness of fit of the model\n\\(R^2 adjusted\\) - the proportion of the variance explained for multiple (several predictors) models, the higher the better\nRMSE - Root Mean Square Error is a measure of spread of the residuals around predictions (prediction errors), the lower the better\nSigma - standard deviation is a measure of spread of the data around the mean, the lower the better\n\n\nm <- lm(mpg ~ hp + cyl, data = mtcars)\n\nmodel_performance(m)\n\n\n# Indices of model performance\n\nAIC    |    BIC |   R2 | R2 (adj.) | RMSE | Sigma\n-------------------------------------------------\n169.56 | 175.42 | 0.74 |      0.72 | 3.02 |  3.17\n\nComplex mixed-effects models\nMixed effects model provide two different \\(R^2\\)s and ICC:\nconditional \\(R^2\\) shows modelâ€™s total explanatory power and\nmarginal \\(R^2\\) show the part related to the fixed effects (predictors) alone\nICC - intraclass correlation coefficient, is similar to \\(R^2\\) and also shows the goodness of fit or, in other words, quantifies the proportion of variance explained by a grouping (random) factor in mixed-effects (multilevel/hierarchical) models.\n\n\nlibrary(lme4)\n\nm1 <- lmer(mpg ~  hp * cyl + wt + (1 | am), data = mtcars)\n\nmodel_performance(m1)\n\n\n# Indices of model performance\n\nAIC    |    BIC | R2 (cond.) | R2 (marg.) |      ICC | RMSE | Sigma\n-------------------------------------------------------------------\n167.07 | 177.33 |       0.86 |       0.86 | 8.79e-03 | 2.06 |  2.24\n\nFancy Bayesian mixed-effects models\nELPD - expected log pointwise predictive density is a measure the prediction accuracy of Bayesian models, the closer to 0 the better. The out-of-sample predictive fit can either be estimated by Bayesian leave-one-out cross-validation (LOO) or by widely applicable information criterion (WAIC). Thus, the two next indicators are:\nLOOIC - leave-one-out cross-validation information criterion, the lower the better\nWAIC - widely applicable information criterion, the lower the better\n\n\nlibrary(rstanarm)\n\nm2 <- stan_glmer(mpg ~ hp * cyl + wt  + (cyl | am), data = mtcars, refresh=0)\n\nmodel_performance(m2)\n\n\n# Indices of model performance\n\nELPD   | ELPD_SE |  LOOIC | LOOIC_SE |   WAIC |   R2 | R2 (marg.) | R2 (adj.) | RMSE | Sigma\n--------------------------------------------------------------------------------------------\n-75.75 |    3.95 | 151.51 |     7.90 | 150.85 | 0.86 |       0.85 |      0.83 | 2.07 |  2.28\n\nCompare models\nUnfortunately we canâ€™t compare not similar models, for instance different types of models (e.g.Â linear vs.Â mixed effects) or models done with different amounts of data, because one of them will be better for reasons other then model quality. Even more unfortunately, such comparisons are often conducted anyway. I was also guilty of it in the past. One of the common R functions allowing such comparisons is anova(model1, model2). It smartly does not allow the comparison of linear vs.Â mixed effects models (see the red warning below), but stupidly can be out-tricked by placing mixed effects model first.\n\n\nanova(m, m1)\n\n\n\nError: $ operator not defined for this S4 class\n\n\nanova(m1, m)\n\n\nData: mtcars\nModels:\nm: mpg ~ hp + cyl\nm1: mpg ~ hp * cyl + wt + (1 | am)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm     4 169.56 175.43 -80.781   161.56                         \nm1    7 151.05 161.31 -68.524   137.05 24.513  3  1.952e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIn contrast compare_performance() compares them immediately and provides a useful warning (see below). It also provides an somewhat superficial (according to the package author), but still useful ranking of models (see Performance-Score).\n\n\ncompare_performance(m, m1, rank = T)\n\n\n# Comparison of Model Performance Indices\n\nModel |    Type |    AIC |    BIC |    BF | RMSE | Sigma | Performance-Score\n----------------------------------------------------------------------------\nm1    | lmerMod | 167.07 | 177.33 | 0.385 | 2.06 |  2.24 |            75.00%\nm     |      lm | 169.56 | 175.42 |  1.00 | 3.02 |  3.17 |            25.00%\n\nWarning: Models are not of same type. Comparison of indices might be not meaningful.\nHowever, if we compare models which are comparable, no warning will be displayed.\n\n\nm1.1 <- lmer(mpg ~  hp + cyl + wt + (1 | am), data = mtcars)\ncompare_performance(m1, m1.1, rank = T)\n\n\n# Comparison of Model Performance Indices\n\nModel |    Type |    AIC |    BIC |   BF | R2 (cond.) | R2 (marg.) |      ICC | RMSE | Sigma | Performance-Score\n----------------------------------------------------------------------------------------------------------------\nm1    | lmerMod | 167.07 | 177.33 | 1.00 |       0.86 |       0.86 | 8.79e-03 | 2.06 |  2.24 |            71.43%\nm1.1  | lmerMod | 164.24 | 173.03 | 8.58 |       0.83 |       0.83 | 8.48e-03 | 2.35 |  2.51 |            28.57%\n\nMoreover, performance package can also easily plot this comparison, simply by wrapping up the compare_performance() function into a plot() function. It could not be easier or more elegant than that!\n\n\nplot( compare_performance(m1, m1.1) )\n\n\n\n\nUse individual quality indicators\n\n\nr2(m1)    # model fit - the proportion of the variance explained\n\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.864\n     Marginal R2: 0.862\n\nr2(m2)    # Bayesian R2 even with Credible Intervals\n\n\n# Bayesian R2 with Standard Error\n\n  Conditional R2: 0.861 (89% CI [0.800, 0.919])\n     Marginal R2: 0.853 (89% CI [0.766, 0.930])\n\nicc(m2)   # Intraclass Correlation Coefficient (ICC)\n\n\n# Intraclass Correlation Coefficient\n\n     Adjusted ICC: 0.847\n  Conditional ICC: 0.461\n\n# not part of the \"performance\" package, but fits well here\nAIC(m1)   # Akaike's Information Criterion\n\n\n[1] 167.0742\n\nBIC(m1)   # Bayesian Information Criterion\n\n\n[1] 177.3343\n\nCheck assumptions: or modern (Jan 2020) model diagnostics\nAll assumptions at once!\nThe first time I discovered check_model() function, I have got an intensive intellectual â€œnerdgasmâ€. I could not believe how simple and at the same time sophisticated this function is! I then checked out other work of the authors of {performance} package and was stunned, how many useful things they already produced. And I donâ€™t think theyâ€™ll stop any time soon. Thus, if you just began to learn R and stats, check out their work to quickly step up your data science game!\n\n\ncitation(\"performance\")\n\n\n\nTo cite performance in publications use:\n\n  LÃ¼decke, Makowski, Waggoner & Patil (2020). Assessment of\n  Regression Models Performance. CRAN. Available from\n  https://easystats.github.io/performance/\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {performance: Assessment of Regression Models Performance},\n    author = {Daniel LÃ¼decke and Dominique Makowski and Philip Waggoner and Indrajeet Patil},\n    journal = {CRAN},\n    year = {2020},\n    note = {R package},\n    doi = {10.5281/zenodo.3952174},\n    url = {https://easystats.github.io/performance/},\n  }\n\nNow the function itself: it visually checks all the assumptions you need to check and gives you a big-picture overview of assumptions for almost any model you can have (at least for all the common ones). Two examples below display such a big-picture of a usual linear model and a mixed-effects model with random effects. The subplots of this picture even explain what you should look for! For instance: â€œDots should be plotted along the lineâ€ in the residuals diagnostics plots. Have a look at the big-picture first and go to the next chapter for some more details on the particular assumption. Why? Because the package provides the opportunity to check individual assumptions too and even goes one step deeper into it!\n\n\ncheck_model(m)\n\n\n\n\n\n\ncheck_model(m1)\n\n\n\n\nIndividual assumptions with reports of statistical tests!\nChecking individual assumption is also very intuitive. For instance, for checking the normality of the residuals, use check_normality() function. It will conduct the Shapiro-Wilk Normality test and report the result of it. Put check_normality(m) inside of the plot() function to visualize the result. It is even preferable to visually inspect the residuals, because Shapiro-Wilk Test will often produce significant results for large sample sizes (and in the age of big data we always have large samples) even if data is perfectly normally distributed.\n\n\ncheck_normality(m)     # shapiro.test, however, visual inspection (e.g. Q-Q plots) are preferable\n\n\nOK: residuals appear as normally distributed (p = 0.212).\n\ncheck_normality(m1)\n\n\nWarning: Non-normality of residuals detected (p = 0.010).\n\nplot( check_normality(m1) )\n\n\nWarning: Non-normality of residuals detected (p = 0.010).\n\n\nThe collinearity is measured by the variance inflation factor (VIF). VIF<5 is acceptable for individual predictors, while VIF<10 is moderate, so, that it gives you an idea that some variables in the model might be redundant. If you model interactions, VIF would naturally increase, and the unwritten (I forgot where did I learned it from ðŸ™ˆ) rule is that interactions with VIF<20 are still acceptable.\n\n\ncheck_collinearity(m)  # by variance inflation factor (VIF)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n Parameter  VIF Increased SE\n        hp 3.26         1.80\n       cyl 3.26         1.80\n\ncheck_collinearity(m1)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n Parameter  VIF Increased SE\n        wt 2.50         1.58\n\nHigh Correlation\n\n Parameter    VIF Increased SE\n        hp  78.22         8.84\n       cyl  12.22         3.50\n    hp:cyl 122.72        11.08\n\nplot( check_collinearity(m1) )\n\n\n\n\nIf you wanna check the heteroscedasticity, use function check_heteroscedasticity(). The heteroscedasticity assumption itself is kind of self-explanatory. But what I canâ€™t explain is that why nobody in the history of R programming language came up with such intuitive functions before {performance}?\n\n\ncheck_heteroscedasticity(m)\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.041).\n\ncheck_heteroscedasticity(m1)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.153).\n\nplot( check_heteroscedasticity(m) )\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.041).\n\n\nOne particular function somehow didnâ€™t finish up in the â€œbig-pictureâ€. Namely check_outliers(). May be because there are too many methods for identifying outliers (â€œzscoreâ€, â€œiqrâ€, â€œcookâ€, â€œparetoâ€, â€œmahalanobisâ€, â€œrobustâ€, â€œmcdâ€, â€œicsâ€, â€œopticsâ€, â€œlofâ€)? I donâ€™t know. Z-Scores is the default method though, however, the method can be easily specified as an argument of the function:\n\n\ncheck_outliers(mtcars$mpg)\n\n\nWarning: 4 outliers detected (cases 18, 19, 20, 28).\n\nplot(check_outliers(mtcars$mpg))\n\n\n\nplot(check_outliers(mtcars$mpg, method = \"iqr\"))\n\n\n\n\nThere are many more useful functions in this package. And there is no need to describe them all here. If you liked what you have seen so far, just type â€œ?performanceâ€ in the RStudio console, go to Help, scroll down to the bottom of the Help page, click Index and enjoy the package ðŸ˜‰.\nIf you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.\nCheers guys, have a good one!\nUseful references\nhttps://easystats.github.io/performance/\nhttps://cran.r-project.org/web/packages/performance/performance.pdf\nhttps://www.rdocumentation.org/packages/performance/versions/0.6.1\n\n\n\n",
    "preview": "posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/14.png",
    "last_modified": "2021-01-16T09:27:05+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/",
    "title": "Survival analysis 1: a gentle introduction into Kaplan-Meier Curves",
    "description": "Survival time analysis is necessary in any study which investigates the time to a particular outcome of interest. Cancer studies in the medicine and the first failure of the car in the engineering field (failure time analysis) are good examples. The outcome of interest could be death, remission to relapse, progression, or failure. Point in time of reaching that outcome is generally called the event. Thank goodness, not every â€œeventâ€ is fatal ðŸ˜ƒ, but can sometimes even be a favorable outcome such as discharge from hospital. And thus, survival analysis is also a generic term, because it is not only about survival.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-03",
    "categories": [
      "survival analysis",
      "videos"
    ],
    "contents": "\n\nContents\nPrevious topics\nWhy do we need survival analysis?Death is not the only option! Or: â€œWhat is an event?â€\nCensoring\n\nHow to calculate Kaplan-Meier survival curve â€œmanuallyâ€ step by stepSurvival probability\n\nHow to compute Kaplan-Meier survival curveInterpretation of Kaplan-Meier Curve\n\nComparing survival of groups2 groups\nInterpretation of groups comparison using 4 benchmarks\nLog-Rank test\n> 2 groups and multiple pairwise (post-hoc) Log-Rank test\nMultiple survival curves\n\nConclusions\nWhatâ€™s next?\nFurther readings, videos and references\n\n\n\nlibrary(tidyverse)  # data wrangling and visualization\nlibrary(sjPlot)     # for plotting results of log.regr.\nlibrary(effects)    # for probability output and plots\nlibrary(survival)   # for core survival analysis routines\nlibrary(survminer)  # for drawing survival curves\nlibrary(knitr)      # for a wonderful-looking tables\n\n\n\nIf you are more of a visual person, you can watch the video below first, but for more details and R-code have a look at the article.\n\n\n\n\n\n\n\nCan there be something more horrifying, then a Titanic crash with lots of immediate deaths? Well, unfortunately, yes! Namely, not immediate deaths. Imagine survived people in the middle of the cold dark ocean, some in life jackets, some without, clinging on ship remains, with a huge panic and little hope for rescue. There will be no helicopter to save them (itâ€™s 1912). There might not be a single ship in those waters for months. How long will they survive? Will time simply prolong their suffering or increase the probability of survival, because eventually they will be found by some random ship? Do women have higher chances of survival then men? What about rich vs.Â pure passengers? Well, in this post weâ€™ll find answers to all of these questions and along the way learn how survival analysis works. Particularly, we will:\nlearn about the most important concepts of survival analysis: survival curve, censoring and log-rank test,\nmanually calculate, then compute and interpret survival curves, and\ntest survival differences between two or more groups, e.g.Â females vs.Â males\n\nSource\nPrevious topics\nA good understanding of linear and logistic regressions would improve the digestion of this post.\nWhy do we need survival analysis?\n\n\nlibrary(tidyverse)\n\nd <- tibble(\n  time   = c(1,1,1,2,2,3,4,5,7,10), \n  status = c(1,1,1,1,0,1,0,0,1,0)\n)\n\nd %>% kable()\n\n\ntime\nstatus\n1\n1\n1\n1\n1\n1\n2\n1\n2\n0\n3\n1\n4\n0\n5\n0\n7\n1\n10\n0\n\nTo answer this question, letâ€™s start with a small example of Titanic survivors: only 10 people and only 10 days on the sea after the crash and analyse it with the classic statistical methods, linear and logistic regressions. We are interested in how many people and how long survived and how many died on a particular day. Thus, two main parameters of interest for us are (1) time (in days) and (2) â€œlife-statusâ€ of people, where â€œstatus = 1â€ means death and â€œstatus = 0â€ means not-death. And having the status of people we could just have counted survivors and not-survivors at each particular day to understand what was going on, wright?\n\n\nggplot(d, aes(time, fill = factor(status)))+\n  geom_bar(position = position_dodge())+\n  theme_bw()\n\n\n\n\nWell, not quite. Besides, the fact that the plot with our counts is not particularly revealing, we have two problems with that:\nThe first one is that â€œstatus = 0â€ does not always mean a survival. First, the person might be carried away by the waves and disappear. We lost the person and we donâ€™t know itâ€™s status. It might have died, it might have been saved by a fisher boat or is still alive somewhere in the ocean. We are not sure. The only thing what we are sure about is that this person can not be counted as dead or survived. Secondly, if some people survived 10 days, they might die at day 11. So, the time plays a role here and brings us to the second problem.\nWe do not have the data for every of these 10 days. Only for 7. Moreover, if I did not eat anything for 5 days, my survival probability at day 5, where I certainly survived, is not 100%. It is lower, because hunger accumulates over time. Similarly, a decrease of my survival probability also accumulates over time, which can not be determined by a simple counting of survivors vs.Â not-survivors at a particular day.\nDespite this problems we still can analyse these data. We only need to find the right method for it.\nLooking at the numeric variable â€œtimeâ€, we might be tempted to use a linear regression to model the survival time for two different status groups, 0 & 1:\n\n\nm <- lm(time ~ status, d %>% mutate(status = factor(status)))\n\nlibrary(effects)\nplot(allEffects(m))\n\n\n\n\nBut when we plot the model results, we realize that something isnâ€™t quite right there. All the information we have to model the survival time are zeros and ones. And the average time of dying (status = 1) of 2.5 days misses lotâ€™s of information. For instance, half of the people (3 persons) died on the very first day! And the further we go in time, the less additional people die, which we can see on the very first counts-plot above. So, the survival time is not really linear and can certainly not be described with only zeros and ones! Thus, a linear model does not seem to be useful here. Then what is?\nSince we have zeros and ones in the status column we can use a logistic regression, right?! Yes! Especially because probability curve is not linear and will catch the trend of survival over time. Cool! Letâ€™s go with it:\n\n\nm <- glm(status ~ time, d, family = binomial)\n\nlibrary(sjPlot)\nplot_model(m, type = \"pred\", ci.lvl = NA)\n\n\n$time\n\n\nNow we have the non-linear survival probabilities :), which is a great improvement as compared to a linear regression. However, the fifth observation has a status of 0 at day 2. This means that the person survived for 2 days for sure, but was lost after it. â€œSo what?â€, - you might ask. The probabilities in logistic regression would count this person as a 100% survivor after the second day simply because itâ€™s status is not â€œ=1â€, which is wrong (or biased), since we donâ€™t know whether this person survived. Thus, logistic regression overestimates survival probability and is therefore also an inappropriate tool to analyze survival data.\nSo, it looks like we canâ€™t analyse survival data with classic methods. And that is exactly why we need survival analysis method.\nWhile linear regression models describe the time, but miss a non-linear survival probability, logistic regression catches a non-linear trend, but overestimates survival probability. In contrast, survival analysis solves both issues, since it models non-linear survival probabilities over time while accounting for lost subjects of the study which are either dead nor survived.\nDeath is not the only option! Or: â€œWhat is an event?â€\n\nImage by Luke Southern on unsplash.\nSurvival time analysis is necessary in any study which investigates the time to a particular outcome of interest. Cancer studies in the medicine and the first failure of the car in the engineering field (failure time analysis) are good examples. The outcome of interest could be death, remission to relapse, progression, or failure. Point in time of reaching that outcome is generally called the event. Thank goodness, not every â€œeventâ€ is fatal ðŸ˜ƒ, but can sometimes even be a favorable outcome such as discharge from hospital. And thus, survival analysis is also a generic term, because it is not only about survival.\nThe event, as a final fixed point in time is needed because we canâ€™t observe or experiment forever. If we study lung cancer, we canâ€™t wait until patients die from other causes, e.g.Â being old. Only when some of patients survive cancer, or some cars donâ€™t break by a certain time (event), we can get valuable insights. For instance, what is the time from the start of the treatment to progression, or what is the probability to survive lung cancer after exactly 1 year? Moreover, it opens the possibility to compare survival or failure times among different groups, e.g.Â lung cancer between smokers and non-smokers, or breakdown time between German and Korean cars.\nSo, the survival time has a start and a finish point - the event. But what if a patient withdraw from the study due to a personal reasons, or a car got stolen 1 day before the event? Will they â€œsurviveâ€ by the time of the event? We donâ€™t know! But they certainly survived from the start of the study until the point we lost them. And this is a valuable information we surely want to include in our analysis! But how do we differentiate â€œlostâ€ survivors from the â€œrealâ€ survivors? Well, we just call them a new name - censored. StrÐ°nge word to apply to a person, right? But as I thought about the inappropriate information in a book or swearing on TV, which is often getting censored, the concept of censoring a patient became more digestible to me. See, since we can not say that person has died or is still alive, simply because we DO NOT HAVE ENOUGH INFORMATION, both of conclusions would be inappropriate, and thus, we censor this person. The concept of censoring is so important, that it deserves an extra chapter.\nCensoring\n\nThe need for censoring arises from the fact that the actual survival times will be unknown for some individuals. There are lots of ways to loose a subject of study:\nwe can loose an individual for no reason, when it simply does not appear anymore\nthe individual may experience another event, e.g.Â death or accident\neven patients who survived till the very end of the study can be treated as censored due the unknown survival time (I personally prefer to see these patients as survived)\nAll examples above are considered to be right censoring due to their direction from left to right on the time-axes. Most of survival data are right censored. There are two other kinds of censoring. Left censoring appears if we do not know where the sickness began, while interval censoring happens when the exact time of patient loss is not known, but only a time window. Both left and interval censorings are rare, difficult to analyse, are often a result of a bad study design, and thus, will not be covered here.\nImportant to remember is that censored observations still provide useful information! That is why they need to be included into analysis.\n\nHow to calculate Kaplan-Meier survival curve â€œmanuallyâ€ step by step\nImagine a peaceful sunny day right before the Titanic crash. No one has died, but all 10 people are at high risk (hazard) of death, they just donâ€™t know it yet. Guess, how many people would probably die the day before the crash? The answer is - probably zero. Literally, the probability of dying is zero, because we know the crash will not happen the day before the actual crush. To answer this question more properly, we need to remember the definition of probability: Probabilities are ratios of something happening, to everything what can happen. Thus, if nobody out of all 10 people died the day before the crash, the probability of dying is 0%, while the probability of surviving is 100%:\n\\[ probability \\ of \\ dying = \\frac{0}{10} = 0\\]\n\\[ probability \\ of \\ surviving = \\frac{10}{10} = 1  = 100\\%\\]\nFor a better representation, letâ€™s put all the numbers in one single table:\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\", \"10/10 = 1\"\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n\nThen, at the day of the crash, where all 10 people are now know that they are at high risk of dying, 3 of them have actually died. The probability of dying at the first day is then \\(3/10 = 0.3 \\approx 30\\%\\) and the probability of surviving is \\(7/10 = 0.7 \\approx 70\\%\\). As you can see, the survival can be also calculated from the probability of dying \\(1-0.3 = 0.7\\), which is sometimes very useful.\nSurvival probability\n\nOne important moment here is that the probability of surviving is cumulative, which means it accumulates day by day. This accumulation happens via multiplying the new probability of surviving day 1 \\((1 - \\frac{d_i}{n_i}) = (1 - \\frac{3}{10}) = 70\\%\\) by the old probability of surviving all the time before, which in our case is the day before the accident, or - day zero \\(S(t_{i-1}) = 100\\%\\):\n\\[ S(t_i) = S(t_{i-1})*(1 - \\frac{d_i}{n_i}) = 1 * (1 - \\frac{3}{10}) = 0.7 \\]\nWhere,\n\\(S(t_{iâˆ’1})\\) = the probability of being alive at \\(t_{iâˆ’1}\\)\n\\(n_i\\) = the number of patients alive just before \\(t_i\\)\n\\(d_i\\) = the number of events at \\(t_i\\)\n\\(t_0\\) = 0, \\(S(0)\\) = 1\nThe survival probability at a certain time, \\(S(t)\\), is conditional because the person needs to have survived beyond that certain time, e.g.Â the zero day (thatâ€™s the condition) in order to remain in the experiment for the first day.\n\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\",   \"10/10      = 1\",\n  1, 10, 3, \"3/10 = 0.3\", \"1 * (7/10) = 0.7\",\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n1\n10\n3\n3/10 = 0.3\n1 * (7/10) = 0.7\n\nThe second day is a little more interesting, because one person died and one simply disappeared (status = 0). This lost person was most likely carried away by the waves and hopefully was rescued. The hope is big, because the further people are scattered in the ocean, the higher the chances are that one of them will be found alive. And this one will let the world know that others are still out there. So, despite the fact that missing person seems bad, it might turn out to be a good thing. So, we certainly can not count a missed person as - dead. Besides, as a German proverb says: â€œHope is the last to dieâ€ ðŸ˜‰.\nBut since we are also not sure whether this person is still alive, we canâ€™t say - the person survived. That produces a dilemma: despite not being dead, the person is not part of our experiment anymore and we have to remove it from the number of people at risk. So, while the second day would have 7 people at risk left (since 3 have died at the very first day), the third day would be left with only 5 people, because 1 person died and one disappeared (was censored) as compared to day two. Similarly to the first day, the survival probability is cumulative and always includes the probability of the day before.\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\",    \"10/10       = 1\",\n  1, 10, 3, \"3/10 = 0.3\",  \"1 * (7/10)  = 0.7\",\n  2, 7,  1, \"1/7  = 0.14\", \"0.7 * (6/7) = 0.6\",\n  3, 5,  1, \"1/5  = 0.20\", \"0.6 * (4/5) = 0.48\"\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n1\n10\n3\n3/10 = 0.3\n1 * (7/10) = 0.7\n2\n7\n1\n1/7 = 0.14\n0.7 * (6/7) = 0.6\n3\n5\n1\n1/5 = 0.20\n0.6 * (4/5) = 0.48\n\nThe next two days two people disappeared, so, we censor them. There is no need to calculate something for censored data in survival analysis, because we donâ€™t know whether they survived or not. And since we are interested in either survival or death, the final table below contains only dead cases. At day 6 nobody died or disappeared, that is why we do not calculate anything for that day either. The last person in our experiment died at day 7. So, â€œN_diedâ€ will be 1, and â€œN_at_riskâ€ will be 2, since 1 out of remaining 5 has died and 2 were censored. Thus, the full â€œmanuallyâ€ calculated table will look like this:\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\",    \"10/10        = 1\",\n  1, 10, 3, \"3/10 = 0.3\",  \"1 * (7/10)   = 0.7\",\n  2, 7,  1, \"1/7  = 0.14\", \"0.7 * (6/7)  = 0.6\",\n  3, 5,  1, \"1/5  = 0.20\", \"0.6 * (4/5)  = 0.48\",\n  7, 2,  1, \"1/2  = 0.50\", \"0.48 * (1/2) = 0.24\"\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n1\n10\n3\n3/10 = 0.3\n1 * (7/10) = 0.7\n2\n7\n1\n1/7 = 0.14\n0.7 * (6/7) = 0.6\n3\n5\n1\n1/5 = 0.20\n0.6 * (4/5) = 0.48\n7\n2\n1\n1/2 = 0.50\n0.48 * (1/2) = 0.24\n\nBut what happens if we ignore censoring and simply calculate the probability of being not dead? Well, since 4 people did not die on day 7, the probability of survival would be \\(\\frac{4}{10} = 40\\%\\) which is almost twice as high, and thus weâ€™ll massively overestimate survival probability, as compared to the actual probability of 0.24 which accounts for censoring!\n\nStory time:\n\nSource\nOnly paying attention to survivors even has a name - survivorship bias, and there is a small story to it. During the Second World War some planes came back from the battle field with a lot of damage from bullets. They barely could fly, but they still came back. So, the military decided to protect the aircraft with more armor at the places where the most bullet-holes were, like wings, and reduce the armor at the places with no bullet-holes. Surprisingly, the percentage of planes which came back did not increase. The engineers were puzzled! Until one mathematician, Abraham Walt, which were invited to solve this problem said: â€œPut more armor on places with no bullet-holes, because if these place are shot, the plane wonâ€™t come back.â€ And as the others thought about it, they realized that all the planes which came back did not have any bullet-holes on the cockpit or the engines. The bullet-holes shows all the places where the aircraft can be shot but still come back, or survive! That is the survivorship bias. So, it seems to me, that logistic regression has a survivorship bias as compared to the survival analysis if we wanna analyse survival data.\n\nHow to compute Kaplan-Meier survival curve\nLoad all needed packages at once to avoid interruptions.\n\n\nlibrary(tidyverse)  # data wrangling and visualization\nlibrary(knitr)      # beautifying tables\nlibrary(car)        # for checking assumptions, e.g. vif etc.\nlibrary(broom)      # for tidy model output\nlibrary(sjPlot)     # for plotting results of log.regr.\nlibrary(sjmisc)     # for plotting results of log.regr.\nlibrary(effects)    # for probability output and plots\n\n\n\nFirst of all, please, be sure you know exactly what 0s and 1s in your data mean! Because in a logistic regression 1 is the survival, while in a survival analysis 1 is death! Secondly, we have to differentiate censored cases somehow, e.g.Â we can mark them with a plus sign in the data or on the plot. The data below shows that people were censored on days 2+, 4+, 5+ and 10+. Install and load a survival package to be able to execute the code below:\n\n\n# install.packages(\"survival\")\nlibrary(survival)\n\nSurv(time = d$time, event = d$status)\n\n\n [1]  1   1   1   2   2+  3   4+  5+  7  10+\n\nThe Surv function unites â€œtimeâ€ and â€œstatusâ€ data into a single â€œsurvivalâ€ object, which allows to account for censored observations. This object can then be used to model survival probability by the survfit function. We model the survival by adding â€œ~ 1â€ to the object, where 1 means no variables which could have influenced the survival. Our survival object on the left side of the tilde is then the response variable and on the right site of the ~ (tilde) are predictors, or in our case of â€œ1â€ - nothing. Letâ€™s produce our fist survival model and have a look at the model output:\n\n\nsurvival_model <- survfit(Surv(time, status) ~ 1, data = d)\n\n# small summary\nsurvival_model\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n      n  events  median 0.95LCL 0.95UCL \n     10       6       3       1      NA \n\nwhere we have:\nn - number of participants,\nevents - number of deaths, which are generically called event since you can study other types of â€œdeadlinesâ€ ðŸ˜‚, like relapse of the sickness, or first break of the car,\nthe median survival time estimated instead of the mean due to a non-parametric (explained later) nature of survival analysis and\n95% confidence intervals for the median survival time\nMuch more information can be assessed by the summary function, which, surprisingly ðŸ˜‰, delivers the same table which we just calculated above manually:\n\n\n# big summary\nsummary(survival_model)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     10       3     0.70   0.145       0.4665        1.000\n    2      7       1     0.60   0.155       0.3617        0.995\n    3      5       1     0.48   0.164       0.2458        0.938\n    7      2       1     0.24   0.188       0.0515        1.000\n\nNow you can be sure, our calculations were correct and you have successfully learned how to do survival analysis. Congrats! So that from now on you can start using software. It has a lot of advantages! For instance, a summary function, which only displays the result of dead cases, can be filled with an additional argument censored = TRUE, which displays all, dead and censored cases:\n\n\nsummary(survival_model, censored = T)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     10       3     0.70   0.145       0.4665        1.000\n    2      7       1     0.60   0.155       0.3617        0.995\n    3      5       1     0.48   0.164       0.2458        0.938\n    4      4       0     0.48   0.164       0.2458        0.938\n    5      3       0     0.48   0.164       0.2458        0.938\n    7      2       1     0.24   0.188       0.0515        1.000\n   10      1       0     0.24   0.188       0.0515        1.000\n\nGreat! Right? But what if you have thousands of days (your table would be huge!), but you are interested in only a few of them? Well, you can specify the results of which day you want to see by adding a times = ... argument to the summary function. A cool ðŸ˜Ž thing about it is that you can even ask for the days, where no data were available for, e.g.Â day 8 or 9 in our simple example. For instance, the probability of survival beyond day 8 is 24%:\n\n\nsummary(survival_model, times = c(8, 9))\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    8      1       6     0.24   0.188       0.0515            1\n    9      1       0     0.24   0.188       0.0515            1\n\nNow, since we know the exact probability of survival on each day, even on days we did not have the data for, we can visualize the model results. Install and load a survminer package to be able to execute the code below:\n\n\n# install.packages(\"survminer\")\nlibrary(survminer)\n\nggsurvplot(\n  survival_model, \n  conf.int = FALSE, \n  surv.median.line = \"hv\",\n  xlab = \"Days\", \n  ylab = \"Survival probability\", \n  break.time.by = 1,\n  risk.table = T\n)\n\n\n\n\nDespite the fact that we could have easily calculated the probabilities for every day, youâ€™ll never do it by hand. It could be thousands of day. Thus, youâ€™ll let the software do the work and youâ€™ll get much more results then you can and want calculate yourself, e.g.Â confidence intervals (CIs) for everyday survival or the survival plot. The CIs are visualized per default, but we can remove them if needed via the conf.int = FALSE command. The last row in the code above displays a â€œNumber at riskâ€ table, which we also calculated manually above. So, why did we calculate something manually at all then, if we can get everything and more from the software? Well, it was important to go through the calculation process step-by-step in order to increase your intuition about the survival analysis curve!\nInterestingly, this curve was independently described by two different scientists at the same time. Edward Kaplan and Paul Meier then published their findings together as the Kaplan-Meier (KM) estimator  in 1958.1 And survfit function fits type = \"kaplan-meier\" curve by default.\nMost of the survival data shows a lot of events in the beginning and lower number of events throughout the time. This makes the survival curve non-linear and most the survival data skewed or non-normally (not bell shaped) distributed. These are the reasons why KM method estimates median survival time instead of the mean as a measure of central tendency. The non-linearity and the â€œstepinessâ€ of the KM curve make it impossible to summarize the data into one single parameter, e.g.Â the slope, which makes the KM method non-parametric.\nInterpretation of Kaplan-Meier Curve\nThe x-axis represents time in days, and the y-axis shows the probability of surviving or the proportion of people surviving. So, the curve itself shows the exact survival probability over time. A vertical drop in the curve indicates at least one event. The height of a vertical drop shows the change in cumulative survival probability. A horizontal part of the curve represents survival duration for the certain time interval, which is terminated by the next event (and drop of the curve). The KM curve looks like a strange staircase with uneven steps, where survival probability is constant between the events, and is therefore a step function that changes value only at the time of each event. In this way each patient contributes valuable information to the calculations for as long as it is alive. Censored people are shown exactly like in the survival object, with pluses, as you can see on the day 2. However, most of the pluses look like vertical ticks, since they lie on the horizontal part of the curve, i.e.Â days 4 and 5. The tick marks are shown by default, but could be suppressed using the argument censor = FALSE. The risk table below the plot shows the number of people at risk, which are actually all â€œreally aliveâ€ people in the experiment which did not experience the event or censoring at a particular time point. Dashed line represents the median survival time which corresponds to a survival probability of 50%. And if we ignore censoring and simply estimate the median of time (for only dead people), weâ€™ll get 1.5 instead of 3, which will increase the survival provability from 48% to 70%. Again, ignoring censoring will result into overestimation of survival probability due to a survivorship bias.\n\n\nd %>% \n  filter(status == 1) %>% \n  summarize(median_survival_whithout_censoring = median(time))\n\n\n# A tibble: 1 x 1\n  median_survival_whithout_censoring\n                               <dbl>\n1                                1.5\n\nComparing survival of groups\n2 groups\nNow, letâ€™s follow 100 people after Titanic crash instead of 10 and look at their survival using the Kaplan-Meier curve below. The numbers in the risk table are getting bigger and weâ€™d better display the percentages at risk in brackets near the absolute values. The risk.table = \"abs_pct\" argument helps with that. The â€œroundâ€ example of 100 people gives us exactly the same percentages as the absolute numbers, however with a less â€œroundâ€ number or several groups the percentages would become very useful. The median survival time of people in the cold ocean is around 70 days and the confidence intervals arenâ€™t very wide, so that we can be pretty confident in our numbers:\n\n\nset.seed(999) # for reproducible example\nd <- ggstatsplot::Titanic_full %>% \n  mutate(survived = ifelse(Survived == \"No\", 1, 0),\n         time     = runif(n=2201, min=1, max=100)) %>% \n  sample_n(100) \n\nm <- survfit(Surv(time, survived) ~ 1, data = d)\n\nggsurvplot(m, \n           conf.int = TRUE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\")\n\n\n\n\nBut can we have two survival curves on the same plot and compare them somehow? Of coarse! In fact, that is the moment where the fun starts. For instance, we can compare survival probabilities of males vs.Â females. For this we only need to replace ~1 in the model formula with the name of a categorical variable of interest, e.g.Â sex. Survival plot then displays a Kaplan-Meier curve for every category of your variable:\n\n\nm <- survfit(Surv(time, survived) ~ Sex, data = d)\n\nggsurvplot(m, \n           pval = TRUE, \n           conf.int = TRUE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\") \n\n\n\n\nInterpretation of groups comparison using 4 benchmarks\nthe visual comparison of curves: just by looking at two groups we can say whether there is a difference. For instance, our plot reveals that females have higher probability of survival almost thought the whole time period (x-axes). However, whether this difference is statistically significant requires a formal statistical test. We could go further and look at the numbers of the curve, for instance: at the sunny and beautiful day before the crush, the survival probability of both groups is 1.0 (or 100% of the passengers are alive). At day 50, the probability of survival of females is ca. 0.75 (or 75%) and only ca. 0.65 (or 65%) for males. At day 75, the survival is ca. 60 and ca. 30% accordingly.\ncomparison of confidence intervals (CIs): overlapping CIs show that survival of males and females is not too different and could be due to chance all the way to ca. 80 days. After 80 days CIs stop overlapping which suggests significant difference in survival at a particular time point, say 90 days.\nestimated median survival times reveals more then confidence intervals. The median survival time for each group represents the time at which the survival probability is 50%. For instance the median survival of Females is much higher (87 days) then males (65 days). Such a huge difference of 23 days sounds significant to me, because females have 23 more days to be found by some fishing boat. Here again, only test can tell and that is why the last benchmark for comparing two groups is the p-value estimated by a Log-Rank test (Peto et al, 1977), which is so important, that it deserves an extra chapter.\n\n\nm\n\n\nCall: survfit(formula = Surv(time, survived) ~ Sex, data = d)\n\n            n events median 0.95LCL 0.95UCL\nSex=Female 31     10   87.0    60.9      NA\nSex=Male   69     54   64.5    53.6    72.9\n\nLog-Rank test\nA non-parametric Log-Rank (sometimes called Mantel-Haenszel) statistical test compares median survival times of groups. Log-Rank test is similar to a (1) non-parametric Wilcoxon-Rank test, which also compares medians using ranks and it is also similar to a (2) Chi-square test, where we compare the observed number of events to the expected ones by calculating Chi-square statistic:\n\\[ X^2 = \\sum_{i = 1}^{g} \\frac{(O_i - E_i)^2}{E_i}\\]\nThe expected numbers of events are calculated for each time point and each group as compared to the previous time point. These values are then summed over all time points to give the total expected number of events in each group.\nThe non-parametric nature of the test makes no assumptions about the survival distributions. So, is survival time normal (â€œbell curvyâ€)? It can be, but does not have to! In fact, survival data are very rarely normally distributed, but are often skewed due to a typically many early events and relatively few late ones.\nThe null hypothesis of the Log-Rank test is that there is no difference in survival between the two groups. The p-value of 0.011 allows us to reject the null hypothesis and indicates a significant median difference in survival time between females and males.\nThe Log-Rank test is soo widely used for comparing two or more survival curves, that you actually have to heavily justify the usage of any other test.\nThe function survdiff() computes Log-Rank test and returns following components:\nthe number of subjects in each group.\nthe weighted observed number of events in each group.\nthe weighted expected number of events in each group.\nthe Chi-square statistic for a test of equality\nand the p-value for the difference in survival among the groups\n\n\nsurvdiff(Surv(time, survived) ~ Sex, data = d)\n\n\nCall:\nsurvdiff(formula = Surv(time, survived) ~ Sex, data = d)\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\nSex=Female 31       10     19.3      4.45      6.46\nSex=Male   69       54     44.7      1.92      6.46\n\n Chisq= 6.5  on 1 degrees of freedom, p= 0.01 \n\nIs the Log-Rank test perfect? Unfortunately, no. One problem is: it does not provide an effect size which leaves us with only p-value as a measure of difference. Another problem with Log-Rank test (and Kaplan-Meier method in general) is that it does not allow confounders, it ignores other factors/variables. Fortunately, we still can compare more then two groups of a single variable ðŸ˜‰.\n> 2 groups and multiple pairwise (post-hoc) Log-Rank test\nLog-Rank test can compare more then two groups and say whether there is a significant (p-value < 0.05) difference among these groups. The test results displayed in the table and KM curve below show that there is a significant difference in survival (p-value = 0.024) among groups of people in different ticket classes. However, like the most other tests (e.g.Â ANOVA) it does not say between which groups exactly. That is why we need an additional analysis which pairwisely compares each group to each other group. Such analysis is often called a post-hoc.\n\n\nsurvdiff(Surv(time, survived) ~ Class, data = d)\n\n\nCall:\nsurvdiff(formula = Surv(time, survived) ~ Class, data = d)\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\nClass=1st  14        4    11.03     4.477     5.532\nClass=2nd  13        5     8.49     1.432     1.666\nClass=3rd  37       30    21.81     3.074     4.760\nClass=Crew 36       25    22.68     0.238     0.383\n\n Chisq= 9.5  on 3 degrees of freedom, p= 0.02 \n\nm <- survfit(Surv(time, survived) ~ Class, data = d)\n\nggsurvplot(m, \n           pval = TRUE, \n           conf.int = FALSE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\",\n           ncensor.plot = TRUE,\n           break.time.by = 20,\n           risk.table.y.text.col = TRUE, risk.table.y.text = FALSE) \n\n\n\n\nThree further useful plot arguments which can help to better visualize survival data are:\nbreak.time.by = 20 break the x-axis into wishful time intervals.\nrisk.table.y.text.col = TRUE and risk.table.y.text = FALSE plots bars instead of names in text annotations of the legend of risk table\nncensor.plot = TRUE displays the number of censored subjects, which helps to understand what is the cause that the risk number becomes smaller: the event or censoring.\nInterpretation\nThe median survival is ca. 90 days for the passengers in the 1st and 2nd classes, 58 days for the 3rd class and 65 days for the crew of the Titanic, suggesting a good survival of rich people, as compared to the rest. A low p-value (p = 0.024) suggests that there is a significant difference in survival among groups. Among which, only post-hoc can tell. The results of such post-hoc Log-Rank analysis can be conducted with pairwise_survdiff function form the survminer package and are displayed below. As usual, if we have multiple comparisons, we run into a risk of making a false discovery or missing an important discovery. Thus, we have to adjust the p-values in order to reduce the probability of making an error. I personally prefer the Benjamini & Hochberg (1995) adjustment method to the famous but conservative Bonferroni method:\n\n\npairwise_survdiff(\n  formula = Surv(time, survived) ~ Class, data = d, p.adjust.method = \"fdr\"\n  )\n\n\n\n    Pairwise comparisons using Log-Rank test \n\ndata:  d and Class \n\n     1st   2nd   3rd  \n2nd  0.414 -     -    \n3rd  0.028 0.139 -    \nCrew 0.095 0.317 0.414\n\nP value adjustment method: fdr \n\nThe results of the post-hoc analysis revealed that the survival of the 1st class is significantly higher as compared to the 3rd class and the Crew. And despite the fact that the median survival of the 2nd class passengers is very similar to the first, the confidence intervals of this survival are wide and do overlap with other groups a lot (not shown to avoid clutterness). That is why the 2nd class passengers do not generally have significantly higher survival time despite much higher median survival time.\nMultiple survival curves\nAs mentioned above, the Log-Rank test can not be applied to several variables. However, we still can plot survival curves of several variables in order to get some intuition for our data. Further methods, e.g.Â Cox models, are able to estimate difference among several variables, but they are more complex than the KM-method and thus will be covered in future posts.\n\n\nm2 <- survfit( Surv(time, survived) ~ Sex + Class, data = d )\n\nggsurv <- ggsurvplot(m2, conf.int = TRUE)\n   \nggsurv$plot +theme_bw() + \n  theme (legend.position = \"right\")+\n  facet_grid(Sex ~ .)\n\n\n\n\nThe intuition we can get from the plots above is that rich women (1st and 2nd ticket classes) and (only 2) women from the crew have 100% probability of survival, while pure women in the 3rd class will die with a similar certainty as men. Heavily overlapping CIs of men survival suggests that all man will eventually die after the Titanic accident.\nConclusions\nSurvival analysis investigates the time it takes for an event of interest to occur. Most of the univariate (single variable) survival analyses uses Kaplan-Meier plots to visualize the survival curves and Log-Rank test to compare the survival curves of two or more groups.\nAdvantages:\nThe crucial advantage of survival probability curve vs.Â logistic regression curve is accounting for censored data, which are neither dead, nor alive. Logistic regression treats all the people who didnâ€™t die as survived, which is wrong, simply because we donâ€™t know the survival status of a missed person. Patients leave the study due to two main reasons, they either fill so bad, that they donâ€™t care about your experiment anymore, or they feel much better and forget your study. Some patients may simply move to the other city without saying anything. Counting all of them as survived, as logistic regression does, would overestimate the survival probability (survivorship bias) and underestimate the hazard of death. The survival analysis is therefore more precise as compared to a logistic regression, while it still catches a non-linear trend in probabilities.\nAnother advantage is the non-parametric nature of the Kaplan-Meier method, which does not have too many assumptions. In fact the only important assumption is that censoring should be non-informative. Why? More information is always better, right? Yes! But if we know why people leave the study, we could use this information as a new variable and study itâ€™s influence on survival. The KM method would then be inappropriate, because it will miss this information. However, often we donâ€™t know (no-info) why people leave (are censored). And in this case the KM-method squeezes the most inference out of such non-informative data.\nDisadvantages:\nThe Kaplan-Meier â€œcurveâ€ does not actually look like a curve. Oppositely to the logistic regression Kaplan-Meier method can not be described as a smooth function (curve) by a few parameters, e.g.Â the slope or odds-ratio. That is why we need the fool table of results or a graph.\nKaplan-Meier method canâ€™t model numeric variables, but only categorical.\nKaplan-Meier method canâ€™t include many explanatory variables. Itâ€™s bad, because comparing groups in terms of survival may miss the effect of other factors, known as covariates or confounders, which could potentially affect the survival time of a particular group.\nRecommendations:\nalways display statistical uncertainty by including 95% CIs or/and a p-value of the Log-Rank test. Displaying CIs at a few important time points on the plot for each treatment group may sometimes be clearer then displaying them for all time points. Non-overlapping CIs indicate significant difference between groups.\nconsider cutting the x-axis. Why? Well, the eye is naturally drawn to the right part of the plot, where the time ends. However, the end of the plot contains the least amount of information and greatest uncertainty due to just a low number of remaining participants. How far in time to extend the plot? Itâ€™s up to you.\nalways display the risk table showing the numbers of patients event-free and still in follow-up in each treatment group at relevant time points.\nWhatâ€™s next?\nSeveral methods can address the disadvantages of the Kaplan-Meier method. Particularly, exponential parametric models provide a smooth function which is able to describe the survival curve as an actual curve in a few parameters, like slope (thatâ€™s why - parametric). And Cox-Proportional-Hazard model can be extended to several variables (in progress).\nIf you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.\nThank you for learning!\nFurther readings, videos and references\nClark, T., Bradburn, M., Love, S., & Altman, D. (2003). Survival analysis part I: Basic concepts and first analyses. 232-238. ISSN 0007-0920.\nthe whole playlist of videos from MarinStatsLectures! They are amazing! https://www.youtube.com/watch?v=vX3l36ptrTU&list=PLqzoL9-eJTNDdnKvep_YHIwk2AMqHhuJ0\nsurvminer cheat sheet: https://rpkgs.datanovia.com/survminer/survminer_cheatsheet.pdf\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#part_1:_introduction_to_survival_analysis\n\nKaplan, E.L. and Meier, P. (1958) Nonparametric Estimation from Incomplete Observations. Journal of the American Statistical Association, 53, 457-481. http://dx.doi.org/10.1080/01621459.1958.10501452â†©ï¸Ž\n",
    "preview": "posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/thumbnail_survival_1.png",
    "last_modified": "2021-01-03T23:22:22+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-02-r-package-reviews-janitor-clean-your-data/",
    "title": "R package reviews {janitor} clean your data!",
    "description": "Data Scientists spend up to 80% of their time cleaning and preparing data for analysis. \" Happy families are all alike; every unhappy family is unhappy in its own way\" â€” Leo Tolstoy. \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way\" - Hadley Wickham. Thats when \"janitor\" helps to clean the mess.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-02",
    "categories": [
      "R package reviews",
      "videos"
    ],
    "contents": "\n\nContents\nGet dirty data and look at it\nR demo for how to clean your data\nMain functionsclean_names()\nremove_empty() & remove_constant()\nget_dupes()\nround_to_fraction()\nconvert_to_date()\nrow_to_names()\n\nGenerate â€œadorableâ€ frequency table (1-, 2-, or 3-way).\n\n\n\nlibrary(tidyverse)          # data manipulation\nlibrary(janitor)            # data cleaning\nlibrary(readxl)             # data importing\nlibrary(kableExtra)         # beautifying tables\n\n\n\nGet dirty data and look at it\nIf you are reading this post, you are probably already familiar with the concept of tidy data. If not, have a look at it. And you have most likely already worked with the messy (dirty) data. I did, and that is why I found the janitor package sooo useful!\nThe messy data displayed below can be found here. Some of the indicators of messyness are: strange (difficult) names, empty columns and rows, constant columns, which do not provide much of a value, duplicates, strange dates which do not look like dates etc..\n\n\ndirty_data <- read_excel(\"dirty_data.xlsx\")\ndirty_data %>%\n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nFirst Name\n\n\nLast Name\n\n\nEmployee Status\n\n\nSubject\n\n\nHire Date\n\n\n% Allocated\n\n\nFull time?\n\n\ndo not edit! â€”>\n\n\nCertificationâ€¦9\n\n\nCertificationâ€¦10\n\n\nCertificationâ€¦11\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nInstr. music\n\n\nVocal music\n\n\nNA\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nComputers\n\n\nNA\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nEnglish 6-12\n\n\nNA\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPhysical ed\n\n\nNA\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nPolitical sci.\n\n\nNA\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nNA\n\n\nVocal music\n\n\nEnglish\n\n\nNA\n\n\nR demo for how to clean your data\nA short (ca. 12 min) video below shows how to clean this data and the code youâ€™ll see in the video is provided below.\n\n\n\n\n\n\n\nMain functions\nclean_names()\n\nThis function removes all the non-letters and signs from the names and connects several words with underscores. You can ignore the kbl() and kable_classic_2() rows in the code below, they just make the HTML table look clean, but do not really clean anything in our dataset.\n\n\nd <- dirty_data %>% \n  clean_names()\n\nd %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ndo_not_edit\n\n\ncertification_9\n\n\ncertification_10\n\n\ncertification_11\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nInstr. music\n\n\nVocal music\n\n\nNA\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nComputers\n\n\nNA\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nEnglish 6-12\n\n\nNA\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPhysical ed\n\n\nNA\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nPolitical sci.\n\n\nNA\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nNA\n\n\nVocal music\n\n\nEnglish\n\n\nNA\n\n\nremove_empty() & remove_constant()\nremove_empty() removes both empty rows and empty columns. We have two empty columns and one empty row. They are just useless. Lets add two constant columns to the dataset and see how we can remove all this junk with janitor.\n\n\n# add two constant columns\nd <- d %>% \n  mutate(constant_column   = 42,\n         constant_column_2 = \"text\")\n\n# remove the junk\nd %>% \n  remove_constant() %>%  \n  remove_empty() %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ncertification_9\n\n\ncertification_10\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nPhysical ed\n\n\nTheater\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nPhysical ed\n\n\nTheater\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nInstr. music\n\n\nVocal music\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nPENDING\n\n\nComputers\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nPENDING\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nScience 6-12\n\n\nPhysics\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nScience 6-12\n\n\nPhysics\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nEnglish 6-12\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nPENDING\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nPhysical ed\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPolitical sci.\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nVocal music\n\n\nEnglish\n\n\nget_dupes()\nYou can hunt duplicates rows in several columns. The function also returns the counts for every duplicate.\n\n\nd %>% \n  get_dupes(first_name) %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\ndupe_count\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ndo_not_edit\n\n\ncertification_9\n\n\ncertification_10\n\n\ncertification_11\n\n\nconstant_column\n\n\nconstant_column_2\n\n\nChien-Shiung\n\n\n2\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\n42\n\n\ntext\n\n\nChien-Shiung\n\n\n2\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\n42\n\n\ntext\n\n\nJason\n\n\n2\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\n42\n\n\ntext\n\n\nJason\n\n\n2\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\n42\n\n\ntext\n\n\nround_to_fraction()\nThis can be very useful if you have lots of data. I do work with agricultural animals and they have all kinds of scores. One of them - Body Condition Score (BCS) is always recorded in quarters, e.g.Â 2.25, 2.5, 2.75 etc. So, if some colleagues try to be very smart and very exact, they stupidly record values like 0.8 or 3.149 instead of wanted 0.75 and 3, you need to correct this. round_to_fraction() simplifies this task enormously! Have a look at the last number below, before and after applying round_to_fraction() to a variable.\n\n\n# before\nd$percent_allocated\n\n\n [1] 0.75 0.25 1.00 1.00 1.00 0.50 0.50   NA 0.50 0.50   NA   NA 0.80\n\n# after\nround_to_fraction(d$percent_allocated, denominator = 4) # digits = 3\n\n\n [1] 0.75 0.25 1.00 1.00 1.00 0.50 0.50   NA 0.50 0.50   NA   NA 0.75\n\nconvert_to_date()\nA modern Excel always tries to automate things, and I hate it! ðŸ˜‚ For instance you write a number into a cell and it sometimes immediately converts it into date. Then you try to have a date in a cell, and it returns a number. Moreover, Excel also has some strange date encoding systems, which can be confused with a normal numeric columns. Luckily, our dirty dataset has a â€œdateâ€ word in the name of a column â€œhire_dateâ€, otherwise we wouldnâ€™t know that it is a date:\n\n\nd$hire_date\n\n\n [1] 39690 39690 37118 27515 41431 11037 11037    NA 32994 27919 42221\n[12] 34700 40071\n\nconvert_to_date(d$hire_date)\n\n\n [1] \"2008-08-30\" \"2008-08-30\" \"2001-08-15\" \"1975-05-01\" \"2013-06-06\"\n [6] \"1930-03-20\" \"1930-03-20\" NA           \"1990-05-01\" \"1976-06-08\"\n[11] \"2015-08-05\" \"1995-01-01\" \"2009-09-15\"\n\nrow_to_names()\nPeople often have several header columns, in order to beautifully explain everything, make things accurate and donâ€™t miss any important information. Iâ€™ve been there too. But as soon as I started to work with software, I realized that this â€œbeautyâ€ hurts. I get a lot of Excel datasets like that, which is not a problem, I just delete not needed rows and continue working. But then I get the same table with a few corrected values, even if I tell colleagues to have only one header. That is why I was pleased to discover row_to_names() function. Have a look at the dataset â€œxâ€ below and how easy can we handle it.\n\n\nx <- data.frame(\n  X_1 = c(\"some general description\", \"Real title\", 1:3),\n  X_2 = c(\"something `very!!! important` :) \", \"Which we wont!\", 4:6))\n\nx\n\n\n                       X_1                               X_2\n1 some general description something `very!!! important` :) \n2               Real title                    Which we wont!\n3                        1                                 4\n4                        2                                 5\n5                        3                                 6\n\nx %>%\n  row_to_names(row_number = 2)\n\n\n  Real title Which we wont!\n3          1              4\n4          2              5\n5          3              6\n\nGenerate â€œadorableâ€ frequency table (1-, 2-, or 3-way).\nThe table() function is actually cool, but as I discovered tabyl() in the janitor packages, I couldnâ€™t go back. First of all, I always need to explicitly write , useNA = â€œifanyâ€ if I wanna see whether there NAs. Secondly the proportions have to be called with an extra function prop.table() on top of the table() function. Now, look at what tabyl() does:\n\n\n# old way\ntable(d$employee_status, useNA = \"ifany\")\n\n\n\nAdministration          Coach        Teacher           <NA> \n             1              2              9              1 \n\nprop.table(table(d$employee_status))\n\n\n\nAdministration          Coach        Teacher \n    0.08333333     0.16666667     0.75000000 \n\n# new way\ntabyl(d$employee_status) # \"show_na\" is TRUE by default\n\n\n d$employee_status n    percent valid_percent\n    Administration 1 0.07692308    0.08333333\n             Coach 2 0.15384615    0.16666667\n           Teacher 9 0.69230769    0.75000000\n              <NA> 1 0.07692308            NA\n\n# new way with two variables\nd %>% \n  tabyl(employee_status, full_time)\n\n\n employee_status No Yes NA_\n  Administration  0   1   0\n           Coach  2   0   0\n         Teacher  3   6   0\n            <NA>  0   0   1\n\nMoreover, along the counts janitorâ€™s tabyl can also display totals, formatted percentages and even all of them together by using a family of â€œadorableâ€ functions.\n\n\n#  \nd %>%\n  tabyl(employee_status, full_time) %>%\n  adorn_totals(c(\"col\", \"row\"))\n\n\n employee_status No Yes NA_ Total\n  Administration  0   1   0     1\n           Coach  2   0   0     2\n         Teacher  3   6   0     9\n            <NA>  0   0   1     1\n           Total  5   7   1    13\n\nd %>%\n  tabyl(employee_status, full_time) %>%\n  adorn_totals(\"row\") %>%\n  adorn_percentages(\"row\") %>%\n  adorn_pct_formatting() %>%\n  adorn_ns(position = \"front\") %>%               \n  adorn_title() %>% \n  adorn_title(\"combined\")\n\n\n employee_status/full_time  full_time                      \n           employee_status         No        Yes        NA_\n            Administration 0   (0.0%) 1 (100.0%) 0   (0.0%)\n                     Coach 2 (100.0%) 0   (0.0%) 0   (0.0%)\n                   Teacher 3  (33.3%) 6  (66.7%) 0   (0.0%)\n                      <NA> 0   (0.0%) 0   (0.0%) 1 (100.0%)\n                     Total 5  (38.5%) 7  (53.8%) 1   (7.7%)\n\n\n\nmtcars %>%\n  tabyl(am, cyl) %>%\n  adorn_totals(c(\"col\", \"row\")) %>% \n  adorn_percentages(\"all\") %>% \n  adorn_pct_formatting() %>%\n  adorn_ns() \n\n\n    am          4         6          8       Total\n     0  9.4%  (3) 12.5% (4) 37.5% (12)  59.4% (19)\n     1 25.0%  (8)  9.4% (3)  6.2%  (2)  40.6% (13)\n Total 34.4% (11) 21.9% (7) 43.8% (14) 100.0% (32)\n\nYou could also compare columns of two dataframes to see the difference. To see more function, type ?janitor in the console of RStudio, scroll down and press index.\n\n\nd1 <- d %>% \n  mutate(new_column = 42, \n         second_new = \"into it\")\n\ncompare_df_cols(d, d1) %>% View()\n\n\n\nEvery single function from janitor package makes your life easier and more productive for a moment, some of them a lot easier, e.g.Â clean_names() and remove_empty(). But the real power of it accumulates over time, because you free your mind and time for creative work, instead of solving problems. Thus, thousand thanks to package-developer Sam Firke!\n\n\ncitation(\"janitor\")\n\n\n\nTo cite package 'janitor' in publications use:\n\n  Sam Firke (2020). janitor: Simple Tools for Examining and\n  Cleaning Dirty Data. R package version 2.0.1.\n  https://CRAN.R-project.org/package=janitor\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {janitor: Simple Tools for Examining and Cleaning Dirty Data},\n    author = {Sam Firke},\n    year = {2020},\n    note = {R package version 2.0.1},\n    url = {https://CRAN.R-project.org/package=janitor},\n  }\n\nWhat is you favorite R package?\nIf you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.\n\n\n\n",
    "preview": "posts/2021-01-02-r-package-reviews-janitor-clean-your-data/9.png",
    "last_modified": "2021-01-15T22:52:28+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/",
    "title": "How to visualize models, their assumptions and post-hocs",
    "description": "A picture is worth a thousand words! This article shows how to visualize results of 16 different models in R: from a simple linear model to a multiple-additive-non-linear-mixed-effects model. Among them are logistic, multinomial, additive and survival models with and without interactions. **Goal: minimum R code & maximum output!** We'll also go a bit beyond only model visualization. So, don't miss the bonuses ðŸ˜‰.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-01",
    "categories": [
      "visualization",
      "videos",
      "models"
    ],
    "contents": "\n\nContents\nA demo for the first 7 linear models1. Simple linear model with one categorical predictor\n2. Simple linear model with one numeric predictor\n3. Multiple linear model with several categorical predictors\n4. Multiple linear model with several numeric predictors\n5. Multiple linear model with numeric and categorical predictors\n\nBonus 1: check all the assumption in one line of code6. Multiple linear model with interactions\n\nBonus 2: easy post-hocs7. Multiple linear model with interactions bwtween numeric predictors\n\nBonus 3: quick multiple models with ggplot\nA demo for the next 9 models: non-linear, logistic, multinomial, mixed-effects, survivalâ€¦8. Multiple non-linear polynomial model with interactions\n9. Multiple non-linear Generalized Additive Models (GAM)\n10. Multiple logistic regression with interactions\n\nBonus 4: visualize the post-hoc analysis with the PairWise P-value Plot (pwpp)11. Multinomial logistic regression models via neural networks\n12. Multiple Linear Mixed-Effects-Model with interactions\n\nBonus 5: how to choose the best model13. GAMMs - Multiple Generalised Additive (non-linear) Mixed Effects Models\n14. Kaplan-Meier survival model\n15. Exponential Parametric Models\n16. Cox proportional hazard models\n\n\nA demo for the first 7 linear models\nTwo YouTube videos in this article demonstrate the code presented below and explain all you need to know. Thus, I reduced the text to a minimum in order to decrease redundancy. If you still have a question, drop me a comment on YouTube or on this article below. Iâ€™ll try to respond as quick as I can.\n\n\n\n\n\n\n\nLetâ€™s get our first data. Weâ€™ll use several dataset through out this article.\n\n\n# install.packages(\"tidyverse\") \n# install.packages(\"ISLR\") \nlibrary(tidyverse) # data wrangling\nlibrary(ISLR)      # get \"Wage\" dataset\n\n# reproducibility: the same seed grows the same tree\nset.seed(1)        \n\nd <- Wage %>%\n  sample_n(1000) %>% \n  rename(salary = wage) \n\n# have a look at the data\nglimpse(d)\n\n\nRows: 1,000\nColumns: 11\n$ year       <int> 2004, 2003, 2007, 2006, 2004, 2006, 2004, 2003, â€¦\n$ age        <int> 59, 45, 21, 49, 59, 41, 63, 42, 46, 42, 50, 26, â€¦\n$ maritl     <fct> 2. Married, 2. Married, 1. Never Married, 2. Marâ€¦\n$ race       <fct> 1. White, 1. White, 1. White, 1. White, 1. Whiteâ€¦\n$ education  <fct> 5. Advanced Degree, 4. College Grad, 2. HS Grad,â€¦\n$ region     <fct> 2. Middle Atlantic, 2. Middle Atlantic, 2. Middlâ€¦\n$ jobclass   <fct> 2. Information, 2. Information, 2. Information, â€¦\n$ health     <fct> 2. >=Very Good, 2. >=Very Good, 1. <=Good, 2. >=â€¦\n$ health_ins <fct> 1. Yes, 1. Yes, 2. No, 1. Yes, 1. Yes, 1. Yes, 1â€¦\n$ logwage    <dbl> 5.204120, 4.602060, 4.565848, 5.107210, 4.490520â€¦\n$ salary     <dbl> 182.02062, 99.68946, 96.14407, 165.20877, 89.167â€¦\n\n1. Simple linear model with one categorical predictor\nThe effects() package is the main visualization package weâ€™ll use. It can visualize results of almost any model. Weâ€™ll start with the visualization of predicted values.\n\n\nm <- lm(salary ~ jobclass, d)\n\n# install.packages(\"effects\")\nlibrary(effects)    # for model visualization & more\n\nplot(allEffects(m))\n\n\n\n\n2. Simple linear model with one numeric predictor\nHere weâ€™ll add grid = TRUE for a better readability, so that we donâ€™t need to stare on the y-axis and guess the result.\n\n\nm <- lm(salary ~ age, d)\n\nplot(allEffects(m), grid = TRUE)\n\n\n\n\n3. Multiple linear model with several categorical predictors\nWe can visualize all predictors at once, or any particular predictor from a multiple model individually.\n\n\nm <- lm(salary ~ jobclass + education, d)\n\nplot(allEffects(m))\n\n\n\nplot(predictorEffect(predictor = \"education\", mod = m))\n\n\n\n\n4. Multiple linear model with several numeric predictors\nHere weâ€™ll see how to change the appearance of confidence intervals and introduce another amazing model-visualization package - sjPlot. In this chapter weâ€™ll use the effects package for the visualization of the predicted values and sjPlot package for the visualization of the estimates with their confidence intervals.\n\n\nm <- lm(salary ~ age + year, d)\n\nplot(allEffects(m))\n\n\n\nplot(allEffects(m), confint=list(style=\"bars\"))\n\n\n\n# install.packages(\"sjPlot\")\nlibrary(sjPlot)    # for model visualization\nplot_model(m)\n\n\n\n\n5. Multiple linear model with numeric and categorical predictors\nWe can change design of the plot by determining the number of rows and columns in the plot(allEffects()). The plot_model() function can also display the numeric values of the estimates and the significance stars, which is often all we need. Besides, it looks much better than a table-looking output of the model results.\n\n\nm <- lm(salary ~ age + education, d)\n\nplot(allEffects(m))\n\n\n\n\n\n\nplot(allEffects(m), rows = 2, cols = 1)\n\n\n\n\n\n\nplot_model(m, show.values = TRUE)\n\n\n\n\nBonus 1: check all the assumption in one line of code\ncheck_model is just awesome! One of my favorite R functions! I get a â€œnerdgasmâ€ every time I use it ðŸ˜‚. The video explains it very well.\n\n\n# install.packages(\"performance\")\nlibrary(performance)    # model assumptions & performance\ncheck_model(m)\n\n\n\n\n6. Multiple linear model with interactions\nFirst of all, the allEffects() functions visualizes interactions easily! Secondly, we can put several lines on the same plot with or without confidence intervals by using argument multiline = TRUE. â€œTâ€ instead of â€œTRUEâ€ also works. The sjPlot package is not only able to also easily visualize interactions, but can in addition be extended with the usual ggplot2 syntax, which can greatly improve the appearance of the plot.\n\n\nm <- lm(salary ~ education * jobclass, d)\n\n# not too neat representation!\nplot(allEffects(m))\n\n\n\n# better representation\nplot(allEffects(m), lines = list(multiline = TRUE))\n\n\n\n\n\n\n# perfect representation\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, type = \"int\")+theme_blank()+theme(legend.position = \"top\")\n\n\n\n\nBonus 2: easy post-hocs\nThe emmeans package is one of my favorite for conducting post-hocs. In this chapter we only display the results in the text/table form. Later weâ€™ll also visualize post-hocs.\n\n\n# install.packages(\"emmeans\")\nlibrary(emmeans)      # for post-hocs\nemmeans(m, pairwise ~ jobclass | education, adjust = \"fdr\")$contrasts\n\n\neducation = 1. < HS Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information   0.7937 8.50 990  0.093  0.9257 \n\neducation = 2. HS Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information   0.0431 4.49 990  0.010  0.9923 \n\neducation = 3. Some College:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information  -8.7985 5.33 990 -1.651  0.0990 \n\neducation = 4. College Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information  -7.8854 4.94 990 -1.597  0.1105 \n\neducation = 5. Advanced Degree:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information -21.2809 8.24 990 -2.581  0.0100 \n\n7. Multiple linear model with interactions bwtween numeric predictors\n\n\nm <- lm(salary ~ age * health, d)\n\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, type = \"int\")+\n  theme_minimal()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\ncheck_model(m)\n\n\n\n\nBonus 3: quick multiple models with ggplot\nWe do not always need to explicitly model things in order to explore our data. A geom_smooth() function from ggplot2 package will automatically fit numeric data. Mostly with non-linear models (e.g.Â GAM), but the argument method=â€œlmâ€ can force it to take the linear one. Moreover, we could also use any formula inside of the geom_smooth(), but if we need to write the formula anyway, weâ€™d rather produce an explicit model. However, the code presented below is quick and easy, and therefore very practical! On this note weâ€™ll enter the world of non-linear models and I recommend to watch the second video first, before checking out the code.\n\n\nggplot(d, aes(age, salary))+\n  geom_point()+\n  geom_smooth()+    # the quickest way to model numeric data\n  facet_grid(education~jobclass, scales = \"free\") # quick multiple model \n\n\n\n\nA demo for the next 9 models: non-linear, logistic, multinomial, mixed-effects, survivalâ€¦\n\n\n\n\n\n\n\n8. Multiple non-linear polynomial model with interactions\nThe allEffects function can also easily handle polynomial non-linear models. In contrast, the plot_model function canâ€™t. However, we still can plot the estimates.\n\n\nm <- lm(log(salary) ~ poly(age, 2) * health, d)\n\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, show.values = T)+\n  theme_bw()\n\n\n\n\n\n\ncheck_model(m)\n\n\n\n\n9. Multiple non-linear Generalized Additive Models (GAM)\nThere are two main GAM packages: gam and mgcv. They both have an identical function which we need - gam(), which produces a conflict and R might be confused about it. Thus, use gam::gam() to specify from which package exactly the gam() function should be used. The gam package has itâ€™s own plotting function - plot.Gam and you can display all subplots on one plot by using par(mfrow = c(2, 2)). Please, donâ€™t use â€œplot(allEffects(gam1))â€ for GAM models, since it will produce only linear results.\ns() function indicates that we would like to use smoothing splines.\n\n\n# install.packages(\"gam\")\nlibrary(gam)\n\ngam1 <- gam::gam(salary~s(year, df = 4)+s(age, df = 5)+education + jobclass, data=d)\n\npar(mfrow = c(2, 2) )\nplot.Gam(gam1 , se=TRUE, col= \"blue\")\n\n\n\n\n10. Multiple logistic regression with interactions\nInteractions in logistic regression are not a problem for both packages. Moreover, have a look at three different displays of post-hocs from the same model and find code differences. Hint: I love the â€œ|â€ part of it!\n\n\nm <- glm(health ~ jobclass * health_ins, d, family = binomial)\n\nplot(allEffects(m))\n\n\n\nplot_model(m, type = \"int\")\n\n\n\nemmeans(m, pairwise ~ jobclass | health_ins, adjust = \"fdr\")$contrasts\n\n\nhealth_ins = 1. Yes:\n contrast                       estimate    SE  df z.ratio p.value\n 1. Industrial - 2. Information  -0.0553 0.177 Inf -0.312  0.7552 \n\nhealth_ins = 2. No:\n contrast                       estimate    SE  df z.ratio p.value\n 1. Industrial - 2. Information  -0.1830 0.241 Inf -0.760  0.4472 \n\nResults are given on the log odds ratio (not the response) scale. \n\nemmeans(m, pairwise ~ health_ins | jobclass, adjust = \"fdr\")$contrasts\n\n\njobclass = 1. Industrial:\n contrast       estimate    SE  df z.ratio p.value\n 1. Yes - 2. No    0.623 0.200 Inf 3.117   0.0018 \n\njobclass = 2. Information:\n contrast       estimate    SE  df z.ratio p.value\n 1. Yes - 2. No    0.495 0.222 Inf 2.228   0.0259 \n\nResults are given on the log odds ratio (not the response) scale. \n\n\n\nemmeans(m, pairwise ~ health_ins * jobclass, adjust = \"fdr\")$contrasts\n\n\n contrast                                     estimate    SE  df\n 1. Yes 1. Industrial - 2. No 1. Industrial     0.6232 0.200 Inf\n 1. Yes 1. Industrial - 1. Yes 2. Information  -0.0553 0.177 Inf\n 1. Yes 1. Industrial - 2. No 2. Information    0.4402 0.226 Inf\n 2. No 1. Industrial - 1. Yes 2. Information   -0.6785 0.196 Inf\n 2. No 1. Industrial - 2. No 2. Information    -0.1830 0.241 Inf\n 1. Yes 2. Information - 2. No 2. Information   0.4954 0.222 Inf\n z.ratio p.value\n  3.117  0.0055 \n -0.312  0.7552 \n  1.950  0.0768 \n -3.460  0.0032 \n -0.760  0.5367 \n  2.228  0.0518 \n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: fdr method for 6 tests \n\nBonus 4: visualize the post-hoc analysis with the PairWise P-value Plot (pwpp)\nAs promised, here is the visualization of the post-hocs. Notice the marginal means on the y axis and color-coded design of the plot, which connects the pairs the p-values for which were calculated and adjusted (x-axis).\n\n\npwpp(emmeans(m, ~ health_ins * jobclass), type = \"response\", adjust = \"fdr\"\n     )+theme_minimal()\n\n\n\n\n11. Multinomial logistic regression models via neural networks\nNo special treatment of the fancy neural networks needed. plot(allEffects()) just works!\n\n\n# get the data\nd <- foreign::read.dta(\"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta\")\n\nm <- nnet::multinom(prog ~ ses + write, d)\n\n\n# weights:  15 (8 variable)\ninitial  value 219.722458 \niter  10 value 179.985215\nfinal  value 179.981726 \nconverged\n\nplot(allEffects(m), \n     lines = list(multiline = T), \n     confint = list(style = \"auto\"), rows = 2, cols = 1)\n\n\n\n\n12. Multiple Linear Mixed-Effects-Model with interactions\nThe mixed-effects models are very complex. But fortunately, all 4 main functions, namely plot(allEffects()), plot_model(), emmeans() and check_model() work flawlessly and simply deliver! The check_model() even checks the assumptions for the random effects! The random effects themselves can also be visualized, but they are rarely interpreted, so, why bother?\n\n\n# install.packages(\"lme4\")\n# install.packages(\"lmerTest\")\n\nlibrary(lme4)\nlibrary(lmerTest)\n\n# get the data\nset.seed(9)\nd <- InstEval %>% \n  group_by(service, studage) %>% \n  sample_n(100) %>% \n  mutate(dept = as.numeric(dept))\n\nm1 <- lmer(y ~ service * studage + (1|s) + (1|d), data=d)\n\nplot(\n  allEffects(m1), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m1, type = \"int\")+\n  theme_blank()+\n  theme(legend.position = \"top\")\n\n\n\n# post-hocs\nemmeans(m1, pairwise ~ service | studage, adjust = \"none\")$contrasts \n\n\nstudage = 2:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.447 0.186 787 2.404   0.0164 \n\nstudage = 4:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.183 0.186 783 0.983   0.3261 \n\nstudage = 6:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.120 0.184 785 0.650   0.5156 \n\nstudage = 8:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.344 0.185 782 1.858   0.0635 \n\nDegrees-of-freedom method: kenward-roger \n\npwpp(emmeans(m1, ~ service * studage), type = \"response\", adjust = \"none\")+\n  theme_minimal()\n\n\n\n\n\n\n# check model assumptions\ncheck_model(m1)\n\n\n\n\nBonus 5: how to choose the best model\nThe performance package, near the already insanely useful check_model() function, provides a compare_performance() function, which compares models with multiple quality indicators, e.g.Â \\(R^2\\) or AIC. It is not only more informative as compared to the anova() function, which is often used for model comparison, but also works much better, because it displays a warning when two models shouldnâ€™t be compared, while anova(m, m1) simply fails, when models arenâ€™t supposed to be compared, but can be tricked by placing the mixed effects model (m1) first.\n\n\nm  <- lm(y ~ service * studage, data=d)\nm1 <- lmer(y ~ service * studage + (1|s) + (1|d), data=d)\n\ncompare_performance(m, m1)\n\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n# Comparison of Model Performance Indices\n\nModel |            Type |     AIC |     BIC |      BF | RMSE | Sigma |   R2 | R2 (adj.) | R2 (cond.) | R2 (marg.)\n-----------------------------------------------------------------------------------------------------------------\nm     |              lm | 2702.04 | 2744.21 |    1.00 | 1.30 |  1.30 | 0.02 |  6.68e-03 |            |           \nm1    | lmerModLmerTest | 2709.21 | 2760.74 | < 0.001 | 1.09 |  1.18 |      |           |            |       0.02\n\nanova(m1, m)\n\n\nData: d\nModels:\nm: y ~ service * studage\nm1: y ~ service * studage + (1 | s) + (1 | d)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm     9 2702.0 2744.2 -1342.0   2684.0                         \nm1   11 2688.7 2740.2 -1333.3   2666.7 17.389  2  0.0001675 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n13. GAMMs - Multiple Generalised Additive (non-linear) Mixed Effects Models\nThese kind of models are just madness, can be easily visualized though.\n\n\n# install.packages(\"mgcViz\")\nlibrary(mgcViz)\n\nm <- gammV(y ~ s(as.numeric(d), k = 3) + lectage, random=list(s=~1), data= InstEval %>% \n             slice(1:5000))\n\nplot(m, allTerms = T)\n\n\nHit <Return> to see next plot:\n\nHit <Return> to see next plot:\n\n\n14. Kaplan-Meier survival model\nSurvival models can be visualized with survminer package. Even some statistical details can be displayed. I still did not figured out how to visualize the post-hocs for survival analysis. It you know how, please let me know. Thus, I here simply provide the p-values of the post-hocs.\n\n\n# install.packages(\"survival\")\n# install.packages(\"survminer\")\nlibrary(survival)\nlibrary(survminer)\n\nset.seed(1)\nd <- lung %>% \n  filter(ph.ecog != 3) %>% \n  sample_n(100)\n\nm <- survfit(Surv(time, status) ~ ph.ecog, data = d)\n\n# simple plot\nggsurvplot(m)\n\n\n\n\n\n\n# fancy plot\nggsurvplot(m, \n           pval = TRUE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\")\n\n\n\n\n\n\n# post-hocs for survival analysis\npairwise_survdiff(\n  formula = Surv(time, status) ~ ph.ecog, data = d, p.adjust.method = \"fdr\"\n)\n\n\n\n    Pairwise comparisons using Log-Rank test \n\ndata:  d and ph.ecog \n\n  0       1      \n1 0.14954 -      \n2 0.00042 0.01507\n\nP value adjustment method: fdr \n\n15. Exponential Parametric Models\nThese models are very rarely used.\n\n\n# install.packages(\"flexsurv\")\nlibrary(flexsurv)    # for Parametric Survival Modelling\n\nex <- flexsurvreg(Surv(time, status) ~ factor(ph.ecog), data = d, dist=\"exponential\")\n\nggsurvplot(ex)\n\n\n\n\n16. Cox proportional hazard models\nCox models are more common as compared to the exponential models and can be visualized with a beautiful ggforest() plot.\n\n\nm <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data =  d)\n\nggforest(m, d)\n\n\n\n\nI hope you found this article useful. The are of coarse more interesting models out there. Thus, please let me know what kind of models you make and how you visualize them.\nIf you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.\n\n\n\n",
    "preview": "posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/thumbnail_visualize_models.png",
    "last_modified": "2021-01-02T10:27:18+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/",
    "title": "How to create a blog or a website in R with {Distill} package",
    "description": "If you're not online, you don't exist. A personal webpage or a blog became the business card of the digital century. It shows who you are and what you are capable of. Thus: show, don't tell.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2020-12-26",
    "categories": [
      "R & the Web",
      "videos"
    ],
    "contents": "\n\nContents\nWhy should we use Distill?\nManual intermittent publishing (deploying)1. Install Distill package\n2. Create your blog either manually with RStudio or with one line of code2.1. with RStudio\n2.2. with one line of code\n\n3. Build your website\n4. Publish your blog (or a website) via Netlify\n5. Create a new blog-post\n\nAutomated continuous publishing (deploying)1. Install distill and usethis packages\n2. Create your blog either manually with RStudio or with one line of code2.1. with RStudio\n2.2. with one line of code\n\n3. Build your website\n4. Connect to your github4.1. Create new repository\n4.2. Stage and Commit\n4.3. Run â€œusethis::use_github()â€ to connect a local repository to Github\n\n5. Publish your blog (or a website) via Netlify\n6. Create a new blog-post\n\nBlog configuration with \"_site.yml\"\nTheming - change desing and appearence of your blog\nPimp your contentLinks\nPlots\nFigures\nTables\nAsides\nEquations\nFootnotes\nThumbnails\nCitations\n\nUseful ressources\n\nWhy should we use Distill?\nBecause itâ€™s an easy, quick and free way to go online today. All you need to create a website or a blog is R, RStudio and Netlify account. Moreover, it requires very little programming. It might not look too fancy in the beginning (though totally enough for me), but with some effort youâ€™ll be able to re-design your website as you wish. Besides, Distill was originally aimed to make scientific online-publishing easier, so, it canâ€™t be too bad ;). By the way, The blog you reading right now was created with Distill. How? Thatâ€™s what this article is all about.\nIf you donâ€™t have or donâ€™t want to have a Github account (and continuous deployment), just read on. But if you are comfortable with Github and wanna continuously deploy via Github, jump to the chapter â€œAutomated continuous publishing (deploying)â€.\nManual intermittent publishing (deploying)\nWhy should we even consider manual instead of automated? Well, for some non-programmers going with the continuous deployment via Github immediately might be very challenging and frustrating. Iâ€™ve been there. We all know what is â€œbetterâ€ and â€œmore time effectiveâ€ in the long run, but the complexity might sometimes turn us off from doing something we would love to, resulting into â€œitâ€™s just not my cup of teaâ€ attitude. Other do not even need the continuous deployment, because they publish rarely, e.g.Â once per month, but would still like to be online. Going to the WordPress and co., which would most likely screw either code or code-output, is not always a solution. Fortunately, Distill and Netlify greatly reduce the complexity of going online. Thus, have a look at the quick (<5 minutes) video and then follow the step by step procedure described below and youâ€™ll be online with your blog in minutes.\n\n\n\n\n\n\n\n1. Install Distill package\n\n\ninstall.packages(\"distill\")\n\n\n\n2. Create your blog either manually with RStudio or with one line of code\n2.1. with RStudio\nOpen RStudio\nGo to File => New Projectâ€¦ => New Directory => Distill Blog\nâ€œDirectory nameâ€ will be the name of the folder, where all the documents from your blog will be stored\nChoose the directory where this folder supposed to be stored\nGive your blog a name. Youâ€™ll be able to rename it later\nClick â€œCreate projectâ€\n2.2. with one line of code\n\n\ndistill::create_blog(dir = \"amazing blog\", title = \"A cool name of my blog\")\n# or\ndistill::create_website(dir = \"amazing website\", title = \"A cool name of my Website\")\n\n\n\n3. Build your website\nA new RStudio window with newly created files and folders (right bottom corner) will open itself. One of the folders \"_posts\" will contain all your posts. There are already some examples of it. But donâ€™t bother about it now.\nFind the â€œBuildâ€ tab and press â€œBuild Websiteâ€. Youâ€™ll see the process of building.\nClick â€œOpen in Browserâ€ and explore your website\nWe arenâ€™t online yet. But very soon!\n4. Publish your blog (or a website) via Netlify\ngo the https://www.netlify.com/, sing up for Netlify (I used the Email way) and confirm your Email.\nIn Netlify youâ€™ll see a window with: â€œWant to deploy a new site without connecting to Git? Drag and drop your site folder hereâ€. If you somehow donâ€™t see it, find and press â€œTeam overviewâ€.\ngo to the directory of your blog and find the \"_site\" folder\ndrag and drop the \"_site\" folder from your computed into this window\nwait a moment till the â€œProductionâ€ tab produces green colored â€œPublishedâ€ and youâ€™ll get a funny named website in the left top corner, also green and starting with â€œhttps://â€. My was â€œhttps://condescending-darwin-bc567f.netlify.appâ€ :)\nclick on it\nCongrats, you are online!\ngo back to Netlify, click â€œSite settingsâ€ => â€œChange site nameâ€\nrename your site (e.g.Â better-name) and hit â€œSaveâ€\nClick on the â€œbetter-name.netlify.appâ€ to make sure the name has changed and the site is still working\ngo back to RStudio to populate your blog\n5. Create a new blog-post\nrun the line below with the name of your choice\n\n\ndistill::create_post(\"My second blog post...this one will be really good :-)\")\n\n\n\na new partly pre-filled Rmarkdown document will open itself in RStudio\nfill it with some text and code as you usually do with Rmarkdown documents\nhit â€œKnitâ€. NOTE: youâ€™ll need to always â€œKnitâ€ all changed or created blog-posts individually. It is the only way to update them. â€œBuild Websiteâ€ would not re-render them for you, because itâ€™s computationally expensive and prone to problems. However, the Distill-Website (we are doing Distill-Blog now) would. You can learn more about the difference between them here.\nThis new post exists only on your local computer, still not online, thusâ€¦\ngo back to Netlify and click â€œDeploysâ€ tab, where youâ€™ll see another window with: â€œNeed to update your site? Drag and drop your site folder hereâ€\ndrag and drop the \"_site\" folder there and wait till â€œProductionâ€ tab produces green â€œPublishedâ€\nclick on the â€œbetter-name.netlify.appâ€ to make sure the new blog-post appeared\nclick on your post and enjoy your creative work for a moment ;), then\ngo back to RStudio and repeat step 5\nAutomated continuous publishing (deploying)\nContinuous deployment is cool! But the path there can be a little prickly. This path may take a couple of hours or days (in my case ðŸ™ˆ). But, once there, you quickly forget all the troubles and using Github with continuous deployment becomes your second nature. So, I think there are many people which deploy either still manually or already continuously. But not many of them in the middle (I might be wrong though). Thus, I will assume you already have installed Git on your computer, created a Github account and connected your RStudio to your Github. If not, but you wanna be there, I could recommend a single short free online book which helped me go through it: Happy Git and GitHub for the useR. Youâ€™ll only need it once! If you ready to proceed, have a look at the quick (ca. 8 minutes) video and then follow the step by step procedure described below and youâ€™ll be online with your blog in minutes.\n\n\n\n\n\n\n\n1. Install distill and usethis packages\n\n\ninstall.packages(\"distill\")\ninstall.packages(\"usethis\")\n\n\n\n2. Create your blog either manually with RStudio or with one line of code\n2.1. with RStudio\nOpen RStudio\nGo to File => New Projectâ€¦ => New Directory => Distill Blog\nâ€œDirectory nameâ€ will be the name of the folder, where all the documents from your blog will be stored\nChoose the directory where this folder supposed to be stored\nGive your blog a name. Youâ€™ll be able to rename it later\nClick â€œCreate projectâ€\n2.2. with one line of code\n\n\ndistill::create_blog(dir = \"amazing blog\", title = \"A cool name of my blog\")\n# or\ndistill::create_website(dir = \"amazing website\", title = \"A cool name of my Website\")\n\n\n\n3. Build your website\nA new RStudio window with newly created files and folders (right bottom corner) will open itself. One of the folders \"_posts\" will contain all your posts. There are already some examples of it. But donâ€™t bother about it now.\nFind the â€œBuildâ€ tab and press â€œBuild Websiteâ€. Your blog will be created in a new window.\nClick â€œOpen in Browserâ€ and explore your website\nWe arenâ€™t online yet. But very soon!\n4. Connect to your github\n4.1. Create new repository\nGo back to RStudio and run use_git() in order to create a new local Git repository\nthen answer two questions:\nâ€œIs it ok to commit them?â€ Donâ€™t commit by typing 3 for â€œNoâ€, or â€œNopeâ€ or similar.\nâ€œA restart of RStudio is required to activate the Git pane Restart now?â€ Restart by typing 2 for â€œYesâ€ or â€œYupâ€.\n\n\n\nusethis::use_git() \n\n\n\nAfter restart youâ€™ll see a new â€œGitâ€ tab appear between the â€œBuildâ€ and â€œTutorialâ€ tabs. Thatâ€™s gut!\nclick on the â€œGitâ€ tab and youâ€™ll see empty boxed under â€œStagedâ€, lotâ€™s of yellow question marks under â€œStatusâ€ and the file-names under â€œPathâ€.\n4.2. Stage and Commit\ncheck all the boxes and press â€œCommitâ€ button, which is (vertically) between the â€œStatusâ€ and â€œHistoryâ€ tabs. A colourful window will pop up. This window describes all the changes you are about to make to your blog.\nFind the â€œCommit messageâ€ box and definitely describe what changes you have done (e.g.Â â€œFirst commitâ€), because then youâ€™ll always be able to get back to the previous version, in case something stops working. Thatâ€™s what they call - a version control.\npress â€œCommitâ€\nwait until you see the â€œCloseâ€ button and close â€œGit commitâ€\nforget the other pop up window and go back to RStudio.\n4.3. Run â€œusethis::use_github()â€ to connect a local repository to Github\n\n\nusethis::use_github() \n\n\n\nA new repository will be automatically created on your Github profile and the new browser window with your Github will pop up.\nIf youâ€™ll be asked: â€œWhich git protocol to use?â€, choose the one with â€œhttpsâ€ and if youâ€™ll then be asked: â€œAre title and description ok?â€, agree to proceed.\nNOTE: if something (e.g.Â Github Personal Access Token) doesnâ€™t work, get back to the Happy Git and GitHub for the useR book and work through it if you still didnâ€™t. Youâ€™ll only need it once!\n5. Publish your blog (or a website) via Netlify\nsign into your Netlify account, if have one, if notâ€¦\ngo the https://www.netlify.com/, sing up for Netlify either with your Email or with your Github profile.\nclick a green box â€œNew site from Gitâ€\nconfigure Netlify on â€œGithubâ€,\nchoose a newly created repository, youâ€™ll recognize the name (â€œmy new blogâ€?).\nNOTE: Make sure to set the â€œPublish Directoryâ€ to \"_site\" (could be â€œdocsâ€ if you checked some boxes while creating new project). \"_site\" (or â€œdocsâ€) contains all the information about your blog.\nclick â€œDeployâ€!\nwait a moment till the â€œProductionâ€ tab produces green colored â€œPublishedâ€ and youâ€™ll get a funny named website in the left top corner, also green and starting with â€œhttps://â€. My was â€œhttps://condescending-darwin-bc567f.netlify.appâ€ :)\nclick on it\nCongrats, you are online!\ngo back to Netlify, click â€œSite settingsâ€ => â€œChange site nameâ€\nrename your site (e.g.Â better-name) and hit â€œSaveâ€\nclick on the â€œbetter-name.netlify.appâ€ to make sure the name has changed and the site is still working\ngo back to RStudio to populate your blog\n6. Create a new blog-post\nrun the line below with the name of your choice\n\n\ndistill::create_post(\"My second blog post...this one will be really good :-)\")\n\n\n\na new partly pre-filled RMarkdown document will open itself in RStudio\nfill it with some text and code as you usually do with RMarkdown documents\nhit â€œKnitâ€. NOTE: youâ€™ll need to always â€œKnitâ€ all changed or created blog-posts individually. It is the only way to update them. â€œBuild Websiteâ€ would not re-render them for you, because itâ€™s computationally expensive and prone to problems. However, the Distill-Website (we are doing Distill-Blog now) would. You can learn more about the difference between them here.\nThis new post exists only on your local computer, still not online, thusâ€¦\ngo to the â€œGitâ€ tab in RStudio and check all the boxes\npress â€œCommitâ€, a new window will pop up\nadd description of your commit\nagain press â€œCommitâ€\nwait until you see the â€œCloseâ€ button and close â€œGit commitâ€ pop up window\npress â€œPushâ€. Pushing will transfer changes in your blog from your local computer to a remote place, namely your Github repository. And since your Github repository is connected to Netlify, this changes will be online after successful push.\nwait until you see the â€œCloseâ€ button and close â€œGit pushâ€ pop up window\nclose or ignore the other pop up window\nget back to your blog â€œbetter-name.netlify.appâ€ and refresh (it may take a few seconds, so, donâ€™t panic if the first refresh donâ€™t work). You should see a new blog-post.\nCongrats! You now continuously deploy your online blog!\nclick on your post and enjoy your creative work for a moment ;), then\ngo back to RStudio and repeat step 6 with following routine:\nCreate or change posts\nKnit\nCommit\nPush\n\nYou donâ€™t need to commit and push every change, only important ones. Think of this process as really saving the progress you made on your blog. Another useful thing I learned to appreciate after committing a couple of thousands of changes (which is annoying!) is to - check all the boxes (under â€œstageâ€) at once! For this:\ngo to â€œTerminalâ€ tab in RStuio (itâ€™s near the â€œConsoleâ€)\ntype â€œgit add -Aâ€ and press enter\ncheck one of the stage-boxes, the rest of them suppose to be then check themselves automatically\nthen press â€œCommitâ€\nBy the way, if you donâ€™t want to publish your post until you really satisfied with it, you can start out as a draft:\n\n\ndistill::create_post(\"Another nice post\", draft = TRUE)\n\n\n\nOr add draft: true to the postâ€™s metadata. When you are ready, delete draft: true.\nBlog configuration with \"_site.yml\"\nOpen \"_site.yml\". Youâ€™ll see something like that:\n\nname: \"Your cool webpage\"\ntitle: \"Your cool webpage title\"\ndescription: |\n  Exploring something very important.\nbase_url: https://beta.rstudioconnect.com/content/your_thing/\nnavbar:\n  logo: images/fancy_logo.png\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\n    - text: \"Rest\"\n      href: rest.html\n    - icon: fa fa-rss\n      href: index.xml\noutput: distill::distill_article\n\n\"_site.yml\" is the most important document for your website. Configure it carefully and slowly. You can add a lot of useful things: categories, google-analytics, customize the navigation bar, add references, new theme with CSS code for an individual design of your site, icons of twitter & co. and much more, which is not to important for a new site and can be added at any time.\nIf you work through the Distill web page, youâ€™ll see a lot of examples for how to design your \"_site.yml\" and â€œtheme.cssâ€ files. But, the best way I found to do this, is just to find the \"_site.yml\" or â€œtheme.cssâ€ of other Distill-blogs on Github and get inspiration from them by playing with your own code (I hope nobody is offended by this sentence due to the open source nature of R, but please let me know if itâ€™s wrong and Iâ€™ll remove this recommendation!).\nWhile you already have \"_site.yml\" file in your blog-folder, you donâ€™t have a â€œtheme.cssâ€ file. To get one, read onâ€¦\nTheming - change desing and appearence of your blog\nYou can modify the CSS code in your theme after creating it by running the following line of code:\n\n\ndistill::create_theme(name = \"theme\") \n\n\n\nTo activate a custom theme site-wide, add a theme key to the top-level of your \"_site.yml\" configuration file:\n\nname: \"Your cool webpage\"\ntitle: \"Your cool webpage title\"\ntheme: theme.css \n(...the rest of your _site.yml)\n\nPimp your content\nMost elements enhancing your content, like links, tables, plots, equations etc., are similar to the usual R Markdown syntax. Thus for a deeper insights go to the R Markdown: The Definitive Guide book, itâ€™s online and free. Below I just display some quick â€œhow toâ€ examples and provide links to a more thorough online resources.\nLinks\nThe links are displayed with the help of two different brackets. First, use the square brackets to produce a [clickable word or phrase], then, directly after the square brackets, use round brackets with the URL inside, e.g.Â (https://bookdown.org/yihui/rmarkdown/). The URL by itself would certainly also work, but itâ€™s not as convenient as this.\nPlots\n\n\nlibrary(tidyverse)\nggplot(mtcars, aes(hp, mpg)) + \n  geom_point() + \n  geom_smooth() +\n  theme_bw()\n\n\n\n\nFigures\nYou can add external static and dynamic figures, plots, photos or diagrams by using knitr::include_graphics() function. More on figures here.\n\n\nknitr::include_graphics(\"images/your_figure.png\")\n\n\n\n\n\n\n\n\n\n\n\nTables\nMore on tables here.\n\n\n# install.packages(\"gtsummary\")\nlibrary(gtsummary)\niris %>% \n  tbl_summary()\n\n\nhtml {\n  font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif;\n}\n\n#dvnhdgjzxt .gt_table {\n  display: table;\n  border-collapse: collapse;\n  margin-left: auto;\n  margin-right: auto;\n  color: #333333;\n  font-size: 16px;\n  font-weight: normal;\n  font-style: normal;\n  background-color: #FFFFFF;\n  width: auto;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #A8A8A8;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #A8A8A8;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n}\n\n#dvnhdgjzxt .gt_heading {\n  background-color: #FFFFFF;\n  text-align: center;\n  border-bottom-color: #FFFFFF;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#dvnhdgjzxt .gt_title {\n  color: #333333;\n  font-size: 125%;\n  font-weight: initial;\n  padding-top: 4px;\n  padding-bottom: 4px;\n  border-bottom-color: #FFFFFF;\n  border-bottom-width: 0;\n}\n\n#dvnhdgjzxt .gt_subtitle {\n  color: #333333;\n  font-size: 85%;\n  font-weight: initial;\n  padding-top: 0;\n  padding-bottom: 4px;\n  border-top-color: #FFFFFF;\n  border-top-width: 0;\n}\n\n#dvnhdgjzxt .gt_bottom_border {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#dvnhdgjzxt .gt_col_headings {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n}\n\n#dvnhdgjzxt .gt_col_heading {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  padding-left: 5px;\n  padding-right: 5px;\n  overflow-x: hidden;\n}\n\n#dvnhdgjzxt .gt_column_spanner_outer {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: normal;\n  text-transform: inherit;\n  padding-top: 0;\n  padding-bottom: 0;\n  padding-left: 4px;\n  padding-right: 4px;\n}\n\n#dvnhdgjzxt .gt_column_spanner_outer:first-child {\n  padding-left: 0;\n}\n\n#dvnhdgjzxt .gt_column_spanner_outer:last-child {\n  padding-right: 0;\n}\n\n#dvnhdgjzxt .gt_column_spanner {\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: bottom;\n  padding-top: 5px;\n  padding-bottom: 6px;\n  overflow-x: hidden;\n  display: inline-block;\n  width: 100%;\n}\n\n#dvnhdgjzxt .gt_group_heading {\n  padding: 8px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#dvnhdgjzxt .gt_empty_group_heading {\n  padding: 0.5px;\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  vertical-align: middle;\n}\n\n#dvnhdgjzxt .gt_from_md > :first-child {\n  margin-top: 0;\n}\n\n#dvnhdgjzxt .gt_from_md > :last-child {\n  margin-bottom: 0;\n}\n\n#dvnhdgjzxt .gt_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  margin: 10px;\n  border-top-style: solid;\n  border-top-width: 1px;\n  border-top-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 1px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 1px;\n  border-right-color: #D3D3D3;\n  vertical-align: middle;\n  overflow-x: hidden;\n}\n\n#dvnhdgjzxt .gt_stub {\n  color: #333333;\n  background-color: #FFFFFF;\n  font-size: 100%;\n  font-weight: initial;\n  text-transform: inherit;\n  border-right-style: solid;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n  padding-left: 12px;\n}\n\n#dvnhdgjzxt .gt_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dvnhdgjzxt .gt_first_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n}\n\n#dvnhdgjzxt .gt_grand_summary_row {\n  color: #333333;\n  background-color: #FFFFFF;\n  text-transform: inherit;\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n}\n\n#dvnhdgjzxt .gt_first_grand_summary_row {\n  padding-top: 8px;\n  padding-bottom: 8px;\n  padding-left: 5px;\n  padding-right: 5px;\n  border-top-style: double;\n  border-top-width: 6px;\n  border-top-color: #D3D3D3;\n}\n\n#dvnhdgjzxt .gt_striped {\n  background-color: rgba(128, 128, 128, 0.05);\n}\n\n#dvnhdgjzxt .gt_table_body {\n  border-top-style: solid;\n  border-top-width: 2px;\n  border-top-color: #D3D3D3;\n  border-bottom-style: solid;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n}\n\n#dvnhdgjzxt .gt_footnotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#dvnhdgjzxt .gt_footnote {\n  margin: 0px;\n  font-size: 90%;\n  padding: 4px;\n}\n\n#dvnhdgjzxt .gt_sourcenotes {\n  color: #333333;\n  background-color: #FFFFFF;\n  border-bottom-style: none;\n  border-bottom-width: 2px;\n  border-bottom-color: #D3D3D3;\n  border-left-style: none;\n  border-left-width: 2px;\n  border-left-color: #D3D3D3;\n  border-right-style: none;\n  border-right-width: 2px;\n  border-right-color: #D3D3D3;\n}\n\n#dvnhdgjzxt .gt_sourcenote {\n  font-size: 90%;\n  padding: 4px;\n}\n\n#dvnhdgjzxt .gt_left {\n  text-align: left;\n}\n\n#dvnhdgjzxt .gt_center {\n  text-align: center;\n}\n\n#dvnhdgjzxt .gt_right {\n  text-align: right;\n  font-variant-numeric: tabular-nums;\n}\n\n#dvnhdgjzxt .gt_font_normal {\n  font-weight: normal;\n}\n\n#dvnhdgjzxt .gt_font_bold {\n  font-weight: bold;\n}\n\n#dvnhdgjzxt .gt_font_italic {\n  font-style: italic;\n}\n\n#dvnhdgjzxt .gt_super {\n  font-size: 65%;\n}\n\n#dvnhdgjzxt .gt_footnote_marks {\n  font-style: italic;\n  font-size: 65%;\n}\nCharacteristic\n      N = 1501\n    Sepal.Length\n      5.80 (5.10, 6.40)\n    Sepal.Width\n      3.00 (2.80, 3.30)\n    Petal.Length\n      4.35 (1.60, 5.10)\n    Petal.Width\n      1.30 (0.30, 1.80)\n    Species\n      \n    setosa\n      50 (33%)\n    versicolor\n      50 (33%)\n    virginica\n      50 (33%)\n    \n        \n          1\n          \n           \n          Statistics presented: Median (IQR); n (%)\n          \n      \n    \n\nAsides\nYou can include notes or even plots â€œasideâ€ (to the right) your article:\n\n\n\n\nHere the funny fact or some important info.\n\n\n\nEquations\nYou can use a Latex syntax for it:\n\n$$\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}$$\n\n\\[\n\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\n\\]\nFootnotes\nFootnotes are created in the usual RMarkdown way, namely ^[here is the content of your footnote] and here are two examples 1 and 2. Point your cursor on the footnote number or scroll to the bottom of the page to see whatâ€™s inside of the footnote.\nThumbnails\nThumbnails (or previews) are the images which are displayed along the post. They serve as an eye catcher. You can add a preview image into the postâ€™s metadata by adding a â€œpreviewâ€ field:\n\ntitle: \"Blog post on how to write blog posts\"\ndescription: |\n  Here we were out of ideas, and therefore we are proud to announce\nour new post about how to write a post. Creativity is a b**ch! ... And I love her!\npreview: images/photo-of-me-because-I-am-sooo-beautifuuulll-toniiiight.png\n\nIf you donâ€™t provide a picture for preview, the first plot or picture from your blog-post will be used as a thumbnail by default. To override this behavior, you can add the preview = TRUE in to the code-chunk, e.g.: {r some_chunk, preview=TRUE}.\nIf you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.\nCitations\nYou either can include a BibTex-like citation of others or make your article citable (see the very bottom of this article). â€œHow to citeâ€ is best described in the citation section of the Distill website.\nUseful ressources\nThe best place to start is actually the Distill website itself: https://rstudio.github.io/distill.\nTom Mocks blog-post on how to build blogs with Distill helped me a lot! https://themockup.blog/posts/2020-08-01-building-a-blog-with-distill/\n\nHere is the first footnote, which does not suppose to interrupt the main text!â†©ï¸Ž\nHere is the secondâ†©ï¸Ž\n",
    "preview": "posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/images/thumbnail.png",
    "last_modified": "2021-01-08T22:31:04+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  }
]

---
title: "R package reviews {performance} check how good your model is! "
description: |
  There are several indicators of model quality, e.g. $R^2$ or AIC, and several assumption for every model which supposed to be checked, e.g. normality of residuals, multicollinearity etc.. R provides solutions for every indicator or assumption you can imagine. However, they are usually spread around different packages and functions. {performance} package brings all of quality indicators and all of the assumption under one roof. Thus, for me it became the one-stop solution for modelling.
author:
  - name: Yury Zablotski
    url: https://yury-zablotski.netlify.app/
date: 01-03-2021
categories:
  - R package reviews
  - videos
  - visualization
preview: 14.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    code_download: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)          # data manipulation
library(performance)        # model performance
```



# R demo for how check model performance and model assumptions

A short (ca. 15 min) video below shows how *performance* package works and the code you'll see in the video is provided below.

```{r, eval=T, echo=F}
vembedr::embed_youtube("EPIxQ5i5oxs")
```

# Check model performance, or model quality

## Usual linear models

In order to see how model performs, use the intuitive function - *model_performance*. It provide several quality indicators, which differ depending on the model. For instance the first model below provides the most "classical" quality indicators:

- AIC - Akaike's Information Criterion, the lower the better. AIC is an estimator of out-of-sample prediction error and thereby relative quality of statistical models for a given set of data.
- BIC - Bayesian Information Criterion, the lower the better
- $R^2$ - the proportion of the variance explained, the higher the better. It is sometimes referred to as a *goodness of fit* of the model
- $R^2 adjusted$ - the proportion of the variance explained for multiple (several predictors) models, the higher the better
- RMSE - Root Mean Square Error is a measure of spread of the residuals around predictions (prediction errors), the lower the better
- Sigma - standard deviation is a measure of spread of the data around the mean, the lower the better 

```{r}
m <- lm(mpg ~ hp + cyl, data = mtcars)

model_performance(m)
```



## Complex mixed-effects models

Mixed effects model provide two different $R^2$s and ICC:

- conditional $R^2$ shows modelâ€™s total explanatory power and 
- marginal $R^2$ show the part related to the fixed effects (predictors) alone
- ICC - intraclass correlation coefficient, is similar to $R^2$ and also shows the goodness of fit or, in other words, quantifies the proportion of variance explained by a grouping (random) factor in mixed-effects (multilevel/hierarchical) models.

```{r}
library(lme4)

m1 <- lmer(mpg ~  hp * cyl + wt + (1 | am), data = mtcars)

model_performance(m1)
```


## Fancy Bayesian mixed-effects models

- ELPD - expected log pointwise predictive density is a measure the prediction accuracy of Bayesian models, the closer to 0 the better. The out-of-sample predictive fit can either be estimated by Bayesian leave-one-out cross-validation (LOO) or by widely applicable information criterion (WAIC). Thus, the two next indicators are:
- LOOIC - leave-one-out cross-validation information criterion, the lower the better
- WAIC - widely applicable information criterion, the lower the better

```{r message=F, warning=F, cashe = T}
library(rstanarm)

m2 <- stan_glmer(mpg ~ hp * cyl + wt  + (cyl | am), data = mtcars, refresh=0)

model_performance(m2)
```



## Compare models

Unfortunately we can't compare not similar models, for instance different types of models (e.g. linear vs. mixed effects) or models done with different amounts of data, because one of them will be better for reasons other then model quality. Even more unfortunately, such comparisons are often conducted anyway. I was also guilty of it in the past. One of the common R functions allowing such comparisons is *anova(model1, model2)*. It smartly does not allow the comparison of linear vs. mixed effects models (see the red warning below), but stupidly can be out-tricked by placing mixed effects model first.

```{r eval=F}
anova(m, m1)
```

<span style="color: red">Error: $ operator not defined for this S4 class</span>

```{r}
anova(m1, m)
```
In contrast *compare_performance()* compares them immediately and provides a useful warning (see below). It also provides an somewhat superficial (according to the package author), but still useful ranking of models (see Performance-Score).


```{r}
compare_performance(m, m1, rank = T)
```

<span style="color: red">Warning: Models are not of same type. Comparison of indices might be not meaningful.</span>

However, if we compare models which are comparable, no warning will be displayed. 

```{r}
m1.1 <- lmer(mpg ~  hp + cyl + wt + (1 | am), data = mtcars)
compare_performance(m1, m1.1, rank = T)
```

Moreover, *performance* package can also easily plot this comparison, simply by wrapping up the *compare_performance()* function into a *plot()* function. It could not be easier or more elegant than that!

```{r}
plot( compare_performance(m1, m1.1) )
```

## Use individual quality indicators

```{r}
r2(m1)    # model fit - the proportion of the variance explained
r2(m2)    # Bayesian R2 even with Credible Intervals

icc(m2)   # Intraclass Correlation Coefficient (ICC)

# not part of the "performance" package, but fits well here
AIC(m1)   # Akaike's Information Criterion
BIC(m1)   # Bayesian Information Criterion
```



# Check assumptions: or modern (Jan 2020) model diagnostics

## All assumptions at once!

The first time I discovered *check_model()* function, I have got an intensive intellectual "nerdgasm". I could not believe how simple and at the same time sophisticated this function is! I then checked out other work of the authors of {performance} package and was stunned, how many useful things they already produced. And I don't think they'll stop any time soon. Thus, if you just began to learn R and stats, [check out their work to quickly step up your data science game!](https://github.com/easystats/easystats)

```{r}
citation("performance")
```
Now the function itself: it visually checks all the assumptions you need to check and gives you a big-picture overview of assumptions for almost any model you can have (at least for all the common ones). Two examples below display such a big-picture of a usual linear model and a mixed-effects model with random effects. The subplots of this picture even explain what you should look for! For instance: "Dots should be plotted along the line" in the residuals diagnostics plots. Have a look at the big-picture first and go to the next chapter for some more details on the particular assumption. Why? Because the package provides the opportunity to check individual assumptions too and even goes one step deeper into it! 

```{r fig.height=9, fig.width = 9, cache=T}
check_model(m)
```


```{r fig.height=9, fig.width=11, cache=T}
check_model(m1)
```



## Individual assumptions with reports of statistical tests!

Checking individual assumption is also very intuitive. For instance, for checking the normality of the residuals, use *check_normality()* function. It will conduct the Shapiro-Wilk Normality test and report the result of it. Put *check_normality(m)* inside of the *plot()* function to visualize the result. It is even preferable to visually inspect the residuals, because Shapiro-Wilk Test will often produce significant results for large sample sizes (and in the age of big data we always have large samples) even if data is perfectly normally distributed.

```{r}
check_normality(m)     # shapiro.test, however, visual inspection (e.g. Q-Q plots) are preferable
check_normality(m1)
plot( check_normality(m1) )
```

The collinearity is measured by the variance inflation factor (VIF). VIF<5 is acceptable for individual predictors, while VIF<10 is moderate, so, that it gives you an idea that some variables in the model might be redundant. If you model interactions, VIF would naturally increase, and the unwritten (I forgot where did I learned it from ðŸ™ˆ) rule is that interactions with VIF<20 are still acceptable. 

```{r}
check_collinearity(m)  # by variance inflation factor (VIF)
check_collinearity(m1)
plot( check_collinearity(m1) )
```

If you wanna check the heteroscedasticity, use function *check_heteroscedasticity()*. The heteroscedasticity assumption itself is kind of self-explanatory. But what I can't explain is that why nobody in the history of R programming language came up with such intuitive functions before {performance}?

```{r}
check_heteroscedasticity(m)
check_heteroscedasticity(m1)
plot( check_heteroscedasticity(m) )
```

One particular function somehow didn't finish up in the "big-picture". Namely *check_outliers()*. May be because there are too many methods for identifying outliers (â€œzscoreâ€, â€œiqrâ€, â€œcookâ€, â€œparetoâ€, â€œmahalanobisâ€, â€œrobustâ€, â€œmcdâ€, â€œicsâ€, â€œopticsâ€, â€œlofâ€)? I don't know. Z-Scores is the default method though, however, the method can be easily specified as an argument of the function:

```{r fig.width=9}
check_outliers(mtcars$mpg)
plot(check_outliers(mtcars$mpg))
plot(check_outliers(mtcars$mpg, method = "iqr"))
```


There are many more useful functions in this package. And there is no need to describe them all here. If you liked what you have seen so far, just type "?performance" in the RStudio console, go to Help, scroll down to the bottom of the Help page, click **Index** and enjoy the package ðŸ˜‰.

If you think, I missed something, please comment on it, and Iâ€™ll improve this tutorial.

Cheers guys, have a good one!

# Useful references

- https://easystats.github.io/performance/

- https://cran.r-project.org/web/packages/performance/performance.pdf

- https://www.rdocumentation.org/packages/performance/versions/0.6.1

- One of the best places to learn R is R-Bloggers platform: http://www.R-bloggers.com 








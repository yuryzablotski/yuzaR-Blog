---
title: "R demo | Repeated Measures ANOVA (One-Way) | How to Conduct, Visualise and Interpret"
description: |
  Can sport increase our selfesteem? Well, one experiment measured self-esteem of 10 people on three different time points and used Repeated Measures ANOVA to answer this question. So, let's learn how to produce this statistically rich plot using only one simple command and how to interpret all these results.
author:
  - name: Yury Zablotski
    url: https://yuzar-blog.netlify.app/
date: "`r format(Sys.time(), '%B %d, %Y')`"
categories:
  - videos
  - statistics
preview: thumbnail.png
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_float: true
    toc_depth: 6
    code_download: true
# draft: true
bibliography: /Users/zablotski/Documents/library.bib
#csl: american-political-science-association.csl
biblio-style: apalike
link-citations: yes
linkcolor: blue
#draft: true
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T)
```

## This post as a video

I recommend to watch a video first, because I highlight things I talk about. It's ca. 7 minutes long. 

```{r, eval=T, echo=F, fig.height=5, fig.width=7}
vembedr::embed_youtube("CL7WlwKz5aw")
library(tidyverse)
library(datarium)
```

## Previous topics

[Paired t-Test](https://yuzar-blog.netlify.app/posts/2022-01-22-pairedsamplesttestinr/) and [Paired Wilcoxon test](https://yuzar-blog.netlify.app/posts/2022-01-13-pairedsampleswilcoxonsigned-ranktestinr/) would help.

## Get the data

```{r eval=FALSE}
# install.packages("tidyverse")  # for everything ;)
library(tidyverse)

# install.packages("datarium")   # for selfesteem data
library(datarium)

# View(selfesteem)
```

```{r echo=FALSE}
library(flextable)
selfesteem %>% 
  mutate_if(is.numeric, ~round(., 2)) %>%
  regulartable() %>% 
  autofit()
```

For that let's take {selfesteem} data from {datarium} package, and gather all three time-points into one column, so that our timepoints become a variable for the x-axis of the plot, and our selfesteem scores become a variable for the y-axis of the plot. For **repeated measures**, the data **needs to be sorted** so that, the first observation of the **first time point**, pairs with the first observation of other **time points**. If our data is sorter, we are ready to compute the test.

```{r}
# make long format
d <- selfesteem %>%
  gather(key = "time", value = "score", t1, t2, t3) 

# View(d)
```


## Compute One-Way Repeated Measures ANOVA 

And the best way to compute Repeated Measures ANOVA (in my opinion) is the {ggwithinstats} function from {ggstatsplot} package, which needs only 4 arguments:

- first, **our data**, which is **d**, then
- **x** - as grouping variable - **time**,
- **y** - will be the **scores of selfesteem** and we choose
- the **parametric type** of statistical approach, which tells {ggwithinstats} to conduct **Repeated Measures ANOVA**

Such simple command results in this statistically rich and publication ready plot! Now, let's interpret the results.

```{r fig.height=7}
# install.packages("ggstatsplot")
library(ggstatsplot)

set.seed(1)   # for Bayesian reproducibility
ggwithinstats(
  data = d,
  x    = time, 
  y    = score, 
  type = "parametric"
)

ggsave(filename = "rm_anova.jpg", plot = last_plot(), width = 5.5, height = 5.5)
```

## Interpret the result

- **F-statistics** and **degrees of freedom (DFs)** were previously used to get p-values. But, since modern statistical software always report **p-values**, we can safely ignore them.

- the **p-value** helps to test the hypothesis, where **Null Hypothesis** says that sample-means are similar, or, to be more exact, that the differences between pairwise samples are equal to Zero, while the **Alternative Hypothesis** says that sample-means differ, or, in other words, that differences between pairwise samples are not-equal to Zero.


```{r}
diffs <- selfesteem %>%
  mutate(
    diff_t3_t1 = t3 - t1,
    diff_t3_t2 = t3 - t2,
    diff_t2_t1 = t2 - t1 )

# View(diffs)
```


```{r echo=FALSE}
diffs %>% 
  mutate_if(is.numeric, ~round(., 2)) %>%
  regulartable() %>% 
  autofit()
```

- our **very low P-value** (p = 0.00000216) shows a **very strong evidence against the null hypothesis (H~0~)**,  **in favor of the alternative hypothesis (H~Alt~)**, that sample-means are different. That tells us that exercise significantly increases selfesteem over time. Which is good to know! But how strong is the effect of sports on selfesteem? P-value can not tell that. A P-value only tells you that there is an effect from training, but not how strong this effect is. 

![](p_value_interpretation.png)

- Fortunately, {ggwithinstats} provides **partian omega squared** with 95% Confidence Intervals as the measure of the **Effect Size** for Repeated Measures ANOVA, which shows that training effect of 0.81 is large.

![](interpret_omega_squared.png){width=50%}

- Moreover, {ggwithinstats} also provides a **Bayesian Effect Size**, namely the **coefficient of determination** - $R_2$ with 95% Highest Density Intervals. $R_2$ shows the explanatory power of our model and since $R_2$ goes from 0 to 100%, the explanatory power of 82% in our model is huge, or, if we interpret $R_2$ as the effect size, our effects is substantial.

![](interpret_r_squared.png){width=50%}

- If that's not enough, we can have a look at the **Bayes Factor**, which is conceptually similar to the **p-value**. Our **Bayes Factor** of -20 indicates a **Decisive evidence for the alternative hypothesis** - that training does increase our selfesteem … which IS in line with the frequentists statistics on the top of the plot.

![](bf_interpretation.png)

- now, both, **Bayes Factor** and **p-value** tell us that difference among time-points exists, however, they don't show between which time-points exactly. That's why we need to compare every timepoint to every other timepoint pairwisely. 

- by the way, our two global tests are often called with a strange name - **omnibus test**, while the pairwise tests between time-points, are sometimes described in a dead latin language as - **post-hoc** - which means - **after the event** - in plain English. I really think those unnecessary names make statistics more complicated then it is. 

- Anyway, {ggwithinstats} **automatically knows that we need Paired t-Tests for Repeated Measures ANOVA, automatically conducts those tests and displays p-values and even corrects p-values for multiple comparisons without any additional code**. How cool is that!

Here is a quick proof that {ggwithinstats} indeed uses paired t-test.

```{r}
pairwise.t.test(d$score, d$time,
                paired=T, 
                p.adjust.method = "holm")
```

## Customise the result

However, if we want to, we can easily customize our plot by using either additional arguments within the function, or arguments from {ggplot2} package outside of it. For example, 

- if you found outliers in your data, you can display them on the plot and 
- use a **robust ANOVA** to minimize the effect of outliers,
- here again, the function automatically uses correct pairwise tests for every omnibus test and corrects p-values for multiple comparisons with a Holm method,
- which you can easily change to a more famous **Bonferroni correction for multiple comparisons** ... but I wouldn't recommend it, because Bonferroni correction is too conservative and we could miss some interesting result
- then, if you want to display **not only significant**, but **all comparisons**,
- if you want to hide either Frequentists or Bayesian statistics, or both...

you can easily do that and much more. Just ask R about {?ggwithinstats} function and try some things out, I am sure you'll enjoy it. 

```{r}
ggwithinstats(
  data = d,
  x    = time, 
  y    = score, 
  outlier.tagging = T,
  type = "robust", 
  p.adjust.method = "bonferroni", 
  pairwise.display = "all",
  # pairwise.comparisons = FALSE,   
  results.subtitle = F,
  bf.message = F
) + 
  ylab("selfesteem score")+
  theme_classic()+
  theme(legend.position = "top")

?ggwithinstats
```





## Check Sphericity & Normality assumptions

Now, the last thing is important - please check both the Sphericity & Normality assumptions, otherwise you'll choose a wrong test and either miss a discovery having a wrong big p-value (also called - Type II Error) or you'll find some nonsense having a wrong small p-value (also called - Type I Error). 

Repeated Measures ANOVA needs Sphericity, where Sphericity simply means that data-spread inside of the groups is similar. To say it a more correct but more boring way, - Sphericity is a condition where variances of the differences between all combinations of related groups don't differ. You know that **Variance** IS IMPORTANT, because the name of our test - ANOVA - is actually the abbreviation of the **AN**alises **O**f **VA**riances. However, since most of the real world data have different variances, {ggwithinstats} already accounts for Sphericity by default. You can still check Sphericity assumption by yourself and I'll show you how in a moment, but for now let's talk about normality...

ANOVA also needs the data to be normally distributed, or bell shaped, but often compares **a lot of groups**, so that checking normality of separate groups for usual ANOVA or differences between those groups for Repeated Measures ANOVA, might become cumbersome. 

The {aov_ez} function from {afex} package allows to easily check both assumptions. First, it conducts **Mauchly Test for Sphericity** and **automatically corrects p-values** of omnibus test. So that, if Sphericity was violated, you'll take the corrected p-value. Secondly, instead of checking the normality of thousands of groups, we can **check the normality of the residuals** of our ANOVA model, where all groups are already included. We then decide whether we stay with the **Parametric Repeated Measures ANOVA** if residuals are normally distributed, like in our example, or go to the **Nonparametric Friedman test** if residuals are not-normally distributed, which is a completely different story.

```{r}
# old hard way
# install.packages(afex)

library(afex)
hard <- aov_ez(
  data   = d,
  id     = "id", 
  dv     = "score",  
  within = "time")

summary(hard)

residuals(hard) %>% shapiro.test()
```


Sphericity is conceptually similar to homogeneity of variances in a between-subjects ANOVA. The violation of Sphericity makes repeated measures ANOVA to become too liberal (increases the Type I Error, or, as I like to say - increases the probability to find nonsense). Luckely, there are corrections for sphericity already build into {ggwithingstats}.

## What's next, or when not to use Repeated Measures ANOVA

- if samples are independent and normally distributed apply Student's (variances are similar) or Welsh ANOVA (homogeneity of variances is violated)
- if samples are small (n<30) and not-normally distributed, use [Friedman test](https://yury-zablotski.netlify.app/post/fr_test/)
- Repeated Measures ANOVA is actually capricious and cranky. It does not like missing values or not exactly the same number of repeated measures for all individuals (imbalanced design), it often overfits, checking assumptions gets to cumbersome if we want to add more then one predictors. Thus, the solution for almost all of these problems and the most logical next step in your learning journey are - [Mixed Effects Models](https://yury-zablotski.netlify.app/post/mixed-effects-models-1/).

---

If you think, I missed something, please comment on it, and I’ll improve this tutorial.

**Thank you for learning!**




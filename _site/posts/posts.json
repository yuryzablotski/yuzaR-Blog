[
  {
    "path": "posts/2021-01-06-survival-analysis-2-parametric-survival-models/",
    "title": "Survival analysis 2: parametric survival models",
    "description": "The non-parametric Kaplan-Meier method (KM) can not describe survival probability by a smooth function, which means it can not predict anything. The parametric models (e.g. Exponential, Weibull etc.) can! Besides, in case where parametric models are appropriate, they are more exact, more effective and more informative than KM or Cox. However, unfortunately, this step is often left out due to the rear use of parametric models. In this post we‚Äôll try to close this gap.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-06",
    "categories": [
      "survival analysis",
      "videos"
    ],
    "contents": "\n\nContents\nPrevious topics\nWhy do we need parametric survival modelsIs Time a Variable or a Constant?\nSteady change in hazard and survival\nPositive exponential change in hazard and survival\nNegative exponential change in hazard and survival\nB(u)ilding Exponential model ‚Ä¶ finally ü•≥\n\nHow to compute parametric models\nFinal thoughts\nFurther readings and references\n\n\n\nlibrary(tidyverse)  # data wrangling and visualization\nlibrary(sjPlot)     # for plotting results of log.regr.\nlibrary(effects)    # for probability output and plots\nlibrary(survival)   # for core survival analysis routines\nlibrary(survminer)  # for drawing survival curves\nlibrary(knitr)      # for a wonderful-looking tables\n\n\n\nPrevious topics\nA good understanding of Kaplan-Meier method (KM) is a prerequisite for this post, but since you are here, I suppose you are already familiar with it üòâ\nIf you are more of a visual person, you can watch the video below first, but for more details and R-code have a look at the article.\n\npreserve95e5e531c3d6d057\n\nWhy do we need parametric survival models\n\n\n\nThe main disadvantage of the non-parametric Kaplan-Meier method (KM) shown in the picture above is that it can not be described survival probability by a smooth function, which means it can not predict anything. The parametric models (e.g.¬†Exponential, Weibull etc.) can! Moreover, parametric models are the logical step on the way from the KM to the semi-parametric Cox models, because they beautifully connect the dots between KM and Cox models and thus greatly improve understanding of survival analysis. Besides, in case where parametric models are appropriate, they are more exact, more effective and more informative than KM or Cox. However, unfortunately, this step is often left out due to the rear use of parametric models. In this post we‚Äôll try to close this gap.\nIs Time a Variable or a Constant?\nSo, how can we describe the survival with a smooth function? To answer this question, let‚Äôs first describe a NOT-survival function, which happens to be a Hazard to die.\n\nImage by Leonardo Yip on unsplash.\nI‚Äôd describe death with two things: the event of death itself and a particular time point at which death happens. These two things always describe a single event, because one only dies üíÄ once. However, if several people die üíÄüíÄüíÄ, they will not die in the exact same moment, right? Nop. Thus, the time of death would vary and the number of death would grow over time, which would make the time itself a variable. That‚Äôs how several events of death at different time points allow us to express death in two ways:\nvia a different number of events per fixed unit of time, which is often called a Risk (Hazard) to die, or simply Hazard (\\(\\lambda\\) - lambda). This makes the number of events a variable, and Hazard a rate of death, e.g, per day. Or,\nvia different stretches of passing time per fixed number of events. The time interval is usually measured until the next event occurs. This makes the time a variable.\nBut whatever changes, the number of events per unite of time, or time per unit of events, the change itself is a key here, and there are different kinds of changes.\nSteady change in hazard and survival\nImagine that every day exactly 3 out of 10 people die in the cold ocean water after Titanic accident. It‚Äôs a pretty stable rate of death, or Hazard, of 30%. If the hazard to die would steadily grow with the same rate, then the probability of survival would steadily decrease at the same rate. Thus, Hazard and Survival can be expressed in terms of each other. Particularly, the Hazard of dying over time can be seen as a Failed survival (\\(F(t)\\) in the left formula below). Or, the Survival over time (\\(S(t)\\) the right formula below) can be seen as the Hazard of NOT-dying, or simply a negative Hazard, which mathematically can be expressed as a ‚Äú-‚Äù minus sign, or ‚Äú1 -‚Äù, in front of the Hazard. Both functions result in straight lines, where Hazard steadily increases and survival steadily decreases (plots below):\n\n\n\\[ F(t) = Hazard * t \\]\n\n\n\\[ S(t) = 1 - Hazard * t \\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuch steady increase or decrease in hazard or survival are rather not natural, not realistic. It‚Äôs kind of hard to plan and monitor death üòâ, except you are a serial killer. The hazard usually either exponentially increases, e.g.¬†in the case of hunger, or exponentially declines, e.g.¬†in the case of a pandemic after vaccine was found. Thus, let‚Äôs have a look at both exponential changes.\nPositive exponential change in hazard and survival\nThink about hunger for a moment. It accumulates, right? And the longer we stay hungry, the higher is the probability (risk) that we die. The left plot below shows such a development, where a Hazard of dying \\(F(t)\\) expressed in probabilities is small in the beginning (few deaths), but grows exponentially with time (more and more deaths). I like to call such a trend a positive, or accelerating, exponential change. The ‚Äúexp‚Äù in the left formula below is all we need to add in order to plot such a trend.\nAgain, since survival can be seen as a negative hazard, we can express the survival \\(S(t)\\) by simply using a minus sign (or ‚Äú1-‚Äù) in front of the Hazard. The plot on the right displays the results of such survival function. It‚Äôs kind of obvious, that if the hazard of dying is low in the beginning of ‚Äútime‚Äù, then the probability of survival is high. At the end of ‚Äútime‚Äù Survival exponentially drops due to a exponential increase of the Hazard.\n\n\n\\[ F(t)= exp^{Hazard * t} \\]\n\n\n\\[ S(t) = 1 - exp^{Hazard * t} \\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNegative exponential change in hazard and survival\nHowever, in some cases, more people die at the beginning of ‚Äútime‚Äù, where after rate of death declines over time. Think about a pandemic, or a titanic crash. The change is still exponential, and goes in the same direction: up, in the case of the Hazard, and down, in the case of survival. However, the exponential change is kind of turned (bend) inside out. I like to call it a negative, or decelerating, exponential change.\nSuch ‚Äúinside out bending‚Äù can be achieved by a ‚Äú-‚Äù minus sing in front of both a Hazard and the exponential function itself (left equation and picture below). This minus does not describe the negative hazard, as in the example above. It only changes the curve from accelerating to decelerating. In order to get survival for this function, we also, as in the example above, have to use a ‚Äú-‚Äù (or ‚Äú1-‚Äù) in front of the whole exponential hazard expression. Interestingly, two minus signs in front of the ‚Äúexp‚Äù in the Survival formula neutralize each other and become a plus, which leaves us with the \\(exp^{-Hazard * t}\\) (right equation and picture below).\n\n\n\\[ F(t)= - exp^{-Hazard * t} \\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\ \\]\n\n\n\n\n\n\\[ S(t) = 1 - ( - exp^{-Hazard * t}) = exp^{-Hazard * t} \\]\n\n\n\n\n\nB(u)ilding Exponential model ‚Ä¶ finally ü•≥\n\nImage by Mih√•ly K√∂les on unsplash.\nNow we finally have our smooth survival function we wanted to describe in the beginning. Let‚Äôs look at it one more time and plot it on top of the not-smooth Kaplan-Meier step function.\nSince survival function \\(S(t)\\) shows the probability of survival passed the certain time point,\n\\[ S(t) = P(T > t) \\]\nand the exponential function \\(F(t)\\) shows the probability of Failed-survival passed the certain time point,\n\\[ F(t) = P(T \\leq t) = 1 - exp^{-Hazard * t} \\]\na survival function can then be expressed in terms of not-survival (exponential) function:\n\\[ S(t) = P(T > t) = 1 - P(T \\leq t) = 1 - F(t) = 1 - [1 - exp^{-Hazard * t}] = exp^{-Hazard * t} \\]\nSo that the survival function, which shows a rate of decrease, is a flipped hazard function, which showed a rate of increase. Thus, we can rewrite the \\(S(t)\\) as:\n\\[S(t) = exp^{- Hazard * t} = exp^{- \\lambda* t} \\]\nwhere -Hazard and time are two parameters which describe our exponential change in survival probability, which is why such models do have their name - parametric exponential models. And since the Hazard is negative, and exponential function is not-linear (aka. curvy) - out model produces a negative exponential curve (see below). Such smooth rate of decrease describes survival probability much better then a Kaplan-Meier method, which abruptly (step-wisely) drops probability only after an event, while keeping the probability constant between the events.\nHow to compute parametric models\n\n\nlibrary(survival)    # main package for modelling survival data\nlibrary(survminer)   # visualization of survival model results\nlibrary(flexsurv)    # for Parametric Survival Modelling\n\nex <- flexsurvreg(Surv(time, status) ~ 1, data = d2, dist=\"exponential\")\nwe <- flexsurvreg(Surv(time, status) ~ 1, data = d2, dist=\"weibull\")\n\nggsurvplot(\n  ex, \n  conf.int = FALSE, \n  surv.median.line = \"hv\",\n  xlab = \"Days\", \n  ylab = \"Survival probability\", \n  break.time.by = 1,\n  risk.table = F\n)\n\n\n\n\nThe parameters of the curve (\\(-\\lambda * t\\)) allow us to model and predict survival and hazard over time, which is the main advantage of exponential models over the Kaplan-Meier method, which is not-parametric and therefore not ‚Äúmodellable‚Äù. However, the non-linearity is often troublesome, and we‚Äôd rather use the linear regression concept, which summarizes (regresses) a lot of numbers into a few numbers, like the intercept (\\(\\beta_0\\)) and the slope (\\(\\beta_1\\)). Fortunately, a non-linear curve can be easily ‚Äúlinearised‚Äù via a natural logarithm. For this, we don‚Äôt even have to understand how logarithm or the exponential function work, we only need to know that they neutralize each other. Moreover, using ‚Äúlog‚Äù (logarithmazing both sides of the equation below) produces three positive side effects:\nfirst, on the right side of the equation, this would transfer our curve (\\(-\\lambda * t\\)) into a line (\\(b_0 + b_1x_1 + ... + b_kx_k\\)), where we will be able to have an intercept and \\(\\beta\\) coefficients as in a usual linear logistic regression \\[Hazard = exp^{b_0 + b_1x_1 + ... + b_kx_k}\\]\nor\n\\[ log(Hazard) = b_0 + b_1x_1 + ... + b_kx_k \\]\nSuch survival model is in fact the Poisson model. Thus, it might help if you already know Poisson distribution. If not, that‚Äôs OK, you don‚Äôt have to understand Poisson before (citation from Prof.¬†Marin‚Äôs video, see references).\nsecondly, it will help us to connect to further models, like Weibull and Cox models, because the difference between them lie mainly in the intercept ‚Äú\\(b_0\\)‚Äù;\nand finally, it will greatly increase the interpretability, because the Hazard-Ratios (HRs) (that‚Äôs what exponential model delivers) can be interpreted exactly like the Odds-Ratios (ORs) from the logistic regression (as described in my posts on logistic regression). Similarly to the Odds in the logistic regression, the Hazard itself, which it the probability of dying NOW, is less useful then the Hazard-Ratios. A Hazard-Ratio is the ratio of hazard of somebody who is exposed (sick) to somebody who is not exposed (healthy). For instance if HR = 2, the risk of dying of somebody who is exposed is double compared to somebody who is not exposed.\nFinal thoughts\nAre parametric models useful? Of coarse!\n3 curves above which described different kind of changes in survival and hazard over time meant to say that distribution can be very different. If a suitable distribution can be found, parametric model is more informative than the KM or Cox model\nthey can predict survival probabilities at any given point in time, event hazard, mean and median survival times are readily available\nthey are also slightly more efficient and yield more precise estimates due a better fit (see the picture above)\nAre parametric models perfect? Of coarse not!\nparametric models need to specify the distribution, which may be difficult to identify\nthey are also mathematically more complex then, e.g.¬†KM, and are therefore rarer, which greatly decrease comparability of results among studies. Due to their lower popularity I will not go deeper into any particular parametric model: Weibull, Gompertz, Accelerated Failure Time models etc.\nThus, despite the fact that parametric models are a good alternative to the KM and Cox‚Äôs regression model (which do not need to specify any distribution), KM and Cox remain the most popular methods for analyzing survival data. And that is why the next logical step in your statistical journey would be learning about Cox Proportional Hazard Models (in progress).\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nFurther readings and references\nM J Bradburn, T G Clark, S B Love, & D G Altman. (2003). Survival Analysis Part II: Multivariate data analysis ‚Äì an introduction to concepts and methods. British Journal of Cancer, 89(3), 431-436.\nsurvminer cheat sheet: https://rpkgs.datanovia.com/survminer/survminer_cheatsheet.pdf\nthe whole playlist of videos from MarinStatsLectures! They are amazing! https://www.youtube.com/watch?v=vX3l36ptrTU&list=PLqzoL9-eJTNDdnKvep_YHIwk2AMqHhuJ0 especially the second video of marinstatslecture: https://www.youtube.com/watch?v=MdmWdIV5k-I&list=PLqzoL9-eJTNDdnKvep_YHIwk2AMqHhuJ0&index=2\n\n\n\n",
    "preview": "posts/2021-01-06-survival-analysis-2-parametric-survival-models/thumbnail_survival_2.png",
    "last_modified": "2021-01-06T10:31:39+01:00",
    "input_file": "survival-analysis-2-parametric-survival-models.utf8.md",
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/",
    "title": "R package reviews {performance} check how good your model is! ",
    "description": "There are several indicators of model quality, e.g. $R^2$ or AIC, and several assumption for every model which supposed to be checked, e.g. normality of residuals, multicollinearity etc.. R provides solutions for every indicator or assumption you can imagine. However, they are usually spread around different packages and functions. {performance} package brings all of quality indicators and all of the assumption under one roof. Thus, for me it became the one-stop solution for modelling.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-03",
    "categories": [
      "R package reviews",
      "videos",
      "visualization"
    ],
    "contents": "\n\nContents\nR demo for how check model performance and model assumptions\nCheck model performance, or model qualityUsual linear models\nComplex mixed-effects models\nFancy Bayesian mixed-effects models\nCompare models\nUse individual quality indicators\n\nCheck assumptions: or modern (Jan 2020) model diagnosticsAll assumptions at once!\nIndividual assumptions with reports of statistical tests!\n\nUseful references\n\n\n\nlibrary(tidyverse)          # data manipulation\nlibrary(performance)        # model performance\n\n\n\nR demo for how check model performance and model assumptions\nA short (ca. 15 min) video below shows how performance package works and the code you‚Äôll see in the video is provided below.\n\npreserve03620d5a50fb17f6\n\nCheck model performance, or model quality\nUsual linear models\nIn order to see how model performs, use the intuitive function - model_performance. It provide several quality indicators, which differ depending on the model. For instance the first model below provides the most ‚Äúclassical‚Äù quality indicators:\nAIC - Akaike‚Äôs Information Criterion, the lower the better. AIC is an estimator of out-of-sample prediction error and thereby relative quality of statistical models for a given set of data.\nBIC - Bayesian Information Criterion, the lower the better\n\\(R^2\\) - the proportion of the variance explained, the higher the better. It is sometimes referred to as a goodness of fit of the model\n\\(R^2 adjusted\\) - the proportion of the variance explained for multiple (several predictors) models, the higher the better\nRMSE - Root Mean Square Error is a measure of spread of the residuals around predictions (prediction errors), the lower the better\nSigma - standard deviation is a measure of spread of the data around the mean, the lower the better\n\n\nm <- lm(mpg ~ hp + cyl, data = mtcars)\n\nmodel_performance(m)\n\n\n# Indices of model performance\n\nAIC    |    BIC |   R2 | R2 (adj.) | RMSE | Sigma\n-------------------------------------------------\n169.56 | 175.42 | 0.74 |      0.72 | 3.02 |  3.17\n\nComplex mixed-effects models\nMixed effects model provide two different \\(R^2\\)s and ICC:\nconditional \\(R^2\\) shows model‚Äôs total explanatory power and\nmarginal \\(R^2\\) show the part related to the fixed effects (predictors) alone\nICC - intraclass correlation coefficient, is similar to \\(R^2\\) and also shows the goodness of fit or, in other words, quantifies the proportion of variance explained by a grouping (random) factor in mixed-effects (multilevel/hierarchical) models.\n\n\nlibrary(lme4)\n\nm1 <- lmer(mpg ~  hp * cyl + wt + (1 | am), data = mtcars)\n\nmodel_performance(m1)\n\n\n# Indices of model performance\n\nAIC    |    BIC | R2 (cond.) | R2 (marg.) |      ICC | RMSE | Sigma\n-------------------------------------------------------------------\n167.07 | 177.33 |       0.86 |       0.86 | 8.79e-03 | 2.06 |  2.24\n\nFancy Bayesian mixed-effects models\nELPD - expected log pointwise predictive density is a measure the prediction accuracy of Bayesian models, the closer to 0 the better. The out-of-sample predictive fit can either be estimated by Bayesian leave-one-out cross-validation (LOO) or by widely applicable information criterion (WAIC). Thus, the two next indicators are:\nLOOIC - leave-one-out cross-validation information criterion, the lower the better\nWAIC - widely applicable information criterion, the lower the better\n\n\nlibrary(rstanarm)\n\nm2 <- stan_glmer(mpg ~ hp * cyl + wt  + (cyl | am), data = mtcars, refresh=0)\n\nmodel_performance(m2)\n\n\n# Indices of model performance\n\nELPD   | ELPD_SE |  LOOIC | LOOIC_SE |   WAIC |   R2 | R2 (marg.) | R2 (adj.) | RMSE | Sigma\n--------------------------------------------------------------------------------------------\n-75.54 |    3.95 | 151.09 |     7.91 | 150.59 | 0.86 |       0.86 |      0.83 | 2.15 |  2.25\n\nCompare models\nUnfortunately we can‚Äôt compare not similar models, for instance different types of models (e.g.¬†linear vs.¬†mixed effects) or models done with different amounts of data, because one of them will be better for reasons other then model quality. Even more unfortunately, such comparisons are often conducted anyway. I was also guilty of it in the past. One of the common R functions allowing such comparisons is anova(model1, model2). It smartly does not allow the comparison of linear vs.¬†mixed effects models (see the red warning below), but stupidly can be out-tricked by placing mixed effects model first.\n\n\nanova(m, m1)\n\n\n\nError: $ operator not defined for this S4 class\n\n\nanova(m1, m)\n\n\nData: mtcars\nModels:\nm: mpg ~ hp + cyl\nm1: mpg ~ hp * cyl + wt + (1 | am)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm     4 169.56 175.43 -80.781   161.56                         \nm1    7 151.05 161.31 -68.524   137.05 24.513  3  1.952e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nIn contrast compare_performance() compares them immediately and provides a useful warning (see below). It also provides an somewhat superficial (according to the package author), but still useful ranking of models (see Performance-Score).\n\n\ncompare_performance(m, m1, rank = T)\n\n\n# Comparison of Model Performance Indices\n\nModel |    Type |    AIC |    BIC |    BF | RMSE | Sigma | Performance-Score\n----------------------------------------------------------------------------\nm1    | lmerMod | 167.07 | 177.33 | 0.385 | 2.06 |  2.24 |            75.00%\nm     |      lm | 169.56 | 175.42 |  1.00 | 3.02 |  3.17 |            25.00%\n\nWarning: Models are not of same type. Comparison of indices might be not meaningful.\nHowever, if we compare models which are comparable, no warning will be displayed.\n\n\nm1.1 <- lmer(mpg ~  hp + cyl + wt + (1 | am), data = mtcars)\ncompare_performance(m1, m1.1, rank = T)\n\n\n# Comparison of Model Performance Indices\n\nModel |    Type |    AIC |    BIC |   BF | R2 (cond.) | R2 (marg.) |      ICC | RMSE | Sigma | Performance-Score\n----------------------------------------------------------------------------------------------------------------\nm1    | lmerMod | 167.07 | 177.33 | 1.00 |       0.86 |       0.86 | 8.79e-03 | 2.06 |  2.24 |            71.43%\nm1.1  | lmerMod | 164.24 | 173.03 | 8.58 |       0.83 |       0.83 | 8.48e-03 | 2.35 |  2.51 |            28.57%\n\nMoreover, performance package can also easily plot this comparison, simply by wrapping up the compare_performance() function into a plot() function. It could not be easier or more elegant than that!\n\n\nplot( compare_performance(m1, m1.1) )\n\n\n\n\nUse individual quality indicators\n\n\nr2(m1)    # model fit - the proportion of the variance explained\n\n\n# R2 for Mixed Models\n\n  Conditional R2: 0.864\n     Marginal R2: 0.862\n\nr2(m2)    # Bayesian R2 even with Credible Intervals\n\n\n# Bayesian R2 with Standard Error\n\n  Conditional R2: 0.863 (89% CI [0.805, 0.918])\n     Marginal R2: 0.861 (89% CI [0.769, 0.931])\n\nicc(m2)   # Intraclass Correlation Coefficient (ICC)\n\n\n# Intraclass Correlation Coefficient\n\n     Adjusted ICC: 0.971\n  Conditional ICC: 0.829\n\n# not part of the \"performance\" package, but fits well here\nAIC(m1)   # Akaike's Information Criterion\n\n\n[1] 167.0742\n\nBIC(m1)   # Bayesian Information Criterion\n\n\n[1] 177.3343\n\nCheck assumptions: or modern (Jan 2020) model diagnostics\nAll assumptions at once!\nThe first time I discovered check_model() function, I have got an intensive intellectual ‚Äúnerdgasm‚Äù. I could not believe how simple and at the same time sophisticated this function is! I then checked out other work of the authors of {performance} package and was stunned, how many useful things they already produced. And I don‚Äôt think they‚Äôll stop any time soon. Thus, if you just began to learn R and stats, check out their work to quickly step up your data science game!\n\n\ncitation(\"performance\")\n\n\n\nTo cite performance in publications use:\n\n  L√ºdecke, Makowski, Waggoner & Patil (2020). Assessment of\n  Regression Models Performance. CRAN. Available from\n  https://easystats.github.io/performance/\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {performance: Assessment of Regression Models Performance},\n    author = {Daniel L√ºdecke and Dominique Makowski and Philip Waggoner and Indrajeet Patil},\n    journal = {CRAN},\n    year = {2020},\n    note = {R package},\n    doi = {10.5281/zenodo.3952174},\n    url = {https://easystats.github.io/performance/},\n  }\n\nNow the function itself: it visually checks all the assumptions you need to check and gives you a big-picture overview of assumptions for almost any model you can have (at least for all the common ones). Two examples below display such a big-picture of a usual linear model and a mixed-effects model with random effects. The subplots of this picture even explain what you should look for! For instance: ‚ÄúDots should be plotted along the line‚Äù in the residuals diagnostics plots. Have a look at the big-picture first and go to the next chapter for some more details on the particular assumption. Why? Because the package provides the opportunity to check individual assumptions too and even goes one step deeper into it!\n\n\ncheck_model(m)\n\n\n\n\n\n\ncheck_model(m1)\n\n\n\n\nIndividual assumptions with reports of statistical tests!\nChecking individual assumption is also very intuitive. For instance, for checking the normality of the residuals, use check_normality() function. It will conduct the Shapiro-Wilk Normality test and report the result of it. Put check_normality(m) inside of the plot() function to visualize the result. It is even preferable to visually inspect the residuals, because Shapiro-Wilk Test will often produce significant results for large sample sizes (and in the age of big data we always have large samples) even if data is perfectly normally distributed.\n\n\ncheck_normality(m)     # shapiro.test, however, visual inspection (e.g. Q-Q plots) are preferable\n\n\nOK: residuals appear as normally distributed (p = 0.212).\n\ncheck_normality(m1)\n\n\nWarning: Non-normality of residuals detected (p = 0.010).\n\nplot( check_normality(m1) )\n\n\nWarning: Non-normality of residuals detected (p = 0.010).\n\n\nThe collinearity is measured by the variance inflation factor (VIF). VIF<5 is acceptable for individual predictors, while VIF<10 is moderate, so, that it gives you an idea that some variables in the model might be redundant. If you model interactions, VIF would naturally increase, and the unwritten (I forgot where did I learned it from üôà) rule is that interactions with VIF<20 are still acceptable.\n\n\ncheck_collinearity(m)  # by variance inflation factor (VIF)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n Parameter  VIF Increased SE\n        hp 3.26         1.80\n       cyl 3.26         1.80\n\ncheck_collinearity(m1)\n\n\n# Check for Multicollinearity\n\nLow Correlation\n\n Parameter  VIF Increased SE\n        wt 2.50         1.58\n\nHigh Correlation\n\n Parameter    VIF Increased SE\n        hp  78.22         8.84\n       cyl  12.22         3.50\n    hp:cyl 122.72        11.08\n\nplot( check_collinearity(m1) )\n\n\n\n\nIf you wanna check the heteroscedasticity, use function check_heteroscedasticity(). The heteroscedasticity assumption itself is kind of self-explanatory. But what I can‚Äôt explain is that why nobody in the history of R programming language came up with such intuitive functions before {performance}?\n\n\ncheck_heteroscedasticity(m)\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.041).\n\ncheck_heteroscedasticity(m1)\n\n\nOK: Error variance appears to be homoscedastic (p = 0.153).\n\nplot( check_heteroscedasticity(m) )\n\n\nWarning: Heteroscedasticity (non-constant error variance) detected (p = 0.041).\n\n\nOne particular function somehow didn‚Äôt finish up in the ‚Äúbig-picture‚Äù. Namely check_outliers(). May be because there are too many methods for identifying outliers (‚Äúzscore‚Äù, ‚Äúiqr‚Äù, ‚Äúcook‚Äù, ‚Äúpareto‚Äù, ‚Äúmahalanobis‚Äù, ‚Äúrobust‚Äù, ‚Äúmcd‚Äù, ‚Äúics‚Äù, ‚Äúoptics‚Äù, ‚Äúlof‚Äù)? I don‚Äôt know. Z-Scores is the default method though, however, the method can be easily specified as an argument of the function:\n\n\ncheck_outliers(mtcars$mpg)\n\n\nWarning: 4 outliers detected (cases 18, 19, 20, 28).\n\nplot(check_outliers(mtcars$mpg))\n\n\n\nplot(check_outliers(mtcars$mpg, method = \"iqr\"))\n\n\n\n\nThere are many more useful functions in this package. And there is no need to describe them all here. If you liked what you have seen so far, just type ‚Äú?performance‚Äù in the RStudio console, go to Help, scroll down to the bottom of the Help page, click Index and enjoy the package üòâ.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nCheers guys, have a good one!\nUseful references\nhttps://easystats.github.io/performance/\nhttps://cran.r-project.org/web/packages/performance/performance.pdf\nhttps://www.rdocumentation.org/packages/performance/versions/0.6.1\n\n\n\n",
    "preview": "posts/2021-01-03-r-package-reviews-performance-check-how-good-your-model-is/thumbnail_performance.jpg",
    "last_modified": "2021-01-03T14:59:21+01:00",
    "input_file": "r-package-reviews-performance-check-how-good-your-model-is.utf8.md"
  },
  {
    "path": "posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/",
    "title": "Survival analysis 1: a gentle introduction into Kaplan-Meier Curves",
    "description": "Survival time analysis is necessary in any study which investigates the time to a particular outcome of interest. Cancer studies in the medicine and the first failure of the car in the engineering field (failure time analysis) are good examples. The outcome of interest could be death, remission to relapse, progression, or failure. Point in time of reaching that outcome is generally called the event. Thank goodness, not every ‚Äúevent‚Äù is fatal üòÉ, but can sometimes even be a favorable outcome such as discharge from hospital. And thus, survival analysis is also a generic term, because it is not only about survival.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-03",
    "categories": [
      "survival analysis",
      "videos"
    ],
    "contents": "\n\nContents\nPrevious topics\nWhy do we need survival analysis?Death is not the only option! Or: ‚ÄúWhat is an event?‚Äù\nCensoring\n\nHow to calculate Kaplan-Meier survival curve ‚Äúmanually‚Äù step by stepSurvival probability\n\nHow to compute Kaplan-Meier survival curveInterpretation of Kaplan-Meier Curve\n\nComparing survival of groups2 groups\nInterpretation of groups comparison using 4 benchmarks\nLog-Rank test\n> 2 groups and multiple pairwise (post-hoc) Log-Rank test\nMultiple survival curves\n\nConclusions\nWhat‚Äôs next?\nFurther readings, videos and references\n\n\n\nlibrary(tidyverse)  # data wrangling and visualization\nlibrary(sjPlot)     # for plotting results of log.regr.\nlibrary(effects)    # for probability output and plots\nlibrary(survival)   # for core survival analysis routines\nlibrary(survminer)  # for drawing survival curves\nlibrary(knitr)      # for a wonderful-looking tables\n\n\n\nIf you are more of a visual person, you can watch the video below first, but for more details and R-code have a look at the article.\n\npreserveb92bc3428b704955\n\nCan there be something more horrifying, then a Titanic crash with lots of immediate deaths? Well, unfortunately, yes! Namely, not immediate deaths. Imagine survived people in the middle of the cold dark ocean, some in life jackets, some without, clinging on ship remains, with a huge panic and little hope for rescue. There will be no helicopter to save them (it‚Äôs 1912). There might not be a single ship in those waters for months. How long will they survive? Will time simply prolong their suffering or increase the probability of survival, because eventually they will be found by some random ship? Do women have higher chances of survival then men? What about rich vs.¬†pure passengers? Well, in this post we‚Äôll find answers to all of these questions and along the way learn how survival analysis works. Particularly, we will:\nlearn about the most important concepts of survival analysis: survival curve, censoring and log-rank test,\nmanually calculate, then compute and interpret survival curves, and\ntest survival differences between two or more groups, e.g.¬†females vs.¬†males\n\nSource\nPrevious topics\nA good understanding of linear and logistic regressions would improve the digestion of this post.\nWhy do we need survival analysis?\n\n\nlibrary(tidyverse)\n\nd <- tibble(\n  time   = c(1,1,1,2,2,3,4,5,7,10), \n  status = c(1,1,1,1,0,1,0,0,1,0)\n)\n\nd %>% kable()\n\n\ntime\nstatus\n1\n1\n1\n1\n1\n1\n2\n1\n2\n0\n3\n1\n4\n0\n5\n0\n7\n1\n10\n0\n\nTo answer this question, let‚Äôs start with a small example of Titanic survivors: only 10 people and only 10 days on the sea after the crash and analyse it with the classic statistical methods, linear and logistic regressions. We are interested in how many people and how long survived and how many died on a particular day. Thus, two main parameters of interest for us are (1) time (in days) and (2) ‚Äúlife-status‚Äù of people, where ‚Äústatus = 1‚Äù means death and ‚Äústatus = 0‚Äù means not-death. And having the status of people we could just have counted survivors and not-survivors at each particular day to understand what was going on, wright?\n\n\nggplot(d, aes(time, fill = factor(status)))+\n  geom_bar(position = position_dodge())+\n  theme_bw()\n\n\n\n\nWell, not quite. Besides, the fact that the plot with our counts is not particularly revealing, we have two problems with that:\nThe first one is that ‚Äústatus = 0‚Äù does not always mean a survival. First, the person might be carried away by the waves and disappear. We lost the person and we don‚Äôt know it‚Äôs status. It might have died, it might have been saved by a fisher boat or is still alive somewhere in the ocean. We are not sure. The only thing what we are sure about is that this person can not be counted as dead or survived. Secondly, if some people survived 10 days, they might die at day 11. So, the time plays a role here and brings us to the second problem.\nWe do not have the data for every of these 10 days. Only for 7. Moreover, if I did not eat anything for 5 days, my survival probability at day 5, where I certainly survived, is not 100%. It is lower, because hunger accumulates over time. Similarly, a decrease of my survival probability also accumulates over time, which can not be determined by a simple counting of survivors vs.¬†not-survivors at a particular day.\nDespite this problems we still can analyse these data. We only need to find the right method for it.\nLooking at the numeric variable ‚Äútime‚Äù, we might be tempted to use a linear regression to model the survival time for two different status groups, 0 & 1:\n\n\nm <- lm(time ~ status, d %>% mutate(status = factor(status)))\n\nlibrary(effects)\nplot(allEffects(m))\n\n\n\n\nBut when we plot the model results, we realize that something isn‚Äôt quite right there. All the information we have to model the survival time are zeros and ones. And the average time of dying (status = 1) of 2.5 days misses lot‚Äôs of information. For instance, half of the people (3 persons) died on the very first day! And the further we go in time, the less additional people die, which we can see on the very first counts-plot above. So, the survival time is not really linear and can certainly not be described with only zeros and ones! Thus, a linear model does not seem to be useful here. Then what is?\nSince we have zeros and ones in the status column we can use a logistic regression, right?! Yes! Especially because probability curve is not linear and will catch the trend of survival over time. Cool! Let‚Äôs go with it:\n\n\nm <- glm(status ~ time, d, family = binomial)\n\nlibrary(sjPlot)\nplot_model(m, type = \"pred\", ci.lvl = NA)\n\n\n$time\n\n\nNow we have the non-linear survival probabilities :), which is a great improvement as compared to a linear regression. However, the fifth observation has a status of 0 at day 2. This means that the person survived for 2 days for sure, but was lost after it. ‚ÄúSo what?‚Äù, - you might ask. The probabilities in logistic regression would count this person as a 100% survivor after the second day simply because it‚Äôs status is not ‚Äú=1‚Äù, which is wrong (or biased), since we don‚Äôt know whether this person survived. Thus, logistic regression overestimates survival probability and is therefore also an inappropriate tool to analyze survival data.\nSo, it looks like we can‚Äôt analyse survival data with classic methods. And that is exactly why we need survival analysis method.\nWhile linear regression models describe the time, but miss a non-linear survival probability, logistic regression catches a non-linear trend, but overestimates survival probability. In contrast, survival analysis solves both issues, since it models non-linear survival probabilities over time while accounting for lost subjects of the study which are either dead nor survived.\nDeath is not the only option! Or: ‚ÄúWhat is an event?‚Äù\n\nImage by Luke Southern on unsplash.\nSurvival time analysis is necessary in any study which investigates the time to a particular outcome of interest. Cancer studies in the medicine and the first failure of the car in the engineering field (failure time analysis) are good examples. The outcome of interest could be death, remission to relapse, progression, or failure. Point in time of reaching that outcome is generally called the event. Thank goodness, not every ‚Äúevent‚Äù is fatal üòÉ, but can sometimes even be a favorable outcome such as discharge from hospital. And thus, survival analysis is also a generic term, because it is not only about survival.\nThe event, as a final fixed point in time is needed because we can‚Äôt observe or experiment forever. If we study lung cancer, we can‚Äôt wait until patients die from other causes, e.g.¬†being old. Only when some of patients survive cancer, or some cars don‚Äôt break by a certain time (event), we can get valuable insights. For instance, what is the time from the start of the treatment to progression, or what is the probability to survive lung cancer after exactly 1 year? Moreover, it opens the possibility to compare survival or failure times among different groups, e.g.¬†lung cancer between smokers and non-smokers, or breakdown time between German and Korean cars.\nSo, the survival time has a start and a finish point - the event. But what if a patient withdraw from the study due to a personal reasons, or a car got stolen 1 day before the event? Will they ‚Äúsurvive‚Äù by the time of the event? We don‚Äôt know! But they certainly survived from the start of the study until the point we lost them. And this is a valuable information we surely want to include in our analysis! But how do we differentiate ‚Äúlost‚Äù survivors from the ‚Äúreal‚Äù survivors? Well, we just call them a new name - censored. Str–∞nge word to apply to a person, right? But as I thought about the inappropriate information in a book or swearing on TV, which is often getting censored, the concept of censoring a patient became more digestible to me. See, since we can not say that person has died or is still alive, simply because we DO NOT HAVE ENOUGH INFORMATION, both of conclusions would be inappropriate, and thus, we censor this person. The concept of censoring is so important, that it deserves an extra chapter.\nCensoring\n\nThe need for censoring arises from the fact that the actual survival times will be unknown for some individuals. There are lots of ways to loose a subject of study:\nwe can loose an individual for no reason, when it simply does not appear anymore\nthe individual may experience another event, e.g.¬†death or accident\neven patients who survived till the very end of the study can be treated as censored due the unknown survival time (I personally prefer to see these patients as survived)\nAll examples above are considered to be right censoring due to their direction from left to right on the time-axes. Most of survival data are right censored. There are two other kinds of censoring. Left censoring appears if we do not know where the sickness began, while interval censoring happens when the exact time of patient loss is not known, but only a time window. Both left and interval censorings are rare, difficult to analyse, are often a result of a bad study design, and thus, will not be covered here.\nImportant to remember is that censored observations still provide useful information! That is why they need to be included into analysis.\n\nHow to calculate Kaplan-Meier survival curve ‚Äúmanually‚Äù step by step\nImagine a peaceful sunny day right before the Titanic crash. No one has died, but all 10 people are at high risk (hazard) of death, they just don‚Äôt know it yet. Guess, how many people would probably die the day before the crash? The answer is - probably zero. Literally, the probability of dying is zero, because we know the crash will not happen the day before the actual crush. To answer this question more properly, we need to remember the definition of probability: Probabilities are ratios of something happening, to everything what can happen. Thus, if nobody out of all 10 people died the day before the crash, the probability of dying is 0%, while the probability of surviving is 100%:\n\\[ probability \\ of \\ dying = \\frac{0}{10} = 0\\]\n\\[ probability \\ of \\ surviving = \\frac{10}{10} = 1  = 100\\%\\]\nFor a better representation, let‚Äôs put all the numbers in one single table:\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\", \"10/10 = 1\"\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n\nThen, at the day of the crash, where all 10 people are now know that they are at high risk of dying, 3 of them have actually died. The probability of dying at the first day is then \\(3/10 = 0.3 \\approx 30\\%\\) and the probability of surviving is \\(7/10 = 0.7 \\approx 70\\%\\). As you can see, the survival can be also calculated from the probability of dying \\(1-0.3 = 0.7\\), which is sometimes very useful.\nSurvival probability\n\nOne important moment here is that the probability of surviving is cumulative, which means it accumulates day by day. This accumulation happens via multiplying the new probability of surviving day 1 \\((1 - \\frac{d_i}{n_i}) = (1 - \\frac{3}{10}) = 70\\%\\) by the old probability of surviving all the time before, which in our case is the day before the accident, or - day zero \\(S(t_{i-1}) = 100\\%\\):\n\\[ S(t_i) = S(t_{i-1})*(1 - \\frac{d_i}{n_i}) = 1 * (1 - \\frac{3}{10}) = 0.7 \\]\nWhere,\n\\(S(t_{i‚àí1})\\) = the probability of being alive at \\(t_{i‚àí1}\\)\n\\(n_i\\) = the number of patients alive just before \\(t_i\\)\n\\(d_i\\) = the number of events at \\(t_i\\)\n\\(t_0\\) = 0, \\(S(0)\\) = 1\nThe survival probability at a certain time, \\(S(t)\\), is conditional because the person needs to have survived beyond that certain time, e.g.¬†the zero day (that‚Äôs the condition) in order to remain in the experiment for the first day.\n\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\",   \"10/10      = 1\",\n  1, 10, 3, \"3/10 = 0.3\", \"1 * (7/10) = 0.7\",\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n1\n10\n3\n3/10 = 0.3\n1 * (7/10) = 0.7\n\nThe second day is a little more interesting, because one person died and one simply disappeared (status = 0). This lost person was most likely carried away by the waves and hopefully was rescued. The hope is big, because the further people are scattered in the ocean, the higher the chances are that one of them will be found alive. And this one will let the world know that others are still out there. So, despite the fact that missing person seems bad, it might turn out to be a good thing. So, we certainly can not count a missed person as - dead. Besides, as a German proverb says: ‚ÄúHope is the last to die‚Äù üòâ.\nBut since we are also not sure whether this person is still alive, we can‚Äôt say - the person survived. That produces a dilemma: despite not being dead, the person is not part of our experiment anymore and we have to remove it from the number of people at risk. So, while the second day would have 7 people at risk left (since 3 have died at the very first day), the third day would be left with only 5 people, because 1 person died and one disappeared (was censored) as compared to day two. Similarly to the first day, the survival probability is cumulative and always includes the probability of the day before.\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\",    \"10/10       = 1\",\n  1, 10, 3, \"3/10 = 0.3\",  \"1 * (7/10)  = 0.7\",\n  2, 7,  1, \"1/7  = 0.14\", \"0.7 * (6/7) = 0.6\",\n  3, 5,  1, \"1/5  = 0.20\", \"0.6 * (4/5) = 0.48\"\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n1\n10\n3\n3/10 = 0.3\n1 * (7/10) = 0.7\n2\n7\n1\n1/7 = 0.14\n0.7 * (6/7) = 0.6\n3\n5\n1\n1/5 = 0.20\n0.6 * (4/5) = 0.48\n\nThe next two days two people disappeared, so, we censor them. There is no need to calculate something for censored data in survival analysis, because we don‚Äôt know whether they survived or not. And since we are interested in either survival or death, the final table below contains only dead cases. At day 6 nobody died or disappeared, that is why we do not calculate anything for that day either. The last person in our experiment died at day 7. So, ‚ÄúN_died‚Äù will be 1, and ‚ÄúN_at_risk‚Äù will be 2, since 1 out of remaining 5 has died and 2 were censored. Thus, the full ‚Äúmanually‚Äù calculated table will look like this:\n\n\ntribble(\n  ~time_in_days, ~N_at_risk, ~N_died, ~P_dying, ~`P_surviving`,\n  0, 10, 0, \"0/10 = 0\",    \"10/10        = 1\",\n  1, 10, 3, \"3/10 = 0.3\",  \"1 * (7/10)   = 0.7\",\n  2, 7,  1, \"1/7  = 0.14\", \"0.7 * (6/7)  = 0.6\",\n  3, 5,  1, \"1/5  = 0.20\", \"0.6 * (4/5)  = 0.48\",\n  7, 2,  1, \"1/2  = 0.50\", \"0.48 * (1/2) = 0.24\"\n) %>% kable()\n\n\ntime_in_days\nN_at_risk\nN_died\nP_dying\nP_surviving\n0\n10\n0\n0/10 = 0\n10/10 = 1\n1\n10\n3\n3/10 = 0.3\n1 * (7/10) = 0.7\n2\n7\n1\n1/7 = 0.14\n0.7 * (6/7) = 0.6\n3\n5\n1\n1/5 = 0.20\n0.6 * (4/5) = 0.48\n7\n2\n1\n1/2 = 0.50\n0.48 * (1/2) = 0.24\n\nBut what happens if we ignore censoring and simply calculate the probability of being not dead? Well, since 4 people did not die on day 7, the probability of survival would be \\(\\frac{4}{10} = 40\\%\\) which is almost twice as high, and thus we‚Äôll massively overestimate survival probability, as compared to the actual probability of 0.24 which accounts for censoring!\n\nStory time:\n\nSource\nOnly paying attention to survivors even has a name - survivorship bias, and there is a small story to it. During the Second World War some planes came back from the battle field with a lot of damage from bullets. They barely could fly, but they still came back. So, the military decided to protect the aircraft with more armor at the places where the most bullet-holes were, like wings, and reduce the armor at the places with no bullet-holes. Surprisingly, the percentage of planes which came back did not increase. The engineers were puzzled! Until one mathematician, Abraham Walt, which were invited to solve this problem said: ‚ÄúPut more armor on places with no bullet-holes, because if these place are shot, the plane won‚Äôt come back.‚Äù And as the others thought about it, they realized that all the planes which came back did not have any bullet-holes on the cockpit or the engines. The bullet-holes shows all the places where the aircraft can be shot but still come back, or survive! That is the survivorship bias. So, it seems to me, that logistic regression has a survivorship bias as compared to the survival analysis if we wanna analyse survival data.\n\nHow to compute Kaplan-Meier survival curve\nLoad all needed packages at once to avoid interruptions.\n\n\nlibrary(tidyverse)  # data wrangling and visualization\nlibrary(knitr)      # beautifying tables\nlibrary(car)        # for checking assumptions, e.g. vif etc.\nlibrary(broom)      # for tidy model output\nlibrary(sjPlot)     # for plotting results of log.regr.\nlibrary(sjmisc)     # for plotting results of log.regr.\nlibrary(effects)    # for probability output and plots\n\n\n\nFirst of all, please, be sure you know exactly what 0s and 1s in your data mean! Because in a logistic regression 1 is the survival, while in a survival analysis 1 is death! Secondly, we have to differentiate censored cases somehow, e.g.¬†we can mark them with a plus sign in the data or on the plot. The data below shows that people were censored on days 2+, 4+, 5+ and 10+. Install and load a survival package to be able to execute the code below:\n\n\n# install.packages(\"survival\")\nlibrary(survival)\n\nSurv(time = d$time, event = d$status)\n\n\n [1]  1   1   1   2   2+  3   4+  5+  7  10+\n\nThe Surv function unites ‚Äútime‚Äù and ‚Äústatus‚Äù data into a single ‚Äúsurvival‚Äù object, which allows to account for censored observations. This object can then be used to model survival probability by the survfit function. We model the survival by adding ‚Äú~ 1‚Äù to the object, where 1 means no variables which could have influenced the survival. Our survival object on the left side of the tilde is then the response variable and on the right site of the ~ (tilde) are predictors, or in our case of ‚Äú1‚Äù - nothing. Let‚Äôs produce our fist survival model and have a look at the model output:\n\n\nsurvival_model <- survfit(Surv(time, status) ~ 1, data = d)\n\n# small summary\nsurvival_model\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n      n  events  median 0.95LCL 0.95UCL \n     10       6       3       1      NA \n\nwhere we have:\nn - number of participants,\nevents - number of deaths, which are generically called event since you can study other types of ‚Äúdeadlines‚Äù üòÇ, like relapse of the sickness, or first break of the car,\nthe median survival time estimated instead of the mean due to a non-parametric (explained later) nature of survival analysis and\n95% confidence intervals for the median survival time\nMuch more information can be assessed by the summary function, which, surprisingly üòâ, delivers the same table which we just calculated above manually:\n\n\n# big summary\nsummary(survival_model)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     10       3     0.70   0.145       0.4665        1.000\n    2      7       1     0.60   0.155       0.3617        0.995\n    3      5       1     0.48   0.164       0.2458        0.938\n    7      2       1     0.24   0.188       0.0515        1.000\n\nNow you can be sure, our calculations were correct and you have successfully learned how to do survival analysis. Congrats! So that from now on you can start using software. It has a lot of advantages! For instance, a summary function, which only displays the result of dead cases, can be filled with an additional argument censored = TRUE, which displays all, dead and censored cases:\n\n\nsummary(survival_model, censored = T)\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    1     10       3     0.70   0.145       0.4665        1.000\n    2      7       1     0.60   0.155       0.3617        0.995\n    3      5       1     0.48   0.164       0.2458        0.938\n    4      4       0     0.48   0.164       0.2458        0.938\n    5      3       0     0.48   0.164       0.2458        0.938\n    7      2       1     0.24   0.188       0.0515        1.000\n   10      1       0     0.24   0.188       0.0515        1.000\n\nGreat! Right? But what if you have thousands of days (your table would be huge!), but you are interested in only a few of them? Well, you can specify the results of which day you want to see by adding a times = ... argument to the summary function. A cool üòé thing about it is that you can even ask for the days, where no data were available for, e.g.¬†day 8 or 9 in our simple example. For instance, the probability of survival beyond day 8 is 24%:\n\n\nsummary(survival_model, times = c(8, 9))\n\n\nCall: survfit(formula = Surv(time, status) ~ 1, data = d)\n\n time n.risk n.event survival std.err lower 95% CI upper 95% CI\n    8      1       6     0.24   0.188       0.0515            1\n    9      1       0     0.24   0.188       0.0515            1\n\nNow, since we know the exact probability of survival on each day, even on days we did not have the data for, we can visualize the model results. Install and load a survminer package to be able to execute the code below:\n\n\n# install.packages(\"survminer\")\nlibrary(survminer)\n\nggsurvplot(\n  survival_model, \n  conf.int = FALSE, \n  surv.median.line = \"hv\",\n  xlab = \"Days\", \n  ylab = \"Survival probability\", \n  break.time.by = 1,\n  risk.table = T\n)\n\n\n\n\nDespite the fact that we could have easily calculated the probabilities for every day, you‚Äôll never do it by hand. It could be thousands of day. Thus, you‚Äôll let the software do the work and you‚Äôll get much more results then you can and want calculate yourself, e.g.¬†confidence intervals (CIs) for everyday survival or the survival plot. The CIs are visualized per default, but we can remove them if needed via the conf.int = FALSE command. The last row in the code above displays a ‚ÄúNumber at risk‚Äù table, which we also calculated manually above. So, why did we calculate something manually at all then, if we can get everything and more from the software? Well, it was important to go through the calculation process step-by-step in order to increase your intuition about the survival analysis curve!\nInterestingly, this curve was independently described by two different scientists at the same time. Edward Kaplan and Paul Meier then published their findings together as the Kaplan-Meier (KM) estimator  in 1958.1 And survfit function fits type = \"kaplan-meier\" curve by default.\nMost of the survival data shows a lot of events in the beginning and lower number of events throughout the time. This makes the survival curve non-linear and most the survival data skewed or non-normally (not bell shaped) distributed. These are the reasons why KM method estimates median survival time instead of the mean as a measure of central tendency. The non-linearity and the ‚Äústepiness‚Äù of the KM curve make it impossible to summarize the data into one single parameter, e.g.¬†the slope, which makes the KM method non-parametric.\nInterpretation of Kaplan-Meier Curve\nThe x-axis represents time in days, and the y-axis shows the probability of surviving or the proportion of people surviving. So, the curve itself shows the exact survival probability over time. A vertical drop in the curve indicates at least one event. The height of a vertical drop shows the change in cumulative survival probability. A horizontal part of the curve represents survival duration for the certain time interval, which is terminated by the next event (and drop of the curve). The KM curve looks like a strange staircase with uneven steps, where survival probability is constant between the events, and is therefore a step function that changes value only at the time of each event. In this way each patient contributes valuable information to the calculations for as long as it is alive. Censored people are shown exactly like in the survival object, with pluses, as you can see on the day 2. However, most of the pluses look like vertical ticks, since they lie on the horizontal part of the curve, i.e.¬†days 4 and 5. The tick marks are shown by default, but could be suppressed using the argument censor = FALSE. The risk table below the plot shows the number of people at risk, which are actually all ‚Äúreally alive‚Äù people in the experiment which did not experience the event or censoring at a particular time point. Dashed line represents the median survival time which corresponds to a survival probability of 50%. And if we ignore censoring and simply estimate the median of time (for only dead people), we‚Äôll get 1.5 instead of 3, which will increase the survival provability from 48% to 70%. Again, ignoring censoring will result into overestimation of survival probability due to a survivorship bias.\n\n\nd %>% \n  filter(status == 1) %>% \n  summarize(median_survival_whithout_censoring = median(time))\n\n\n# A tibble: 1 x 1\n  median_survival_whithout_censoring\n                               <dbl>\n1                                1.5\n\nComparing survival of groups\n2 groups\nNow, let‚Äôs follow 100 people after Titanic crash instead of 10 and look at their survival using the Kaplan-Meier curve below. The numbers in the risk table are getting bigger and we‚Äôd better display the percentages at risk in brackets near the absolute values. The risk.table = \"abs_pct\" argument helps with that. The ‚Äúround‚Äù example of 100 people gives us exactly the same percentages as the absolute numbers, however with a less ‚Äúround‚Äù number or several groups the percentages would become very useful. The median survival time of people in the cold ocean is around 70 days and the confidence intervals aren‚Äôt very wide, so that we can be pretty confident in our numbers:\n\n\nset.seed(999) # for reproducible example\nd <- ggstatsplot::Titanic_full %>% \n  mutate(survived = ifelse(Survived == \"No\", 1, 0),\n         time     = runif(n=2201, min=1, max=100)) %>% \n  sample_n(100) \n\nm <- survfit(Surv(time, survived) ~ 1, data = d)\n\nggsurvplot(m, \n           conf.int = TRUE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\")\n\n\n\n\nBut can we have two survival curves on the same plot and compare them somehow? Of coarse! In fact, that is the moment where the fun starts. For instance, we can compare survival probabilities of males vs.¬†females. For this we only need to replace ~1 in the model formula with the name of a categorical variable of interest, e.g.¬†sex. Survival plot then displays a Kaplan-Meier curve for every category of your variable:\n\n\nm <- survfit(Surv(time, survived) ~ Sex, data = d)\n\nggsurvplot(m, \n           pval = TRUE, \n           conf.int = TRUE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\") \n\n\n\n\nInterpretation of groups comparison using 4 benchmarks\nthe visual comparison of curves: just by looking at two groups we can say whether there is a difference. For instance, our plot reveals that females have higher probability of survival almost thought the whole time period (x-axes). However, whether this difference is statistically significant requires a formal statistical test. We could go further and look at the numbers of the curve, for instance: at the sunny and beautiful day before the crush, the survival probability of both groups is 1.0 (or 100% of the passengers are alive). At day 50, the probability of survival of females is ca. 0.75 (or 75%) and only ca. 0.65 (or 65%) for males. At day 75, the survival is ca. 60 and ca. 30% accordingly.\ncomparison of confidence intervals (CIs): overlapping CIs show that survival of males and females is not too different and could be due to chance all the way to ca. 80 days. After 80 days CIs stop overlapping which suggests significant difference in survival at a particular time point, say 90 days.\nestimated median survival times reveals more then confidence intervals. The median survival time for each group represents the time at which the survival probability is 50%. For instance the median survival of Females is much higher (87 days) then males (65 days). Such a huge difference of 23 days sounds significant to me, because females have 23 more days to be found by some fishing boat. Here again, only test can tell and that is why the last benchmark for comparing two groups is the p-value estimated by a Log-Rank test (Peto et al, 1977), which is so important, that it deserves an extra chapter.\n\n\nm\n\n\nCall: survfit(formula = Surv(time, survived) ~ Sex, data = d)\n\n            n events median 0.95LCL 0.95UCL\nSex=Female 31     10   87.0    60.9      NA\nSex=Male   69     54   64.5    53.6    72.9\n\nLog-Rank test\nA non-parametric Log-Rank (sometimes called Mantel-Haenszel) statistical test compares median survival times of groups. Log-Rank test is similar to a (1) non-parametric Wilcoxon-Rank test, which also compares medians using ranks and it is also similar to a (2) Chi-square test, where we compare the observed number of events to the expected ones by calculating Chi-square statistic:\n\\[ X^2 = \\sum_{i = 1}^{g} \\frac{(O_i - E_i)^2}{E_i}\\]\nThe expected numbers of events are calculated for each time point and each group as compared to the previous time point. These values are then summed over all time points to give the total expected number of events in each group.\nThe non-parametric nature of the test makes no assumptions about the survival distributions. So, is survival time normal (‚Äúbell curvy‚Äù)? It can be, but does not have to! In fact, survival data are very rarely normally distributed, but are often skewed due to a typically many early events and relatively few late ones.\nThe null hypothesis of the Log-Rank test is that there is no difference in survival between the two groups. The p-value of 0.011 allows us to reject the null hypothesis and indicates a significant median difference in survival time between females and males.\nThe Log-Rank test is soo widely used for comparing two or more survival curves, that you actually have to heavily justify the usage of any other test.\nThe function survdiff() computes Log-Rank test and returns following components:\nthe number of subjects in each group.\nthe weighted observed number of events in each group.\nthe weighted expected number of events in each group.\nthe Chi-square statistic for a test of equality\nand the p-value for the difference in survival among the groups\n\n\nsurvdiff(Surv(time, survived) ~ Sex, data = d)\n\n\nCall:\nsurvdiff(formula = Surv(time, survived) ~ Sex, data = d)\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\nSex=Female 31       10     19.3      4.45      6.46\nSex=Male   69       54     44.7      1.92      6.46\n\n Chisq= 6.5  on 1 degrees of freedom, p= 0.01 \n\nIs the Log-Rank test perfect? Unfortunately, no. One problem is: it does not provide an effect size which leaves us with only p-value as a measure of difference. Another problem with Log-Rank test (and Kaplan-Meier method in general) is that it does not allow confounders, it ignores other factors/variables. Fortunately, we still can compare more then two groups of a single variable üòâ.\n> 2 groups and multiple pairwise (post-hoc) Log-Rank test\nLog-Rank test can compare more then two groups and say whether there is a significant (p-value < 0.05) difference among these groups. The test results displayed in the table and KM curve below show that there is a significant difference in survival (p-value = 0.024) among groups of people in different ticket classes. However, like the most other tests (e.g.¬†ANOVA) it does not say between which groups exactly. That is why we need an additional analysis which pairwisely compares each group to each other group. Such analysis is often called a post-hoc.\n\n\nsurvdiff(Surv(time, survived) ~ Class, data = d)\n\n\nCall:\nsurvdiff(formula = Surv(time, survived) ~ Class, data = d)\n\n            N Observed Expected (O-E)^2/E (O-E)^2/V\nClass=1st  14        4    11.03     4.477     5.532\nClass=2nd  13        5     8.49     1.432     1.666\nClass=3rd  37       30    21.81     3.074     4.760\nClass=Crew 36       25    22.68     0.238     0.383\n\n Chisq= 9.5  on 3 degrees of freedom, p= 0.02 \n\nm <- survfit(Surv(time, survived) ~ Class, data = d)\n\nggsurvplot(m, \n           pval = TRUE, \n           conf.int = FALSE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\",\n           ncensor.plot = TRUE,\n           break.time.by = 20,\n           risk.table.y.text.col = TRUE, risk.table.y.text = FALSE) \n\n\n\n\nThree further useful plot arguments which can help to better visualize survival data are:\nbreak.time.by = 20 break the x-axis into wishful time intervals.\nrisk.table.y.text.col = TRUE and risk.table.y.text = FALSE plots bars instead of names in text annotations of the legend of risk table\nncensor.plot = TRUE displays the number of censored subjects, which helps to understand what is the cause that the risk number becomes smaller: the event or censoring.\nInterpretation\nThe median survival is ca. 90 days for the passengers in the 1st and 2nd classes, 58 days for the 3rd class and 65 days for the crew of the Titanic, suggesting a good survival of rich people, as compared to the rest. A low p-value (p = 0.024) suggests that there is a significant difference in survival among groups. Among which, only post-hoc can tell. The results of such post-hoc Log-Rank analysis can be conducted with pairwise_survdiff function form the survminer package and are displayed below. As usual, if we have multiple comparisons, we run into a risk of making a false discovery or missing an important discovery. Thus, we have to adjust the p-values in order to reduce the probability of making an error. I personally prefer the Benjamini & Hochberg (1995) adjustment method to the famous but conservative Bonferroni method:\n\n\npairwise_survdiff(\n  formula = Surv(time, survived) ~ Class, data = d, p.adjust.method = \"fdr\"\n  )\n\n\n\n    Pairwise comparisons using Log-Rank test \n\ndata:  d and Class \n\n     1st   2nd   3rd  \n2nd  0.414 -     -    \n3rd  0.028 0.139 -    \nCrew 0.095 0.317 0.414\n\nP value adjustment method: fdr \n\nThe results of the post-hoc analysis revealed that the survival of the 1st class is significantly higher as compared to the 3rd class and the Crew. And despite the fact that the median survival of the 2nd class passengers is very similar to the first, the confidence intervals of this survival are wide and do overlap with other groups a lot (not shown to avoid clutterness). That is why the 2nd class passengers do not generally have significantly higher survival time despite much higher median survival time.\nMultiple survival curves\nAs mentioned above, the Log-Rank test can not be applied to several variables. However, we still can plot survival curves of several variables in order to get some intuition for our data. Further methods, e.g.¬†Cox models, are able to estimate difference among several variables, but they are more complex than the KM-method and thus will be covered in future posts.\n\n\nm2 <- survfit( Surv(time, survived) ~ Sex + Class, data = d )\n\nggsurv <- ggsurvplot(m2, conf.int = TRUE)\n   \nggsurv$plot +theme_bw() + \n  theme (legend.position = \"right\")+\n  facet_grid(Sex ~ .)\n\n\n\n\nThe intuition we can get from the plots above is that rich women (1st and 2nd ticket classes) and (only 2) women from the crew have 100% probability of survival, while pure women in the 3rd class will die with a similar certainty as men. Heavily overlapping CIs of men survival suggests that all man will eventually die after the Titanic accident.\nConclusions\nSurvival analysis investigates the time it takes for an event of interest to occur. Most of the univariate (single variable) survival analyses uses Kaplan-Meier plots to visualize the survival curves and Log-Rank test to compare the survival curves of two or more groups.\nAdvantages:\nThe crucial advantage of survival probability curve vs.¬†logistic regression curve is accounting for censored data, which are neither dead, nor alive. Logistic regression treats all the people who didn‚Äôt die as survived, which is wrong, simply because we don‚Äôt know the survival status of a missed person. Patients leave the study due to two main reasons, they either fill so bad, that they don‚Äôt care about your experiment anymore, or they feel much better and forget your study. Some patients may simply move to the other city without saying anything. Counting all of them as survived, as logistic regression does, would overestimate the survival probability (survivorship bias) and underestimate the hazard of death. The survival analysis is therefore more precise as compared to a logistic regression, while it still catches a non-linear trend in probabilities.\nAnother advantage is the non-parametric nature of the Kaplan-Meier method, which does not have too many assumptions. In fact the only important assumption is that censoring should be non-informative. Why? More information is always better, right? Yes! But if we know why people leave the study, we could use this information as a new variable and study it‚Äôs influence on survival. The KM method would then be inappropriate, because it will miss this information. However, often we don‚Äôt know (no-info) why people leave (are censored). And in this case the KM-method squeezes the most inference out of such non-informative data.\nDisadvantages:\nThe Kaplan-Meier ‚Äúcurve‚Äù does not actually look like a curve. Oppositely to the logistic regression Kaplan-Meier method can not be described as a smooth function (curve) by a few parameters, e.g.¬†the slope or odds-ratio. That is why we need the fool table of results or a graph.\nKaplan-Meier method can‚Äôt model numeric variables, but only categorical.\nKaplan-Meier method can‚Äôt include many explanatory variables. It‚Äôs bad, because comparing groups in terms of survival may miss the effect of other factors, known as covariates or confounders, which could potentially affect the survival time of a particular group.\nRecommendations:\nalways display statistical uncertainty by including 95% CIs or/and a p-value of the Log-Rank test. Displaying CIs at a few important time points on the plot for each treatment group may sometimes be clearer then displaying them for all time points. Non-overlapping CIs indicate significant difference between groups.\nconsider cutting the x-axis. Why? Well, the eye is naturally drawn to the right part of the plot, where the time ends. However, the end of the plot contains the least amount of information and greatest uncertainty due to just a low number of remaining participants. How far in time to extend the plot? It‚Äôs up to you.\nalways display the risk table showing the numbers of patients event-free and still in follow-up in each treatment group at relevant time points.\nWhat‚Äôs next?\nSeveral methods can address the disadvantages of the Kaplan-Meier method. Particularly, exponential parametric models provide a smooth function which is able to describe the survival curve as an actual curve in a few parameters, like slope (that‚Äôs why - parametric). And Cox-Proportional-Hazard model can be extended to several variables (in progress).\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nThank you for learning!\nFurther readings, videos and references\nClark, T., Bradburn, M., Love, S., & Altman, D. (2003). Survival analysis part I: Basic concepts and first analyses. 232-238. ISSN 0007-0920.\nthe whole playlist of videos from MarinStatsLectures! They are amazing! https://www.youtube.com/watch?v=vX3l36ptrTU&list=PLqzoL9-eJTNDdnKvep_YHIwk2AMqHhuJ0\nsurvminer cheat sheet: https://rpkgs.datanovia.com/survminer/survminer_cheatsheet.pdf\nhttps://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html#part_1:_introduction_to_survival_analysis\n\nKaplan, E.L. and Meier, P. (1958) Nonparametric Estimation from Incomplete Observations. Journal of the American Statistical Association, 53, 457-481. http://dx.doi.org/10.1080/01621459.1958.10501452‚Ü©Ô∏é\n",
    "preview": "posts/2021-01-03-survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves/thumbnail_survival_1.png",
    "last_modified": "2021-01-03T23:22:22+01:00",
    "input_file": "survival-analysis-1-a-gentle-introduction-into-kaplan-meier-curves.utf8.md",
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2021-01-02-r-package-reviews-janitor-clean-your-data/",
    "title": "R package reviews {janitor} clean your data!",
    "description": "Data Scientists spend up to 80% of their time cleaning and preparing data for analysis. \" Happy families are all alike; every unhappy family is unhappy in its own way\" ‚Äî Leo Tolstoy. \"Like families, tidy datasets are all alike but every messy dataset is messy in its own way\" - Hadley Wickham. Thats when \"janitor\" helps to clean the mess.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-02",
    "categories": [
      "R package reviews",
      "videos"
    ],
    "contents": "\n\nContents\nGet dirty data and look at it\nR demo for how to clean your data\nMain functionsclean_names()\nremove_empty() & remove_constant()\nget_dupes()\nround_to_fraction()\nconvert_to_date()\nrow_to_names()\n\nGenerate ‚Äúadorable‚Äù frequency table (1-, 2-, or 3-way).\n\n\n\nlibrary(tidyverse)          # data manipulation\nlibrary(janitor)            # data cleaning\nlibrary(readxl)             # data importing\nlibrary(kableExtra)         # beautifying tables\n\n\n\nGet dirty data and look at it\nIf you are reading this post, you are probably already familiar with the concept of tidy data. If not, have a look at it. And you have most likely already worked with the messy (dirty) data. I did, and that is why I found the janitor package sooo useful!\nThe messy data displayed below can be found here. Some of the indicators of messyness are: strange (difficult) names, empty columns and rows, constant columns, which do not provide much of a value, duplicates, strange dates which do not look like dates etc..\n\n\ndirty_data <- read_excel(\"dirty_data.xlsx\")\ndirty_data %>%\n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nFirst Name\n\n\nLast Name\n\n\nEmployee Status\n\n\nSubject\n\n\nHire Date\n\n\n% Allocated\n\n\nFull time?\n\n\ndo not edit! ‚Äî>\n\n\nCertification‚Ä¶9\n\n\nCertification‚Ä¶10\n\n\nCertification‚Ä¶11\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nInstr. music\n\n\nVocal music\n\n\nNA\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nComputers\n\n\nNA\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nEnglish 6-12\n\n\nNA\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPhysical ed\n\n\nNA\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nPolitical sci.\n\n\nNA\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nNA\n\n\nVocal music\n\n\nEnglish\n\n\nNA\n\n\nR demo for how to clean your data\nA short (ca. 12 min) video below shows how to clean this data and the code you‚Äôll see in the video is provided below.\n\npreserve84584ba96750c386\n\nMain functions\nclean_names()\nThis function removes all the non-letters and signs from the names and connects several words with underscores. You can ignore the kbl() and kable_classic_2() rows in the code below, they just make the HTML table look clean, but do not really clean anything in our dataset.\n\n\nd <- dirty_data %>% \n  clean_names()\n\nd %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ndo_not_edit\n\n\ncertification_9\n\n\ncertification_10\n\n\ncertification_11\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nInstr. music\n\n\nVocal music\n\n\nNA\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nComputers\n\n\nNA\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nNA\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nEnglish 6-12\n\n\nNA\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nPENDING\n\n\nNA\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPhysical ed\n\n\nNA\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nNA\n\n\nPolitical sci.\n\n\nNA\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nNA\n\n\nVocal music\n\n\nEnglish\n\n\nNA\n\n\nremove_empty() & remove_constant()\nremove_empty() removes both empty rows and empty columns. We have two empty columns and one empty row. They are just useless. Lets add two constant columns to the dataset and see how we can remove all this junk with janitor.\n\n\n# add two constant columns\nd <- d %>% \n  mutate(constant_column   = 42,\n         constant_column_2 = \"text\")\n\n# remove the junk\nd %>% \n  remove_constant() %>%  \n  remove_empty() %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ncertification_9\n\n\ncertification_10\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nPhysical ed\n\n\nTheater\n\n\nJason\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nPhysical ed\n\n\nTheater\n\n\nAlicia\n\n\nKeys\n\n\nTeacher\n\n\nMusic\n\n\n37118\n\n\n1.00\n\n\nYes\n\n\nInstr. music\n\n\nVocal music\n\n\nAda\n\n\nLovelace\n\n\nTeacher\n\n\nNA\n\n\n27515\n\n\n1.00\n\n\nYes\n\n\nPENDING\n\n\nComputers\n\n\nDesus\n\n\nNice\n\n\nAdministration\n\n\nDean\n\n\n41431\n\n\n1.00\n\n\nYes\n\n\nPENDING\n\n\nNA\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nScience 6-12\n\n\nPhysics\n\n\nChien-Shiung\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nScience 6-12\n\n\nPhysics\n\n\nJames\n\n\nJoyce\n\n\nTeacher\n\n\nEnglish\n\n\n32994\n\n\n0.50\n\n\nNo\n\n\nNA\n\n\nEnglish 6-12\n\n\nHedy\n\n\nLamarr\n\n\nTeacher\n\n\nScience\n\n\n27919\n\n\n0.50\n\n\nNo\n\n\nPENDING\n\n\nNA\n\n\nCarlos\n\n\nBoozer\n\n\nCoach\n\n\nBasketball\n\n\n42221\n\n\nNA\n\n\nNo\n\n\nPhysical ed\n\n\nNA\n\n\nYoung\n\n\nBoozer\n\n\nCoach\n\n\nNA\n\n\n34700\n\n\nNA\n\n\nNo\n\n\nNA\n\n\nPolitical sci.\n\n\nMicheal\n\n\nLarsen\n\n\nTeacher\n\n\nEnglish\n\n\n40071\n\n\n0.80\n\n\nNo\n\n\nVocal music\n\n\nEnglish\n\n\nget_dupes()\nYou can hunt duplicates rows in several columns. The function also returns the counts for every duplicate.\n\n\nd %>% \n  get_dupes(first_name) %>% \n  kbl() %>%\n  kable_classic_2(full_width = F)\n\n\n\nfirst_name\n\n\ndupe_count\n\n\nlast_name\n\n\nemployee_status\n\n\nsubject\n\n\nhire_date\n\n\npercent_allocated\n\n\nfull_time\n\n\ndo_not_edit\n\n\ncertification_9\n\n\ncertification_10\n\n\ncertification_11\n\n\nconstant_column\n\n\nconstant_column_2\n\n\nChien-Shiung\n\n\n2\n\n\nWu\n\n\nTeacher\n\n\nPhysics\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\n42\n\n\ntext\n\n\nChien-Shiung\n\n\n2\n\n\nWu\n\n\nTeacher\n\n\nChemistry\n\n\n11037\n\n\n0.50\n\n\nYes\n\n\nNA\n\n\nScience 6-12\n\n\nPhysics\n\n\nNA\n\n\n42\n\n\ntext\n\n\nJason\n\n\n2\n\n\nBourne\n\n\nTeacher\n\n\nPE\n\n\n39690\n\n\n0.75\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\n42\n\n\ntext\n\n\nJason\n\n\n2\n\n\nBourne\n\n\nTeacher\n\n\nDrafting\n\n\n39690\n\n\n0.25\n\n\nYes\n\n\nNA\n\n\nPhysical ed\n\n\nTheater\n\n\nNA\n\n\n42\n\n\ntext\n\n\nround_to_fraction()\nThis can be very useful if you have lots of data. I do work with agricultural animals and they have all kinds of scores. One of them - Body Condition Score (BCS) is always recorded in quarters, e.g.¬†2.25, 2.5, 2.75 etc. So, if some colleagues try to be very smart and very exact, they stupidly record values like 0.8 or 3.149 instead of wanted 0.75 and 3, you need to correct this. round_to_fraction() simplifies this task enormously! Have a look at the last number below, before and after applying round_to_fraction() to a variable.\n\n\n# before\nd$percent_allocated\n\n\n [1] 0.75 0.25 1.00 1.00 1.00 0.50 0.50   NA 0.50 0.50   NA   NA 0.80\n\n# after\nround_to_fraction(d$percent_allocated, denominator = 4) # digits = 3\n\n\n [1] 0.75 0.25 1.00 1.00 1.00 0.50 0.50   NA 0.50 0.50   NA   NA 0.75\n\nconvert_to_date()\nA modern Excel always tries to automate things, and I hate it! üòÇ For instance you write a number into a cell and it sometimes immediately converts it into date. Then you try to have a date in a cell, and it returns a number. Moreover, Excel also has some strange date encoding systems, which can be confused with a normal numeric columns. Luckily, our dirty dataset has a ‚Äúdate‚Äù word in the name of a column ‚Äúhire_date‚Äù, otherwise we wouldn‚Äôt know that it is a date:\n\n\nd$hire_date\n\n\n [1] 39690 39690 37118 27515 41431 11037 11037    NA 32994 27919 42221\n[12] 34700 40071\n\nconvert_to_date(d$hire_date)\n\n\n [1] \"2008-08-30\" \"2008-08-30\" \"2001-08-15\" \"1975-05-01\" \"2013-06-06\"\n [6] \"1930-03-20\" \"1930-03-20\" NA           \"1990-05-01\" \"1976-06-08\"\n[11] \"2015-08-05\" \"1995-01-01\" \"2009-09-15\"\n\nrow_to_names()\nPeople often have several header columns, in order to beautifully explain everything, make things accurate and don‚Äôt miss any important information. I‚Äôve been there too. But as soon as I started to work with software, I realized that this ‚Äúbeauty‚Äù hurts. I get a lot of Excel datasets like that, which is not a problem, I just delete not needed rows and continue working. But then I get the same table with a few corrected values, even if I tell colleagues to have only one header. That is why I was pleased to discover row_to_names() function. Have a look at the dataset ‚Äúx‚Äù below and how easy can we handle it.\n\n\nx <- data.frame(\n  X_1 = c(\"some general description\", \"Real title\", 1:3),\n  X_2 = c(\"something `very!!! important` :) \", \"Which we wont!\", 4:6))\n\nx\n\n\n                       X_1                               X_2\n1 some general description something `very!!! important` :) \n2               Real title                    Which we wont!\n3                        1                                 4\n4                        2                                 5\n5                        3                                 6\n\nx %>%\n  row_to_names(row_number = 2)\n\n\n  Real title Which we wont!\n3          1              4\n4          2              5\n5          3              6\n\nGenerate ‚Äúadorable‚Äù frequency table (1-, 2-, or 3-way).\nThe table() function is actually cool, but as I discovered tabyl() in the janitor packages, I couldn‚Äôt go back. First of all, I always need to explicitly write , useNA = ‚Äúifany‚Äù if I wanna see whether there NAs. Secondly the proportions have to be called with an extra function prop.table() on top of the table() function. Now, look at what tabyl() does:\n\n\n# old way\ntable(d$employee_status, useNA = \"ifany\")\n\n\n\nAdministration          Coach        Teacher           <NA> \n             1              2              9              1 \n\nprop.table(table(d$employee_status))\n\n\n\nAdministration          Coach        Teacher \n    0.08333333     0.16666667     0.75000000 \n\n# new way\ntabyl(d$employee_status) # \"show_na\" is TRUE by default\n\n\n d$employee_status n    percent valid_percent\n    Administration 1 0.07692308    0.08333333\n             Coach 2 0.15384615    0.16666667\n           Teacher 9 0.69230769    0.75000000\n              <NA> 1 0.07692308            NA\n\n# new way with two variables\nd %>% \n  tabyl(employee_status, full_time)\n\n\n employee_status No Yes NA_\n  Administration  0   1   0\n           Coach  2   0   0\n         Teacher  3   6   0\n            <NA>  0   0   1\n\nMoreover, along the counts janitor‚Äôs tabyl can also display totals, formatted percentages and even all of them together by using a family of ‚Äúadorable‚Äù functions.\n\n\n#  \nd %>%\n  tabyl(employee_status, full_time) %>%\n  adorn_totals(c(\"col\", \"row\"))\n\n\n employee_status No Yes NA_ Total\n  Administration  0   1   0     1\n           Coach  2   0   0     2\n         Teacher  3   6   0     9\n            <NA>  0   0   1     1\n           Total  5   7   1    13\n\nd %>%\n  tabyl(employee_status, full_time) %>%\n  adorn_totals(\"row\") %>%\n  adorn_percentages(\"row\") %>%\n  adorn_pct_formatting() %>%\n  adorn_ns(position = \"front\") %>%               \n  adorn_title() %>% \n  adorn_title(\"combined\")\n\n\n employee_status/full_time  full_time                      \n           employee_status         No        Yes        NA_\n            Administration 0   (0.0%) 1 (100.0%) 0   (0.0%)\n                     Coach 2 (100.0%) 0   (0.0%) 0   (0.0%)\n                   Teacher 3  (33.3%) 6  (66.7%) 0   (0.0%)\n                      <NA> 0   (0.0%) 0   (0.0%) 1 (100.0%)\n                     Total 5  (38.5%) 7  (53.8%) 1   (7.7%)\n\n\n\nmtcars %>%\n  tabyl(am, cyl) %>%\n  adorn_totals(c(\"col\", \"row\")) %>% \n  adorn_percentages(\"all\") %>% \n  adorn_pct_formatting() %>%\n  adorn_ns() \n\n\n    am          4         6          8       Total\n     0  9.4%  (3) 12.5% (4) 37.5% (12)  59.4% (19)\n     1 25.0%  (8)  9.4% (3)  6.2%  (2)  40.6% (13)\n Total 34.4% (11) 21.9% (7) 43.8% (14) 100.0% (32)\n\nYou could also compare columns of two dataframes to see the difference. To see more function, type ?janitor in the console of RStudio, scroll down and press index.\n\n\nd1 <- d %>% \n  mutate(new_column = 42, \n         second_new = \"into it\")\n\ncompare_df_cols(d, d1) %>% View()\n\n\n\nEvery single function from janitor package makes your life easier and more productive for a moment, some of them a lot easier, e.g.¬†clean_names() and remove_empty(). But the real power of it accumulates over time, because you free your mind and time for creative work, instead of solving problems. Thus, thousand thanks to package-developer Sam Firke!\n\n\ncitation(\"janitor\")\n\n\n\nTo cite package 'janitor' in publications use:\n\n  Sam Firke (2020). janitor: Simple Tools for Examining and\n  Cleaning Dirty Data. R package version 2.0.1.\n  https://CRAN.R-project.org/package=janitor\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {janitor: Simple Tools for Examining and Cleaning Dirty Data},\n    author = {Sam Firke},\n    year = {2020},\n    note = {R package version 2.0.1},\n    url = {https://CRAN.R-project.org/package=janitor},\n  }\n\nWhat is you favorite R package?\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\n\n\n\n",
    "preview": "posts/2021-01-02-r-package-reviews-janitor-clean-your-data/thumbnail_janitor.jpg",
    "last_modified": "2021-01-03T09:51:03+01:00",
    "input_file": "r-package-reviews-janitor-clean-your-data.utf8.md"
  },
  {
    "path": "posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/",
    "title": "How to visualize models, their assumptions and post-hocs",
    "description": "A picture is worth a thousand words! This article shows how to visualize results of 16 different models in R: from a simple linear model to a multiple-additive-non-linear-mixed-effects model. Among them are logistic, multinomial, additive and survival models with and without interactions. **Goal: minimum R code & maximum output!** We'll also go a bit beyond only model visualization. So, don't miss the bonuses üòâ.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2021-01-01",
    "categories": [
      "visualization",
      "videos",
      "models"
    ],
    "contents": "\n\nContents\nA demo for the first 7 linear models1. Simple linear model with one categorical predictor\n2. Simple linear model with one numeric predictor\n3. Multiple linear model with several categorical predictors\n4. Multiple linear model with several numeric predictors\n5. Multiple linear model with numeric and categorical predictors\n\nBonus 1: check all the assumption in one line of code6. Multiple linear model with interactions\n\nBonus 2: easy post-hocs7. Multiple linear model with interactions bwtween numeric predictors\n\nBonus 3: quick multiple models with ggplot\nA demo for the next 9 models: non-linear, logistic, multinomial, mixed-effects, survival‚Ä¶8. Multiple non-linear polynomial model with interactions\n9. Multiple non-linear Generalized Additive Models (GAM)\n10. Multiple logistic regression with interactions\n\nBonus 4: visualize the post-hoc analysis with the PairWise P-value Plot (pwpp)11. Multinomial logistic regression models via neural networks\n12. Multiple Linear Mixed-Effects-Model with interactions\n\nBonus 5: how to choose the best model13. GAMMs - Multiple Generalised Additive (non-linear) Mixed Effects Models\n14. Kaplan-Meier survival model\n15. Exponential Parametric Models\n16. Cox proportional hazard models\n\n\nA demo for the first 7 linear models\nTwo YouTube videos in this article demonstrate the code presented below and explain all you need to know. Thus, I reduced the text to a minimum in order to decrease redundancy. If you still have a question, drop me a comment on YouTube or on this article below. I‚Äôll try to respond as quick as I can.\n\n\n\n\n\n\n\nLet‚Äôs get our first data. We‚Äôll use several dataset through out this article.\n\n\n# install.packages(\"tidyverse\") \n# install.packages(\"ISLR\") \nlibrary(tidyverse) # data wrangling\nlibrary(ISLR)      # get \"Wage\" dataset\n\n# reproducibility: the same seed grows the same tree\nset.seed(1)        \n\nd <- Wage %>%\n  sample_n(1000) %>% \n  rename(salary = wage) \n\n# have a look at the data\nglimpse(d)\n\n\nRows: 1,000\nColumns: 11\n$ year       <int> 2004, 2003, 2007, 2006, 2004, 2006, 2004, 2003, ‚Ä¶\n$ age        <int> 59, 45, 21, 49, 59, 41, 63, 42, 46, 42, 50, 26, ‚Ä¶\n$ maritl     <fct> 2. Married, 2. Married, 1. Never Married, 2. Mar‚Ä¶\n$ race       <fct> 1. White, 1. White, 1. White, 1. White, 1. White‚Ä¶\n$ education  <fct> 5. Advanced Degree, 4. College Grad, 2. HS Grad,‚Ä¶\n$ region     <fct> 2. Middle Atlantic, 2. Middle Atlantic, 2. Middl‚Ä¶\n$ jobclass   <fct> 2. Information, 2. Information, 2. Information, ‚Ä¶\n$ health     <fct> 2. >=Very Good, 2. >=Very Good, 1. <=Good, 2. >=‚Ä¶\n$ health_ins <fct> 1. Yes, 1. Yes, 2. No, 1. Yes, 1. Yes, 1. Yes, 1‚Ä¶\n$ logwage    <dbl> 5.204120, 4.602060, 4.565848, 5.107210, 4.490520‚Ä¶\n$ salary     <dbl> 182.02062, 99.68946, 96.14407, 165.20877, 89.167‚Ä¶\n\n1. Simple linear model with one categorical predictor\nThe effects() package is the main visualization package we‚Äôll use. It can visualize results of almost any model. We‚Äôll start with the visualization of predicted values.\n\n\nm <- lm(salary ~ jobclass, d)\n\n# install.packages(\"effects\")\nlibrary(effects)    # for model visualization & more\n\nplot(allEffects(m))\n\n\n\n\n2. Simple linear model with one numeric predictor\nHere we‚Äôll add grid = TRUE for a better readability, so that we don‚Äôt need to stare on the y-axis and guess the result.\n\n\nm <- lm(salary ~ age, d)\n\nplot(allEffects(m), grid = TRUE)\n\n\n\n\n3. Multiple linear model with several categorical predictors\nWe can visualize all predictors at once, or any particular predictor from a multiple model individually.\n\n\nm <- lm(salary ~ jobclass + education, d)\n\nplot(allEffects(m))\n\n\n\nplot(predictorEffect(predictor = \"education\", mod = m))\n\n\n\n\n4. Multiple linear model with several numeric predictors\nHere we‚Äôll see how to change the appearance of confidence intervals and introduce another amazing model-visualization package - sjPlot. In this chapter we‚Äôll use the effects package for the visualization of the predicted values and sjPlot package for the visualization of the estimates with their confidence intervals.\n\n\nm <- lm(salary ~ age + year, d)\n\nplot(allEffects(m))\n\n\n\nplot(allEffects(m), confint=list(style=\"bars\"))\n\n\n\n# install.packages(\"sjPlot\")\nlibrary(sjPlot)    # for model visualization\nplot_model(m)\n\n\n\n\n5. Multiple linear model with numeric and categorical predictors\nWe can change design of the plot by determining the number of rows and columns in the plot(allEffects()). The plot_model() function can also display the numeric values of the estimates and the significance stars, which is often all we need. Besides, it looks much better than a table-looking output of the model results.\n\n\nm <- lm(salary ~ age + education, d)\n\nplot(allEffects(m))\n\n\n\n\n\n\nplot(allEffects(m), rows = 2, cols = 1)\n\n\n\n\n\n\nplot_model(m, show.values = TRUE)\n\n\n\n\nBonus 1: check all the assumption in one line of code\ncheck_model is just awesome! One of my favorite R functions! I get a ‚Äúnerdgasm‚Äù every time I use it üòÇ. The video explains it very well.\n\n\n# install.packages(\"performance\")\nlibrary(performance)    # model assumptions & performance\ncheck_model(m)\n\n\n\n\n6. Multiple linear model with interactions\nFirst of all, the allEffects() functions visualizes interactions easily! Secondly, we can put several lines on the same plot with or without confidence intervals by using argument multiline = TRUE. ‚ÄúT‚Äù instead of ‚ÄúTRUE‚Äù also works. The sjPlot package is not only able to also easily visualize interactions, but can in addition be extended with the usual ggplot2 syntax, which can greatly improve the appearance of the plot.\n\n\nm <- lm(salary ~ education * jobclass, d)\n\n# not too neat representation!\nplot(allEffects(m))\n\n\n\n# better representation\nplot(allEffects(m), lines = list(multiline = TRUE))\n\n\n\n\n\n\n# perfect representation\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, type = \"int\")+theme_blank()+theme(legend.position = \"top\")\n\n\n\n\nBonus 2: easy post-hocs\nThe emmeans package is one of my favorite for conducting post-hocs. In this chapter we only display the results in the text/table form. Later we‚Äôll also visualize post-hocs.\n\n\n# install.packages(\"emmeans\")\nlibrary(emmeans)      # for post-hocs\nemmeans(m, pairwise ~ jobclass | education, adjust = \"fdr\")$contrasts\n\n\neducation = 1. < HS Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information   0.7937 8.50 990  0.093  0.9257 \n\neducation = 2. HS Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information   0.0431 4.49 990  0.010  0.9923 \n\neducation = 3. Some College:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information  -8.7985 5.33 990 -1.651  0.0990 \n\neducation = 4. College Grad:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information  -7.8854 4.94 990 -1.597  0.1105 \n\neducation = 5. Advanced Degree:\n contrast                       estimate   SE  df t.ratio p.value\n 1. Industrial - 2. Information -21.2809 8.24 990 -2.581  0.0100 \n\n7. Multiple linear model with interactions bwtween numeric predictors\n\n\nm <- lm(salary ~ age * health, d)\n\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, type = \"int\")+\n  theme_minimal()+\n  theme(legend.position = \"top\")\n\n\n\n\n\n\ncheck_model(m)\n\n\n\n\nBonus 3: quick multiple models with ggplot\nWe do not always need to explicitly model things in order to explore our data. A geom_smooth() function from ggplot2 package will automatically fit numeric data. Mostly with non-linear models (e.g.¬†GAM), but the argument method=‚Äúlm‚Äù can force it to take the linear one. Moreover, we could also use any formula inside of the geom_smooth(), but if we need to write the formula anyway, we‚Äôd rather produce an explicit model. However, the code presented below is quick and easy, and therefore very practical! On this note we‚Äôll enter the world of non-linear models and I recommend to watch the second video first, before checking out the code.\n\n\nggplot(d, aes(age, salary))+\n  geom_point()+\n  geom_smooth()+    # the quickest way to model numeric data\n  facet_grid(education~jobclass, scales = \"free\") # quick multiple model \n\n\n\n\nA demo for the next 9 models: non-linear, logistic, multinomial, mixed-effects, survival‚Ä¶\n\n\n\n\n\n\n\n8. Multiple non-linear polynomial model with interactions\nThe allEffects function can also easily handle polynomial non-linear models. In contrast, the plot_model function can‚Äôt. However, we still can plot the estimates.\n\n\nm <- lm(log(salary) ~ poly(age, 2) * health, d)\n\nplot(\n  allEffects(m), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m, show.values = T)+\n  theme_bw()\n\n\n\n\n\n\ncheck_model(m)\n\n\n\n\n9. Multiple non-linear Generalized Additive Models (GAM)\nThere are two main GAM packages: gam and mgcv. They both have an identical function which we need - gam(), which produces a conflict and R might be confused about it. Thus, use gam::gam() to specify from which package exactly the gam() function should be used. The gam package has it‚Äôs own plotting function - plot.Gam and you can display all subplots on one plot by using par(mfrow = c(2, 2)). Please, don‚Äôt use ‚Äúplot(allEffects(gam1))‚Äù for GAM models, since it will produce only linear results.\ns() function indicates that we would like to use smoothing splines.\n\n\n# install.packages(\"gam\")\nlibrary(gam)\n\ngam1 <- gam::gam(salary~s(year, df = 4)+s(age, df = 5)+education + jobclass, data=d)\n\npar(mfrow = c(2, 2) )\nplot.Gam(gam1 , se=TRUE, col= \"blue\")\n\n\n\n\n10. Multiple logistic regression with interactions\nInteractions in logistic regression are not a problem for both packages. Moreover, have a look at three different displays of post-hocs from the same model and find code differences. Hint: I love the ‚Äú|‚Äù part of it!\n\n\nm <- glm(health ~ jobclass * health_ins, d, family = binomial)\n\nplot(allEffects(m))\n\n\n\nplot_model(m, type = \"int\")\n\n\n\nemmeans(m, pairwise ~ jobclass | health_ins, adjust = \"fdr\")$contrasts\n\n\nhealth_ins = 1. Yes:\n contrast                       estimate    SE  df z.ratio p.value\n 1. Industrial - 2. Information  -0.0553 0.177 Inf -0.312  0.7552 \n\nhealth_ins = 2. No:\n contrast                       estimate    SE  df z.ratio p.value\n 1. Industrial - 2. Information  -0.1830 0.241 Inf -0.760  0.4472 \n\nResults are given on the log odds ratio (not the response) scale. \n\nemmeans(m, pairwise ~ health_ins | jobclass, adjust = \"fdr\")$contrasts\n\n\njobclass = 1. Industrial:\n contrast       estimate    SE  df z.ratio p.value\n 1. Yes - 2. No    0.623 0.200 Inf 3.117   0.0018 \n\njobclass = 2. Information:\n contrast       estimate    SE  df z.ratio p.value\n 1. Yes - 2. No    0.495 0.222 Inf 2.228   0.0259 \n\nResults are given on the log odds ratio (not the response) scale. \n\n\n\nemmeans(m, pairwise ~ health_ins * jobclass, adjust = \"fdr\")$contrasts\n\n\n contrast                                     estimate    SE  df\n 1. Yes 1. Industrial - 2. No 1. Industrial     0.6232 0.200 Inf\n 1. Yes 1. Industrial - 1. Yes 2. Information  -0.0553 0.177 Inf\n 1. Yes 1. Industrial - 2. No 2. Information    0.4402 0.226 Inf\n 2. No 1. Industrial - 1. Yes 2. Information   -0.6785 0.196 Inf\n 2. No 1. Industrial - 2. No 2. Information    -0.1830 0.241 Inf\n 1. Yes 2. Information - 2. No 2. Information   0.4954 0.222 Inf\n z.ratio p.value\n  3.117  0.0055 \n -0.312  0.7552 \n  1.950  0.0768 \n -3.460  0.0032 \n -0.760  0.5367 \n  2.228  0.0518 \n\nResults are given on the log odds ratio (not the response) scale. \nP value adjustment: fdr method for 6 tests \n\nBonus 4: visualize the post-hoc analysis with the PairWise P-value Plot (pwpp)\nAs promised, here is the visualization of the post-hocs. Notice the marginal means on the y axis and color-coded design of the plot, which connects the pairs the p-values for which were calculated and adjusted (x-axis).\n\n\npwpp(emmeans(m, ~ health_ins * jobclass), type = \"response\", adjust = \"fdr\"\n     )+theme_minimal()\n\n\n\n\n11. Multinomial logistic regression models via neural networks\nNo special treatment of the fancy neural networks needed. plot(allEffects()) just works!\n\n\n# get the data\nd <- foreign::read.dta(\"https://stats.idre.ucla.edu/stat/data/hsbdemo.dta\")\n\nm <- nnet::multinom(prog ~ ses + write, d)\n\n\n# weights:  15 (8 variable)\ninitial  value 219.722458 \niter  10 value 179.985215\nfinal  value 179.981726 \nconverged\n\nplot(allEffects(m), \n     lines = list(multiline = T), \n     confint = list(style = \"auto\"), rows = 2, cols = 1)\n\n\n\n\n12. Multiple Linear Mixed-Effects-Model with interactions\nThe mixed-effects models are very complex. But fortunately, all 4 main functions, namely plot(allEffects()), plot_model(), emmeans() and check_model() work flawlessly and simply deliver! The check_model() even checks the assumptions for the random effects! The random effects themselves can also be visualized, but they are rarely interpreted, so, why bother?\n\n\n# install.packages(\"lme4\")\n# install.packages(\"lmerTest\")\n\nlibrary(lme4)\nlibrary(lmerTest)\n\n# get the data\nset.seed(9)\nd <- InstEval %>% \n  group_by(service, studage) %>% \n  sample_n(100) %>% \n  mutate(dept = as.numeric(dept))\n\nm1 <- lmer(y ~ service * studage + (1|s) + (1|d), data=d)\n\nplot(\n  allEffects(m1), \n  lines = list(multiline = T), \n  confint = list(style = \"auto\"))\n\n\n\nplot_model(m1, type = \"int\")+\n  theme_blank()+\n  theme(legend.position = \"top\")\n\n\n\n# post-hocs\nemmeans(m1, pairwise ~ service | studage, adjust = \"none\")$contrasts \n\n\nstudage = 2:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.447 0.186 787 2.404   0.0164 \n\nstudage = 4:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.183 0.186 783 0.983   0.3261 \n\nstudage = 6:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.120 0.184 785 0.650   0.5156 \n\nstudage = 8:\n contrast estimate    SE  df t.ratio p.value\n 0 - 1       0.344 0.185 782 1.858   0.0635 \n\nDegrees-of-freedom method: kenward-roger \n\npwpp(emmeans(m1, ~ service * studage), type = \"response\", adjust = \"none\")+\n  theme_minimal()\n\n\n\n\n\n\n# check model assumptions\ncheck_model(m1)\n\n\n\n\nBonus 5: how to choose the best model\nThe performance package, near the already insanely useful check_model() function, provides a compare_performance() function, which compares models with multiple quality indicators, e.g.¬†\\(R^2\\) or AIC. It is not only more informative as compared to the anova() function, which is often used for model comparison, but also works much better, because it displays a warning when two models shouldn‚Äôt be compared, while anova(m, m1) simply fails, when models aren‚Äôt supposed to be compared, but can be tricked by placing the mixed effects model (m1) first.\n\n\nm  <- lm(y ~ service * studage, data=d)\nm1 <- lmer(y ~ service * studage + (1|s) + (1|d), data=d)\n\ncompare_performance(m, m1)\n\n\nRandom effect variances not available. Returned R2 does not account for random effects.\n# Comparison of Model Performance Indices\n\nModel |            Type |     AIC |     BIC |      BF | RMSE | Sigma |   R2 | R2 (adj.) | R2 (cond.) | R2 (marg.)\n-----------------------------------------------------------------------------------------------------------------\nm     |              lm | 2702.04 | 2744.21 |    1.00 | 1.30 |  1.30 | 0.02 |  6.68e-03 |            |           \nm1    | lmerModLmerTest | 2709.21 | 2760.74 | < 0.001 | 1.09 |  1.18 |      |           |            |       0.02\n\nanova(m1, m)\n\n\nData: d\nModels:\nm: y ~ service * studage\nm1: y ~ service * studage + (1 | s) + (1 | d)\n   npar    AIC    BIC  logLik deviance  Chisq Df Pr(>Chisq)    \nm     9 2702.0 2744.2 -1342.0   2684.0                         \nm1   11 2688.7 2740.2 -1333.3   2666.7 17.389  2  0.0001675 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n13. GAMMs - Multiple Generalised Additive (non-linear) Mixed Effects Models\nThese kind of models are just madness, can be easily visualized though.\n\n\n# install.packages(\"mgcViz\")\nlibrary(mgcViz)\n\nm <- gammV(y ~ s(as.numeric(d), k = 3) + lectage, random=list(s=~1), data= InstEval %>% \n             slice(1:5000))\n\nplot(m, allTerms = T)\n\n\nHit <Return> to see next plot:\n\nHit <Return> to see next plot:\n\n\n14. Kaplan-Meier survival model\nSurvival models can be visualized with survminer package. Even some statistical details can be displayed. I still did not figured out how to visualize the post-hocs for survival analysis. It you know how, please let me know. Thus, I here simply provide the p-values of the post-hocs.\n\n\n# install.packages(\"survival\")\n# install.packages(\"survminer\")\nlibrary(survival)\nlibrary(survminer)\n\nset.seed(1)\nd <- lung %>% \n  filter(ph.ecog != 3) %>% \n  sample_n(100)\n\nm <- survfit(Surv(time, status) ~ ph.ecog, data = d)\n\n# simple plot\nggsurvplot(m)\n\n\n\n\n\n\n# fancy plot\nggsurvplot(m, \n           pval = TRUE, \n           risk.table = \"abs_pct\", \n           surv.median.line = \"hv\")\n\n\n\n\n\n\n# post-hocs for survival analysis\npairwise_survdiff(\n  formula = Surv(time, status) ~ ph.ecog, data = d, p.adjust.method = \"fdr\"\n)\n\n\n\n    Pairwise comparisons using Log-Rank test \n\ndata:  d and ph.ecog \n\n  0       1      \n1 0.14954 -      \n2 0.00042 0.01507\n\nP value adjustment method: fdr \n\n15. Exponential Parametric Models\nThese models are very rarely used.\n\n\n# install.packages(\"flexsurv\")\nlibrary(flexsurv)    # for Parametric Survival Modelling\n\nex <- flexsurvreg(Surv(time, status) ~ factor(ph.ecog), data = d, dist=\"exponential\")\n\nggsurvplot(ex)\n\n\n\n\n16. Cox proportional hazard models\nCox models are more common as compared to the exponential models and can be visualized with a beautiful ggforest() plot.\n\n\nm <- coxph(Surv(time, status) ~ age + sex + ph.ecog, data =  d)\n\nggforest(m, d)\n\n\n\n\nI hope you found this article useful. The are of coarse more interesting models out there. Thus, please let me know what kind of models you make and how you visualize them.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\n\n\n\n",
    "preview": "posts/2021-01-01-how-to-visualize-models-their-assumptions-and-post-hocs/thumbnail_visualize_models.png",
    "last_modified": "2021-01-02T10:27:18+01:00",
    "input_file": {},
    "preview_width": 1920,
    "preview_height": 1080
  },
  {
    "path": "posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/",
    "title": "How to create a blog or a website in R with {Distill} package",
    "description": "If you're not online, you don't exist. A personal webpage or a blog became the business card of the digital century. It shows who you are and what you are capable of. Thus: show, don't tell.",
    "author": [
      {
        "name": "Yury Zablotski",
        "url": "https://yury-zablotski.netlify.app/"
      }
    ],
    "date": "2020-12-26",
    "categories": [
      "R & the Web",
      "videos"
    ],
    "contents": "\n\nContents\nWhy should we use Distill?\nManual intermittent publishing (deploying)1. Install Distill package\n2. Create your blog either manually with RStudio or with one line of code2.1. with RStudio\n2.2. with one line of code\n\n3. Build your website\n4. Publish your blog (or a website) via Netlify\n5. Create a new blog-post\n\nAutomated continuous publishing (deploying)1. Install distill and usethis packages\n2. Create your blog either manually with RStudio or with one line of code2.1. with RStudio\n2.2. with one line of code\n\n3. Build your website\n4. Connect to your github4.1. Create new repository\n4.2. Stage and Commit\n4.3. Run ‚Äúusethis::use_github()‚Äù to connect a local repository to Github\n\n5. Publish your blog (or a website) via Netlify\n6. Create a new blog-post\n\nBlog configuration with \"_site.yml\"\nTheming - change desing and appearence of your blog\nPimp your contentLinks\nPlots\nFigures\nTables\nAsides\nEquations\nFootnotes\nThumbnails\nCitations\n\nUseful ressources\n\nWhy should we use Distill?\nBecause it‚Äôs an easy, quick and free way to go online today. All you need to create a website or a blog is R, RStudio and Netlify account. Moreover, it requires very little programming. It might not look too fancy in the beginning (though totally enough for me), but with some effort you‚Äôll be able to re-design your website as you wish. Besides, Distill was originally aimed to make scientific online-publishing easier, so, it can‚Äôt be too bad ;). By the way, The blog you reading right now was created with Distill. How? That‚Äôs what this article is all about.\nIf you don‚Äôt have or don‚Äôt want to have a Github account (and continuous deployment), just read on. But if you are comfortable with Github and wanna continuously deploy via Github, jump to the chapter ‚ÄúAutomated continuous publishing (deploying)‚Äù.\nManual intermittent publishing (deploying)\nWhy should we even consider manual instead of automated? Well, for some non-programmers going with the continuous deployment via Github immediately might be very challenging and frustrating. I‚Äôve been there. We all know what is ‚Äúbetter‚Äù and ‚Äúmore time effective‚Äù in the long run, but the complexity might sometimes turn us off from doing something we would love to, resulting into ‚Äúit‚Äôs just not my cup of tea‚Äù attitude. Other do not even need the continuous deployment, because they publish rarely, e.g.¬†once per month, but would still like to be online. Going to the WordPress and co., which would most likely screw either code or code-output, is not always a solution. Fortunately, Distill and Netlify greatly reduce the complexity of going online. Thus, have a look at the quick (<5 minutes) video and then follow the step by step procedure described below and you‚Äôll be online with your blog in minutes.\n\npreservec4825fea18b25926\n\n1. Install Distill package\n\n\ninstall.packages(\"distill\")\n\n\n\n2. Create your blog either manually with RStudio or with one line of code\n2.1. with RStudio\nOpen RStudio\nGo to File => New Project‚Ä¶ => New Directory => Distill Blog\n‚ÄúDirectory name‚Äù will be the name of the folder, where all the documents from your blog will be stored\nChoose the directory where this folder supposed to be stored\nGive your blog a name. You‚Äôll be able to rename it later\nClick ‚ÄúCreate project‚Äù\n2.2. with one line of code\n\n\ndistill::create_blog(dir = \"amazing blog\", title = \"A cool name of my blog\")\n# or\ndistill::create_website(dir = \"amazing website\", title = \"A cool name of my Website\")\n\n\n\n3. Build your website\nA new RStudio window with newly created files and folders (right bottom corner) will open itself. One of the folders \"_posts\" will contain all your posts. There are already some examples of it. But don‚Äôt bother about it now.\nFind the ‚ÄúBuild‚Äù tab and press ‚ÄúBuild Website‚Äù. You‚Äôll see the process of building.\nClick ‚ÄúOpen in Browser‚Äù and explore your website\nWe aren‚Äôt online yet. But very soon!\n4. Publish your blog (or a website) via Netlify\ngo the https://www.netlify.com/, sing up for Netlify (I used the Email way) and confirm your Email.\nIn Netlify you‚Äôll see a window with: ‚ÄúWant to deploy a new site without connecting to Git? Drag and drop your site folder here‚Äù. If you somehow don‚Äôt see it, find and press ‚ÄúTeam overview‚Äù.\ngo to the directory of your blog and find the \"_site\" folder\ndrag and drop the \"_site\" folder from your computed into this window\nwait a moment till the ‚ÄúProduction‚Äù tab produces green colored ‚ÄúPublished‚Äù and you‚Äôll get a funny named website in the left top corner, also green and starting with ‚Äúhttps://‚Äù. My was ‚Äúhttps://condescending-darwin-bc567f.netlify.app‚Äù :)\nclick on it\nCongrats, you are online!\ngo back to Netlify, click ‚ÄúSite settings‚Äù => ‚ÄúChange site name‚Äù\nrename your site (e.g.¬†better-name) and hit ‚ÄúSave‚Äù\nClick on the ‚Äúbetter-name.netlify.app‚Äù to make sure the name has changed and the site is still working\ngo back to RStudio to populate your blog\n5. Create a new blog-post\nrun the line below with the name of your choice\n\n\ndistill::create_post(\"My second blog post...this one will be really good :-)\")\n\n\n\na new partly pre-filled Rmarkdown document will open itself in RStudio\nfill it with some text and code as you usually do with Rmarkdown documents\nhit ‚ÄúKnit‚Äù. NOTE: you‚Äôll need to always ‚ÄúKnit‚Äù all changed or created blog-posts individually. It is the only way to update them. ‚ÄúBuild Website‚Äù would not re-render them for you, because it‚Äôs computationally expensive and prone to problems. However, the Distill-Website (we are doing Distill-Blog now) would. You can learn more about the difference between them here.\nThis new post exists only on your local computer, still not online, thus‚Ä¶\ngo back to Netlify and click ‚ÄúDeploys‚Äù tab, where you‚Äôll see another window with: ‚ÄúNeed to update your site? Drag and drop your site folder here‚Äù\ndrag and drop the \"_site\" folder there and wait till ‚ÄúProduction‚Äù tab produces green ‚ÄúPublished‚Äù\nclick on the ‚Äúbetter-name.netlify.app‚Äù to make sure the new blog-post appeared\nclick on your post and enjoy your creative work for a moment ;), then\ngo back to RStudio and repeat step 5\nAutomated continuous publishing (deploying)\nContinuous deployment is cool! But the path there can be a little prickly. This path may take a couple of hours or days (in my case üôà). But, once there, you quickly forget all the troubles and using Github with continuous deployment becomes your second nature. So, I think there are many people which deploy either still manually or already continuously. But not many of them in the middle (I might be wrong though). Thus, I will assume you already have installed Git on your computer, created a Github account and connected your RStudio to your Github. If not, but you wanna be there, I could recommend a single short free online book which helped me go through it: Happy Git and GitHub for the useR. You‚Äôll only need it once! If you ready to proceed, have a look at the quick (ca. 8 minutes) video (will be online soon!) and then follow the step by step procedure described below and you‚Äôll be online with your blog in minutes.\n1. Install distill and usethis packages\n\n\ninstall.packages(\"distill\")\ninstall.packages(\"usethis\")\n\n\n\n2. Create your blog either manually with RStudio or with one line of code\n2.1. with RStudio\nOpen RStudio\nGo to File => New Project‚Ä¶ => New Directory => Distill Blog\n‚ÄúDirectory name‚Äù will be the name of the folder, where all the documents from your blog will be stored\nChoose the directory where this folder supposed to be stored\nGive your blog a name. You‚Äôll be able to rename it later\nClick ‚ÄúCreate project‚Äù\n2.2. with one line of code\n\n\ndistill::create_blog(dir = \"amazing blog\", title = \"A cool name of my blog\")\n# or\ndistill::create_website(dir = \"amazing website\", title = \"A cool name of my Website\")\n\n\n\n3. Build your website\nA new RStudio window with newly created files and folders (right bottom corner) will open itself. One of the folders \"_posts\" will contain all your posts. There are already some examples of it. But don‚Äôt bother about it now.\nFind the ‚ÄúBuild‚Äù tab and press ‚ÄúBuild Website‚Äù. Your blog will be created in a new window.\nClick ‚ÄúOpen in Browser‚Äù and explore your website\nWe aren‚Äôt online yet. But very soon!\n4. Connect to your github\n4.1. Create new repository\nGo back to RStudio and run use_git() in order to create a new local Git repository\nthen answer two questions:\n‚ÄúIs it ok to commit them?‚Äù Don‚Äôt commit by typing 3 for ‚ÄúNo‚Äù, or ‚ÄúNope‚Äù or similar.\n‚ÄúA restart of RStudio is required to activate the Git pane Restart now?‚Äù Restart by typing 2 for ‚ÄúYes‚Äù or ‚ÄúYup‚Äù.\n\n\n\nusethis::use_git() \n\n\n\nAfter restart you‚Äôll see a new ‚ÄúGit‚Äù tab appear between the ‚ÄúBuild‚Äù and ‚ÄúTutorial‚Äù tabs. That‚Äôs gut!\nclick on the ‚ÄúGit‚Äù tab and you‚Äôll see empty boxed under ‚ÄúStaged‚Äù, lot‚Äôs of yellow question marks under ‚ÄúStatus‚Äù and the file-names under ‚ÄúPath‚Äù.\n4.2. Stage and Commit\ncheck all the boxes and press ‚ÄúCommit‚Äù button, which is (vertically) between the ‚ÄúStatus‚Äù and ‚ÄúHistory‚Äù tabs. A colourful window will pop up. This window describes all the changes you are about to make to your blog.\nFind the ‚ÄúCommit message‚Äù box and definitely describe what changes you have done (e.g.¬†‚ÄúFirst commit‚Äù), because then you‚Äôll always be able to get back to the previous version, in case something stops working. That‚Äôs what they call - a version control.\npress ‚ÄúCommit‚Äù\nwait until you see the ‚ÄúClose‚Äù button and close ‚ÄúGit commit‚Äù\nforget the other pop up window and go back to RStudio.\n4.3. Run ‚Äúusethis::use_github()‚Äù to connect a local repository to Github\n\n\nusethis::use_github() \n\n\n\nA new repository will be automatically created on your Github profile and the new browser window with your Github will pop up.\nIf you‚Äôll be asked: ‚ÄúWhich git protocol to use?‚Äù, choose the one with ‚Äúhttps‚Äù and if you‚Äôll then be asked: ‚ÄúAre title and description ok?‚Äù, agree to proceed.\nNOTE: if something (e.g.¬†Github Personal Access Token) doesn‚Äôt work, get back to the Happy Git and GitHub for the useR book and work through it if you still didn‚Äôt. You‚Äôll only need it once!\n5. Publish your blog (or a website) via Netlify\nsign into your Netlify account, if have one, if not‚Ä¶\ngo the https://www.netlify.com/, sing up for Netlify either with your Email or with your Github profile.\nclick a green box ‚ÄúNew site from Git‚Äù\nconfigure Netlify on ‚ÄúGithub‚Äù,\nchoose a newly created repository, you‚Äôll recognize the name (‚Äúmy new blog‚Äù?).\nNOTE: Make sure to set the ‚ÄúPublish Directory‚Äù to \"_site\" (could be ‚Äúdocs‚Äù if you checked some boxes while creating new project). \"_site\" (or ‚Äúdocs‚Äù) contains all the information about your blog.\nclick ‚ÄúDeploy‚Äù!\nwait a moment till the ‚ÄúProduction‚Äù tab produces green colored ‚ÄúPublished‚Äù and you‚Äôll get a funny named website in the left top corner, also green and starting with ‚Äúhttps://‚Äù. My was ‚Äúhttps://condescending-darwin-bc567f.netlify.app‚Äù :)\nclick on it\nCongrats, you are online!\ngo back to Netlify, click ‚ÄúSite settings‚Äù => ‚ÄúChange site name‚Äù\nrename your site (e.g.¬†better-name) and hit ‚ÄúSave‚Äù\nclick on the ‚Äúbetter-name.netlify.app‚Äù to make sure the name has changed and the site is still working\ngo back to RStudio to populate your blog\n6. Create a new blog-post\nrun the line below with the name of your choice\n\n\ndistill::create_post(\"My second blog post...this one will be really good :-)\")\n\n\n\na new partly pre-filled RMarkdown document will open itself in RStudio\nfill it with some text and code as you usually do with RMarkdown documents\nhit ‚ÄúKnit‚Äù. NOTE: you‚Äôll need to always ‚ÄúKnit‚Äù all changed or created blog-posts individually. It is the only way to update them. ‚ÄúBuild Website‚Äù would not re-render them for you, because it‚Äôs computationally expensive and prone to problems. However, the Distill-Website (we are doing Distill-Blog now) would. You can learn more about the difference between them here.\nThis new post exists only on your local computer, still not online, thus‚Ä¶\ngo to the ‚ÄúGit‚Äù tab in RStudio and check all the boxes\npress ‚ÄúCommit‚Äù, a new window will pop up\nadd description of your commit\nagain press ‚ÄúCommit‚Äù\nwait until you see the ‚ÄúClose‚Äù button and close ‚ÄúGit commit‚Äù pop up window\npress ‚ÄúPush‚Äù. Pushing will transfer changes in your blog from your local computer to a remote place, namely your Github repository. And since your Github repository is connected to Netlify, this changes will be online after successful push.\nwait until you see the ‚ÄúClose‚Äù button and close ‚ÄúGit push‚Äù pop up window\nclose or ignore the other pop up window\nget back to your blog ‚Äúbetter-name.netlify.app‚Äù and refresh (it may take a few seconds, so, don‚Äôt panic if the first refresh don‚Äôt work). You should see a new blog-post.\nCongrats! You now continuously deploy your online blog!\nclick on your post and enjoy your creative work for a moment ;), then\ngo back to RStudio and repeat step 6 with following routine:\nCreate or change posts\nKnit\nCommit\nPush\n\nYou don‚Äôt need to commit and push every change, only important ones. Think of this process as really saving the progress you made on your blog. Another useful thing I learned to appreciate after committing a couple of thousands of changes (which is annoying!) is to - check all the boxes (under ‚Äústage‚Äù) at once! For this:\ngo to ‚ÄúTerminal‚Äù tab in RStuio (it‚Äôs near the ‚ÄúConsole‚Äù)\ntype ‚Äúgit add -A‚Äù and press enter\ncheck one of the stage-boxes, the rest of them suppose to be then check themselves automatically\nthen press ‚ÄúCommit‚Äù\nBy the way, if you don‚Äôt want to publish your post until you really satisfied with it, you can start out as a draft:\n\n\ndistill::create_post(\"Another nice post\", draft = TRUE)\n\n\n\nOr add draft: true to the post‚Äôs metadata. When you are ready, delete draft: true.\nBlog configuration with \"_site.yml\"\nOpen \"_site.yml\". You‚Äôll see something like that:\n\nname: \"Your cool webpage\"\ntitle: \"Your cool webpage title\"\ndescription: |\n  Exploring something very important.\nbase_url: https://beta.rstudioconnect.com/content/your_thing/\nnavbar:\n  logo: images/fancy_logo.png\n  right:\n    - text: \"Home\"\n      href: index.html\n    - text: \"About\"\n      href: about.html\n    - text: \"Rest\"\n      href: rest.html\n    - icon: fa fa-rss\n      href: index.xml\noutput: distill::distill_article\n\n\"_site.yml\" is the most important document for your website. Configure it carefully and slowly. You can add a lot of useful things: categories, google-analytics, customize the navigation bar, add references, new theme with CSS code for an individual design of your site, icons of twitter & co. and much more, which is not to important for a new site and can be added at any time.\nIf you work through the Distill web page, you‚Äôll see a lot of examples for how to design your \"_site.yml\" and ‚Äútheme.css‚Äù files. But, the best way I found to do this, is just to find the \"_site.yml\" or ‚Äútheme.css‚Äù of other Distill-blogs on Github and get inspiration from them by playing with your own code (I hope nobody is offended by this sentence due to the open source nature of R, but please let me know if it‚Äôs wrong and I‚Äôll remove this recommendation!).\nWhile you already have \"_site.yml\" file in your blog-folder, you don‚Äôt have a ‚Äútheme.css‚Äù file. To get one, read on‚Ä¶\nTheming - change desing and appearence of your blog\nYou can modify the CSS code in your theme after creating it by running the following line of code:\n\n\ndistill::create_theme(name = \"theme\") \n\n\n\nTo activate a custom theme site-wide, add a theme key to the top-level of your \"_site.yml\" configuration file:\n\nname: \"Your cool webpage\"\ntitle: \"Your cool webpage title\"\ntheme: theme.css \n(...the rest of your _site.yml)\n\nPimp your content\nMost elements enhancing your content, like links, tables, plots, equations etc., are similar to the usual R Markdown syntax. Thus for a deeper insights go to the R Markdown: The Definitive Guide book, it‚Äôs online and free. Below I just display some quick ‚Äúhow to‚Äù examples and provide links to a more thorough online resources.\nLinks\nThe links are displayed with the help of two different brackets. First, use the square brackets to produce a [clickable word or phrase], then, directly after the square brackets, use round brackets with the URL inside, e.g.¬†(https://bookdown.org/yihui/rmarkdown/). The URL by itself would certainly also work, but it‚Äôs not as convenient as this.\nPlots\n\n\nlibrary(tidyverse)\nggplot(mtcars, aes(hp, mpg)) + \n  geom_point() + \n  geom_smooth() +\n  theme_bw()\n\n\n\n\nFigures\nYou can add external static and dynamic figures, plots, photos or diagrams by using knitr::include_graphics() function. More on figures here.\n\n\nknitr::include_graphics(\"images/your_figure.png\")\n\n\n\n\n\n\n\n\n\n\n\nTables\nMore on tables here.\n\n\n# install.packages(\"gtsummary\")\nlibrary(gtsummary)\niris %>% \n  tbl_summary()\n\n\npreserve31f973584197b215\n\nAsides\nYou can include notes or even plots ‚Äúaside‚Äù (to the right) your article:\n\n\n\n\nHere the funny fact or some important info.\n\n\n\nEquations\nYou can use a Latex syntax for it:\n\n$$\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}$$\n\n\\[\n\\sigma = \\sqrt{ \\frac{1}{N} \\sum_{i=1}^N (x_i -\\mu)^2}\n\\]\nFootnotes\nFootnotes are created in the usual RMarkdown way, namely ^[here is the content of your footnote] and here are two examples 1 and 2. Point your cursor on the footnote number or scroll to the bottom of the page to see what‚Äôs inside of the footnote.\nThumbnails\nThumbnails (or previews) are the images which are displayed along the post. They serve as an eye catcher. You can add a preview image into the post‚Äôs metadata by adding a ‚Äúpreview‚Äù field:\n\ntitle: \"Blog post on how to write blog posts\"\ndescription: |\n  Here we were out of ideas, and therefore we are proud to announce\nour new post about how to write a post. Creativity is a b**ch! ... And I love her!\npreview: images/photo-of-me-because-I-am-sooo-beautifuuulll-toniiiight.png\n\nIf you don‚Äôt provide a picture for preview, the first plot or picture from your blog-post will be used as a thumbnail by default. To override this behavior, you can add the preview = TRUE in to the code-chunk, e.g.: {r some_chunk, preview=TRUE}.\nIf you think, I missed something, please comment on it, and I‚Äôll improve this tutorial.\nCitations\nYou either can include a BibTex-like citation of others or make your article citable (see the very bottom of this article). ‚ÄúHow to cite‚Äù is best described in the citation section of the Distill website.\nUseful ressources\nThe best place to start is actually the Distill website itself: https://rstudio.github.io/distill.\nTom Mocks blog-post on how to build blogs with Distill helped me a lot! https://themockup.blog/posts/2020-08-01-building-a-blog-with-distill/\n\nHere is the first footnote, which does not suppose to interrupt the main text!‚Ü©Ô∏é\nHere is the second‚Ü©Ô∏é\n",
    "preview": "posts/2020-12-26-how-to-create-a-blog-or-a-website-in-r-with-distill-package/images/thumbnail.png",
    "last_modified": "2021-01-04T08:55:02+01:00",
    "input_file": "how-to-create-a-blog-or-a-website-in-r-with-distill-package.utf8.md",
    "preview_width": 1920,
    "preview_height": 1080
  }
]
